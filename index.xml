<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>nutriverse</title>
    <link>https://nutriverse.io/</link>
      <atom:link href="https://nutriverse.io/index.xml" rel="self" type="application/rss+xml" />
    <description>nutriverse</description>
    <generator>Hugo -- gohugo.io</generator><language>en-gb</language><lastBuildDate>Wed, 25 Mar 2020 00:00:00 +0000</lastBuildDate>
    <item>
      <title>Build a model</title>
      <link>https://nutriverse.io/start/models/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://nutriverse.io/start/models/</guid>
      <description>&lt;h2 id=&#34;intro&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;How do you create a statistical model using tidymodels? In this article, we will walk you through the steps. We start with data for modeling, learn how to specify and train models with different engines using the 
&lt;a href=&#34;https://tidymodels.github.io/parsnip/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;parsnip package&lt;/a&gt;, and understand why these functions are designed this way.&lt;/p&gt;
&lt;p&gt;To use code in this article,  you will need to install the following packages: readr, rstanarm, and tidymodels.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;library&lt;/span&gt;(tidymodels)  &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# for the parsnip package, along with the rest of tidymodels&lt;/span&gt;

&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# Helper packages&lt;/span&gt;
&lt;span style=&#34;color:#00f&#34;&gt;library&lt;/span&gt;(readr)       &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# for importing data&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;data&#34;&gt;The Sea Urchins Data&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s use the data from 
&lt;a href=&#34;https://link.springer.com/article/10.1007/BF00349318&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Constable (1993)&lt;/a&gt; to explore how three different feeding regimes affect the size of sea urchins over time. The initial size of the sea urchins at the beginning of the experiment probably affects how big they grow as they are fed.&lt;/p&gt;
&lt;p&gt;To start, let&amp;rsquo;s read our urchins data into R, which we&amp;rsquo;ll do by providing 
&lt;a href=&#34;https://readr.tidyverse.org/reference/read_delim.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;readr::read_csv()&lt;/code&gt;&lt;/a&gt; with a url where our CSV data is located (&amp;ldquo;&lt;a href=&#34;https://tidymodels.org/start/models/urchins.csv&#34;&gt;https://tidymodels.org/start/models/urchins.csv&lt;/a&gt;&amp;rdquo;):&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;urchins &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt;
  &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# Data were assembled for a tutorial &lt;/span&gt;
  &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# at https://www.flutterbys.com.au/stats/tut/tut7.5a.html&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;read_csv&lt;/span&gt;(&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;https://tidymodels.org/start/models/urchins.csv&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# Change the names to be a little more verbose&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;setNames&lt;/span&gt;(&lt;span style=&#34;color:#00f&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;food_regime&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;initial_volume&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;width&amp;#34;&lt;/span&gt;)) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# Factors are very helpful for modeling, so we convert one column&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;mutate&lt;/span&gt;(food_regime &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;factor&lt;/span&gt;(food_regime, levels &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;Initial&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;Low&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;High&amp;#34;&lt;/span&gt;)))
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Parsed with column specification:&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; cols(&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   TREAT = col_character(),&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   IV = col_double(),&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   SUTW = col_double()&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; )&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Let&amp;rsquo;s take a quick look at the data:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;urchins
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 72 x 3&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;    food_regime initial_volume width&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;    &amp;lt;fct&amp;gt;                &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  1 Initial                3.5 0.01 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  2 Initial                5   0.02 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  3 Initial                8   0.061&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  4 Initial               10   0.051&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  5 Initial               13   0.041&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  6 Initial               13   0.061&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  7 Initial               15   0.041&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  8 Initial               15   0.071&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  9 Initial               16   0.092&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 10 Initial               17   0.051&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # … with 62 more rows&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The urchins data is a 
&lt;a href=&#34;https://tibble.tidyverse.org/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tibble&lt;/a&gt;. If you are new to tibbles, the best place to start is the 
&lt;a href=&#34;https://r4ds.had.co.nz/tibbles.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tibbles chapter&lt;/a&gt; in &lt;em&gt;R for Data Science&lt;/em&gt;. For each of the 72 urchins, we know their:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;experimental feeding regime group (&lt;code&gt;food_regime&lt;/code&gt;: either &lt;code&gt;Initial&lt;/code&gt;, &lt;code&gt;Low&lt;/code&gt;, or &lt;code&gt;High&lt;/code&gt;),&lt;/li&gt;
&lt;li&gt;size in milliliters at the start of the experiment (&lt;code&gt;initial_volume&lt;/code&gt;), and&lt;/li&gt;
&lt;li&gt;suture width at the end of the experiment (&lt;code&gt;width&lt;/code&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As a first step in modeling, it&amp;rsquo;s always a good idea to plot the data:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;ggplot&lt;/span&gt;(urchins,
       &lt;span style=&#34;color:#00f&#34;&gt;aes&lt;/span&gt;(x &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; initial_volume, 
           y &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; width, 
           group &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; food_regime, 
           col &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; food_regime)) &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;geom_point&lt;/span&gt;() &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;geom_smooth&lt;/span&gt;(method &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; lm, se &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;FALSE&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;scale_color_viridis_d&lt;/span&gt;(option &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;plasma&amp;#34;&lt;/span&gt;, end &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;.7&lt;/span&gt;)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; `geom_smooth()` using formula &amp;#39;y ~ x&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;figs/urchin-plot-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see that urchins that were larger in volume at the start of the experiment tended to have wider sutures at the end, but the slopes of the lines look different so this effect may depend on the feeding regime condition.&lt;/p&gt;
&lt;h2 id=&#34;build-model&#34;&gt;Build and fit a model&lt;/h2&gt;
&lt;p&gt;A standard two-way analysis of variance (
&lt;a href=&#34;https://www.itl.nist.gov/div898/handbook/prc/section4/prc43.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ANOVA&lt;/a&gt;) model makes sense for this dataset because we have both a continuous predictor and a categorical predictor. Since the slopes appear to be different for at least two of the feeding regimes, let&amp;rsquo;s build a model that allows for two-way interactions. Specifying an R formula with our variables in this way:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;width &lt;span style=&#34;color:#666&#34;&gt;~&lt;/span&gt; initial_volume &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt; food_regime
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;allows our regression model depending on initial volume to have separate slopes and intercepts for each food regime.&lt;/p&gt;
&lt;p&gt;For this kind of model, ordinary least squares is a good initial approach. With tidymodels, we start by specifying the &lt;em&gt;functional form&lt;/em&gt; of the model that we want using the 
&lt;a href=&#34;https://tidymodels.github.io/parsnip/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;parsnip package&lt;/a&gt;. Since there is a numeric outcome and the model should be linear with slopes and intercepts, the model type is 
&lt;a href=&#34;https://tidymodels.github.io/parsnip/reference/linear_reg.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;linear regression&amp;rdquo;&lt;/a&gt;. We can declare this with:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;linear_reg&lt;/span&gt;()
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Linear Regression Model Specification (regression)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;That is pretty underwhelming since, on its own, it doesn&amp;rsquo;t really do much. However, now that the type of model has been specified, a method for &lt;em&gt;fitting&lt;/em&gt; or training the model can be stated using the &lt;strong&gt;engine&lt;/strong&gt;. The engine value is often a mash-up of the software that can be used to fit or train the model as well as the estimation method. For example, to use ordinary least squares, we can set the engine to be &lt;code&gt;lm&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;linear_reg&lt;/span&gt;() &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;set_engine&lt;/span&gt;(&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;lm&amp;#34;&lt;/span&gt;)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Linear Regression Model Specification (regression)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Computational engine: lm&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The 
&lt;a href=&#34;https://tidymodels.github.io/parsnip/reference/linear_reg.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentation page for &lt;code&gt;linear_reg()&lt;/code&gt;&lt;/a&gt; lists the possible engines. We&amp;rsquo;ll save this model object as &lt;code&gt;lm_mod&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;lm_mod &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;linear_reg&lt;/span&gt;() &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;set_engine&lt;/span&gt;(&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;lm&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;From here, the model can be estimated or trained using the 
&lt;a href=&#34;https://tidymodels.github.io/parsnip/reference/fit.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;fit()&lt;/code&gt;&lt;/a&gt; function:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;lm_fit &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  lm_mod &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;fit&lt;/span&gt;(width &lt;span style=&#34;color:#666&#34;&gt;~&lt;/span&gt; initial_volume &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt; food_regime, data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; urchins)
lm_fit
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; parsnip model object&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Fit time:  3ms &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Call:&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; stats::lm(formula = formula, data = data)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Coefficients:&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;                    (Intercept)                  initial_volume  &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;                      0.0331216                       0.0015546  &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;                 food_regimeLow                 food_regimeHigh  &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;                      0.0197824                       0.0214111  &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  initial_volume:food_regimeLow  initial_volume:food_regimeHigh  &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;                     -0.0012594                       0.0005254&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Perhaps our analysis requires a description of the model parameter estimates and their statistical properties. Although the &lt;code&gt;summary()&lt;/code&gt; function for &lt;code&gt;lm&lt;/code&gt; objects can provide that, it gives the results back in an unwieldy format. Many models have a &lt;code&gt;tidy()&lt;/code&gt; method that provides the summary results in a more predictable and useful format (e.g. a data frame with standard column names):&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;tidy&lt;/span&gt;(lm_fit)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 6 x 5&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   term                            estimate std.error statistic  p.value&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   &amp;lt;chr&amp;gt;                              &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 1 (Intercept)                     0.0331    0.00962      3.44  0.00100 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 2 initial_volume                  0.00155   0.000398     3.91  0.000222&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 3 food_regimeLow                  0.0198    0.0130       1.52  0.133   &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 4 food_regimeHigh                 0.0214    0.0145       1.47  0.145   &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 5 initial_volume:food_regimeLow  -0.00126   0.000510    -2.47  0.0162  &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 6 initial_volume:food_regimeHigh  0.000525  0.000702     0.748 0.457&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;predict-model&#34;&gt;Use a model to predict&lt;/h2&gt;
&lt;p&gt;This fitted object &lt;code&gt;lm_fit&lt;/code&gt; has the &lt;code&gt;lm&lt;/code&gt; model output built-in, which you can access with &lt;code&gt;lm_fit$fit&lt;/code&gt;, but there are some benefits to using the fitted parsnip model object when it comes to predicting.&lt;/p&gt;
&lt;p&gt;Suppose that, for a publication, it would be particularly interesting to make a plot of the mean body size for urchins that started the experiment with an initial volume of 20ml. To create such a graph, we start with some new example data that we will make predictions for, to show in our graph:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;new_points &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;expand.grid&lt;/span&gt;(initial_volume &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;20&lt;/span&gt;, 
                          food_regime &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;Initial&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;Low&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;High&amp;#34;&lt;/span&gt;))
new_points
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   initial_volume food_regime&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 1             20     Initial&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 2             20         Low&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 3             20        High&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;To get our predicted results, we can use the &lt;code&gt;predict()&lt;/code&gt; function to find the mean values at 20ml.&lt;/p&gt;
&lt;p&gt;It is also important to communicate the variability, so we also need to find the predicted confidence intervals. If we had used &lt;code&gt;lm()&lt;/code&gt; to fit the model directly, a few minutes of reading the 
&lt;a href=&#34;https://stat.ethz.ch/R-manual/R-devel/library/stats/html/predict.lm.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentation page&lt;/a&gt; for &lt;code&gt;predict.lm()&lt;/code&gt; would explain how to do this. However, if we decide to use a different model to estimate urchin size (&lt;em&gt;spoiler:&lt;/em&gt; we will!), it is likely that a completely different syntax would be required.&lt;/p&gt;
&lt;p&gt;Instead, with tidymodels, the types of predicted values are standardized so that we can use the same syntax to get these values.&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s generate the mean body width values:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;mean_pred &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;predict&lt;/span&gt;(lm_fit, new_data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; new_points)
mean_pred
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 3 x 1&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;    .pred&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;    &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 1 0.0642&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 2 0.0588&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 3 0.0961&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;When making predictions, the tidymodels convention is to always produce a tibble of results with standardized column names. This makes it easy to combine the original data and the predictions in a usable format:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;conf_int_pred &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;predict&lt;/span&gt;(lm_fit, 
                         new_data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; new_points, 
                         type &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;conf_int&amp;#34;&lt;/span&gt;)
conf_int_pred
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 3 x 2&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   .pred_lower .pred_upper&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;         &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 1      0.0555      0.0729&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 2      0.0499      0.0678&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 3      0.0870      0.105&lt;/span&gt;

&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# Now combine: &lt;/span&gt;
plot_data &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  new_points &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;bind_cols&lt;/span&gt;(mean_pred) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;bind_cols&lt;/span&gt;(conf_int_pred)

&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# and plot:&lt;/span&gt;
&lt;span style=&#34;color:#00f&#34;&gt;ggplot&lt;/span&gt;(plot_data, &lt;span style=&#34;color:#00f&#34;&gt;aes&lt;/span&gt;(x &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; food_regime)) &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;geom_point&lt;/span&gt;(&lt;span style=&#34;color:#00f&#34;&gt;aes&lt;/span&gt;(y &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; .pred)) &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;geom_errorbar&lt;/span&gt;(&lt;span style=&#34;color:#00f&#34;&gt;aes&lt;/span&gt;(ymin &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; .pred_lower, 
                    ymax &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; .pred_upper),
                width &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;.2&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;labs&lt;/span&gt;(y &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;urchin size&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;figs/lm-all-pred-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;new-engine&#34;&gt;Model with a different engine&lt;/h2&gt;
&lt;p&gt;Every one on your team is happy with that plot &lt;em&gt;except&lt;/em&gt; that one person who just read their first book on 
&lt;a href=&#34;https://bayesian.org/what-is-bayesian-analysis/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bayesian analysis&lt;/a&gt;. They are interested in knowing if the results would be different if the model were estimated using a Bayesian approach. In such an analysis, a 
&lt;a href=&#34;https://towardsdatascience.com/introduction-to-bayesian-linear-regression-e66e60791ea7&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;prior distribution&lt;/em&gt;&lt;/a&gt; needs to be declared for each model parameter that represents the possible values of the parameters (before being exposed to the observed data). After some discussion, the group agrees that the priors should be bell-shaped but, since no one has any idea what the range of values should be, to take a conservative approach and make the priors &lt;em&gt;wide&lt;/em&gt; using a Cauchy distribution (which is the same as a t-distribution with a single degree of freedom).&lt;/p&gt;
&lt;p&gt;The 
&lt;a href=&#34;https://mc-stan.org/rstanarm/articles/priors.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentation&lt;/a&gt; on the rstanarm package shows us that the &lt;code&gt;stan_glm()&lt;/code&gt; function can be used to estimate this model, and that the function arguments that need to be specified are called &lt;code&gt;prior&lt;/code&gt; and &lt;code&gt;prior_intercept&lt;/code&gt;. It turns out that &lt;code&gt;linear_reg()&lt;/code&gt; has a 
&lt;a href=&#34;https://tidymodels.github.io/parsnip/reference/linear_reg.html#details&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;stan&lt;/code&gt; engine&lt;/a&gt;. Since these prior distribution arguments are specific to the Stan software, they are passed as arguments to 
&lt;a href=&#34;https://tidymodels.github.io/parsnip/reference/set_engine.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;parsnip::set_engine()&lt;/code&gt;&lt;/a&gt;. After that, the same exact &lt;code&gt;fit()&lt;/code&gt; call is used:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# set the prior distribution&lt;/span&gt;
prior_dist &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; rstanarm&lt;span style=&#34;color:#666&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#00f&#34;&gt;student_t&lt;/span&gt;(df &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;)

&lt;span style=&#34;color:#00f&#34;&gt;set.seed&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;123&lt;/span&gt;)

&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# make the parsnip model&lt;/span&gt;
bayes_mod &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt;   
  &lt;span style=&#34;color:#00f&#34;&gt;linear_reg&lt;/span&gt;() &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;set_engine&lt;/span&gt;(&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;stan&amp;#34;&lt;/span&gt;, 
             prior_intercept &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; prior_dist, 
             prior &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; prior_dist) 

&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# train the model&lt;/span&gt;
bayes_fit &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  bayes_mod &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;fit&lt;/span&gt;(width &lt;span style=&#34;color:#666&#34;&gt;~&lt;/span&gt; initial_volume &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt; food_regime, data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; urchins)

&lt;span style=&#34;color:#00f&#34;&gt;print&lt;/span&gt;(bayes_fit, digits &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;5&lt;/span&gt;)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; parsnip model object&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Fit time:  1.5s &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; stan_glm&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  family:       gaussian [identity]&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  formula:      width ~ initial_volume * food_regime&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  observations: 72&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  predictors:   6&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ------&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;                                Median   MAD_SD  &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; (Intercept)                     0.03452  0.00883&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; initial_volume                  0.00150  0.00037&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; food_regimeLow                  0.01805  0.01221&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; food_regimeHigh                 0.01934  0.01367&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; initial_volume:food_regimeLow  -0.00119  0.00047&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; initial_volume:food_regimeHigh  0.00061  0.00065&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Auxiliary parameter(s):&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;       Median  MAD_SD &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; sigma 0.02121 0.00186&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ------&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; * For help interpreting the printed output see ?print.stanreg&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; * For info on the priors used see ?prior_summary.stanreg&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This kind of Bayesian analysis (like many models) involves randomly generated numbers in its fitting procedure. We can use &lt;code&gt;set.seed()&lt;/code&gt; to ensure that the same (pseudo-)random numbers are generated each time we run this code. The number &lt;code&gt;123&lt;/code&gt; isn&amp;rsquo;t special or related to our data; it is just a &amp;ldquo;seed&amp;rdquo; used to choose random numbers.&lt;/p&gt;
&lt;p&gt;To update the parameter table, the &lt;code&gt;tidy()&lt;/code&gt; method is once again used:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;tidy&lt;/span&gt;(bayes_fit, intervals &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;TRUE&lt;/span&gt;)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 6 x 5&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   term                            estimate std.error     lower     upper&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   &amp;lt;chr&amp;gt;                              &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 1 (Intercept)                     0.0345    0.00883   0.0200    0.0490  &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 2 initial_volume                  0.00150   0.000369  0.000895  0.00212 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 3 food_regimeLow                  0.0181    0.0122   -0.00181   0.0380  &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 4 food_regimeHigh                 0.0193    0.0137   -0.00317   0.0420  &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 5 initial_volume:food_regimeLow  -0.00119   0.000472 -0.00199  -0.000413&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 6 initial_volume:food_regimeHigh  0.000610  0.000651 -0.000490  0.00170&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;A goal of the tidymodels packages is that the &lt;strong&gt;interfaces to common tasks are standardized&lt;/strong&gt; (as seen in the &lt;code&gt;tidy()&lt;/code&gt; results above). The same is true for getting predictions; we can use the same code even though the underlying packages use very different syntax:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;bayes_plot_data &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  new_points &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;bind_cols&lt;/span&gt;(&lt;span style=&#34;color:#00f&#34;&gt;predict&lt;/span&gt;(bayes_fit, new_data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; new_points)) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;bind_cols&lt;/span&gt;(&lt;span style=&#34;color:#00f&#34;&gt;predict&lt;/span&gt;(bayes_fit, new_data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; new_points, type &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;conf_int&amp;#34;&lt;/span&gt;))

&lt;span style=&#34;color:#00f&#34;&gt;ggplot&lt;/span&gt;(bayes_plot_data, &lt;span style=&#34;color:#00f&#34;&gt;aes&lt;/span&gt;(x &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; food_regime)) &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;geom_point&lt;/span&gt;(&lt;span style=&#34;color:#00f&#34;&gt;aes&lt;/span&gt;(y &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; .pred)) &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;geom_errorbar&lt;/span&gt;(&lt;span style=&#34;color:#00f&#34;&gt;aes&lt;/span&gt;(ymin &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; .pred_lower, ymax &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; .pred_upper), width &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;.2&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;labs&lt;/span&gt;(y &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;urchin size&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;ggtitle&lt;/span&gt;(&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;Bayesian model with t(1) prior distribution&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;figs/stan-pred-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This isn&amp;rsquo;t very different from the non-Bayesian results (except in interpretation).&lt;/p&gt;
&lt;div class=&#34;note&#34;&gt;The &lt;a href=&#34;https://parsnip.tidymodels.org/&#34;&gt;parsnip&lt;/a&gt; package can work with many model types, engines, and arguments. Check out &lt;a href=&#34;https://nutriverse.io/find/parsnip/&#34;&gt;tidymodels.org/find/parsnip&lt;/a&gt; to see what is available.&lt;/div&gt;
&lt;h2 id=&#34;why&#34;&gt;Why does it work that way?&lt;/h2&gt;
&lt;p&gt;The extra step of defining the model using a function like &lt;code&gt;linear_reg()&lt;/code&gt; might seem superfluous since a call to &lt;code&gt;lm()&lt;/code&gt; is much more succinct. However, the problem with standard modeling functions is that they don&amp;rsquo;t separate what you want to do from the execution. For example, the process of executing a formula has to happen repeatedly across model calls even when the formula does not change; we can&amp;rsquo;t recycle those computations.&lt;/p&gt;
&lt;p&gt;Also, using the tidymodels framework, we can do some interesting things by incrementally creating a model (instead of using single function call). 
&lt;a href=&#34;https://nutriverse.io/start/tuning/&#34;&gt;Model tuning&lt;/a&gt; with tidymodels uses the specification of the model to declare what parts of the model should be tuned. That would be very difficult to do if &lt;code&gt;linear_reg()&lt;/code&gt; immediately fit the model.&lt;/p&gt;
&lt;p&gt;If you are familiar with the tidyverse, you may have noticed that our modeling code uses the magrittr pipe (&lt;code&gt;%&amp;gt;%&lt;/code&gt;). With dplyr and other tidyverse packages, the pipe works well because all of the functions take the &lt;em&gt;data&lt;/em&gt; as the first argument. For example:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;urchins &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;group_by&lt;/span&gt;(food_regime) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;summarize&lt;/span&gt;(med_vol &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;median&lt;/span&gt;(initial_volume))
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 3 x 2&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   food_regime med_vol&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   &amp;lt;fct&amp;gt;         &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 1 Initial        20.5&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 2 Low            19.2&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 3 High           15&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;whereas the modeling code uses the pipe to pass around the &lt;em&gt;model object&lt;/em&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;bayes_mod &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;fit&lt;/span&gt;(width &lt;span style=&#34;color:#666&#34;&gt;~&lt;/span&gt; initial_volume &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt; food_regime, data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; urchins)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This may seem jarring if you have used dplyr a lot, but it is extremely similar to how ggplot2 operates:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;ggplot&lt;/span&gt;(urchins,
       &lt;span style=&#34;color:#00f&#34;&gt;aes&lt;/span&gt;(initial_volume, width)) &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt;      &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# returns a ggplot object &lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;geom_jitter&lt;/span&gt;() &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt;                         &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# same&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;geom_smooth&lt;/span&gt;(method &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; lm, se &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;FALSE&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt;  &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# same                    &lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;labs&lt;/span&gt;(x &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;Volume&amp;#34;&lt;/span&gt;, y &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;Width&amp;#34;&lt;/span&gt;)         &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# etc&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;session-info&#34;&gt;Session information&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;#&amp;gt; ─ Session info ───────────────────────────────────────────────────────────────
#&amp;gt;  setting  value                       
#&amp;gt;  version  R version 4.0.0 (2020-04-24)
#&amp;gt;  os       macOS Mojave 10.14.6        
#&amp;gt;  system   x86_64, darwin17.0          
#&amp;gt;  ui       X11                         
#&amp;gt;  language (EN)                        
#&amp;gt;  collate  en_US.UTF-8                 
#&amp;gt;  ctype    en_US.UTF-8                 
#&amp;gt;  tz       America/New_York            
#&amp;gt;  date     2020-05-19                  
#&amp;gt; 
#&amp;gt; ─ Packages ───────────────────────────────────────────────────────────────────
#&amp;gt;  package    * version date       lib source        
#&amp;gt;  broom      * 0.5.6   2020-04-20 [1] CRAN (R 4.0.0)
#&amp;gt;  dials      * 0.0.6   2020-04-03 [1] CRAN (R 4.0.0)
#&amp;gt;  dplyr      * 0.8.5   2020-03-07 [1] CRAN (R 4.0.0)
#&amp;gt;  ggplot2    * 3.3.0   2020-03-05 [1] CRAN (R 4.0.0)
#&amp;gt;  infer      * 0.5.1   2019-11-19 [1] CRAN (R 4.0.0)
#&amp;gt;  parsnip    * 0.1.1   2020-05-06 [1] CRAN (R 4.0.0)
#&amp;gt;  purrr      * 0.3.4   2020-04-17 [1] CRAN (R 4.0.0)
#&amp;gt;  readr      * 1.3.1   2018-12-21 [1] CRAN (R 4.0.0)
#&amp;gt;  recipes    * 0.1.12  2020-05-01 [1] CRAN (R 4.0.0)
#&amp;gt;  rlang        0.4.6   2020-05-02 [1] CRAN (R 4.0.0)
#&amp;gt;  rsample    * 0.0.6   2020-03-31 [1] CRAN (R 4.0.0)
#&amp;gt;  rstanarm   * 2.19.3  2020-02-11 [1] CRAN (R 4.0.0)
#&amp;gt;  tibble     * 3.0.1   2020-04-20 [1] CRAN (R 4.0.0)
#&amp;gt;  tidymodels * 0.1.0   2020-02-16 [1] CRAN (R 4.0.0)
#&amp;gt;  tune       * 0.1.0   2020-04-02 [1] CRAN (R 4.0.0)
#&amp;gt;  workflows  * 0.1.1   2020-03-17 [1] CRAN (R 4.0.0)
#&amp;gt;  yardstick  * 0.0.6   2020-03-17 [1] CRAN (R 4.0.0)
#&amp;gt; 
#&amp;gt; [1] /Library/Frameworks/R.framework/Versions/4.0/Resources/library
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Regression models two ways</title>
      <link>https://nutriverse.io/learn/models/parsnip-ranger-glmnet/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://nutriverse.io/learn/models/parsnip-ranger-glmnet/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;To use the code in this article, you will need to install the following packages: AmesHousing, glmnet, randomForest, ranger, and tidymodels.&lt;/p&gt;
&lt;p&gt;We can create regression models with the tidymodels package 
&lt;a href=&#34;https://tidymodels.github.io/parsnip/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;parsnip&lt;/a&gt; to predict continuous or numeric quantities. Here, let&amp;rsquo;s first fit a random forest model, which does &lt;em&gt;not&lt;/em&gt; require all numeric input (see discussion 
&lt;a href=&#34;https://bookdown.org/max/FES/categorical-trees.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;) and discuss how to use &lt;code&gt;fit()&lt;/code&gt; and &lt;code&gt;fit_xy()&lt;/code&gt;, as well as &lt;em&gt;data descriptors&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Second, let&amp;rsquo;s fit a regularized linear regression model to demonstrate how to move between different types of models using parsnip.&lt;/p&gt;
&lt;h2 id=&#34;the-ames-housing-data&#34;&gt;The Ames housing data&lt;/h2&gt;
&lt;p&gt;We&amp;rsquo;ll use the Ames housing data set to demonstrate how to create regression models using parsnip. First, set up the data set and create a simple training/test set split:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;library&lt;/span&gt;(AmesHousing)
ames &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;make_ames&lt;/span&gt;()

&lt;span style=&#34;color:#00f&#34;&gt;library&lt;/span&gt;(tidymodels)

&lt;span style=&#34;color:#00f&#34;&gt;set.seed&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;4595&lt;/span&gt;)
data_split &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;initial_split&lt;/span&gt;(ames, strata &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;Sale_Price&amp;#34;&lt;/span&gt;, p &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;0.75&lt;/span&gt;)

ames_train &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;training&lt;/span&gt;(data_split)
ames_test  &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;testing&lt;/span&gt;(data_split)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The use of the test set here is &lt;em&gt;only for illustration&lt;/em&gt;; normally in a data analysis these data would be saved to the very end after many models have been evaluated.&lt;/p&gt;
&lt;h2 id=&#34;random-forest&#34;&gt;Random forest&lt;/h2&gt;
&lt;p&gt;We&amp;rsquo;ll start by fitting a random forest model to a small set of parameters. Let&amp;rsquo;s create a model with the predictors &lt;code&gt;Longitude&lt;/code&gt;, &lt;code&gt;Latitude&lt;/code&gt;, &lt;code&gt;Lot_Area&lt;/code&gt;, &lt;code&gt;Neighborhood&lt;/code&gt;, and &lt;code&gt;Year_Sold&lt;/code&gt;. A simple random forest model can be specified via:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;rf_defaults &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;rand_forest&lt;/span&gt;(mode &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;regression&amp;#34;&lt;/span&gt;)
rf_defaults
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Random Forest Model Specification (regression)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The model will be fit with the ranger package by default. Since we didn&amp;rsquo;t add any extra arguments to &lt;code&gt;fit&lt;/code&gt;, &lt;em&gt;many&lt;/em&gt; of the arguments will be set to their defaults from the function  &lt;code&gt;ranger::ranger()&lt;/code&gt;. The help pages for the model function describe the default parameters and you can also use the &lt;code&gt;translate()&lt;/code&gt; function to check out such details.&lt;/p&gt;
&lt;p&gt;The parsnip package provides two different interfaces to fit a model:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the formula interface (&lt;code&gt;fit()&lt;/code&gt;), and&lt;/li&gt;
&lt;li&gt;the non-formula interface (&lt;code&gt;fit_xy()&lt;/code&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s start with the non-formula interface:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;preds &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;Longitude&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;Latitude&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;Lot_Area&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;Neighborhood&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;Year_Sold&amp;#34;&lt;/span&gt;)

rf_xy_fit &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  rf_defaults &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;set_engine&lt;/span&gt;(&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;ranger&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;fit_xy&lt;/span&gt;(
    x &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; ames_train[, preds],
    y &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;log10&lt;/span&gt;(ames_train&lt;span style=&#34;color:#666&#34;&gt;$&lt;/span&gt;Sale_Price)
  )

rf_xy_fit
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; parsnip model object&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Fit time:  952ms &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Ranger result&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Call:&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  ranger::ranger(formula = formula, data = data, num.threads = 1,      verbose = FALSE, seed = sample.int(10^5, 1)) &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Type:                             Regression &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Number of trees:                  500 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Sample size:                      2199 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Number of independent variables:  5 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Mtry:                             2 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Target node size:                 5 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Variable importance mode:         none &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Splitrule:                        variance &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; OOB prediction error (MSE):       0.00844 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; R squared (OOB):                  0.736&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The non-formula interface doesn&amp;rsquo;t do anything to the predictors before passing them to the underlying model function. This particular model does &lt;em&gt;not&lt;/em&gt; require indicator variables (sometimes called &amp;ldquo;dummy variables&amp;rdquo;) to be created prior to fitting the model. Note that the output shows &amp;ldquo;Number of independent variables:  5&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;For regression models, we can use the basic &lt;code&gt;predict()&lt;/code&gt; method, which returns a tibble with a column named &lt;code&gt;.pred&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;test_results &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  ames_test &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;select&lt;/span&gt;(Sale_Price) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;mutate&lt;/span&gt;(Sale_Price &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;log10&lt;/span&gt;(Sale_Price)) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;bind_cols&lt;/span&gt;(
    &lt;span style=&#34;color:#00f&#34;&gt;predict&lt;/span&gt;(rf_xy_fit, new_data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; ames_test[, preds])
  )
test_results &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;slice&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;5&lt;/span&gt;)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 5 x 2&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   Sale_Price .pred&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;        &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 1       5.33  5.22&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 2       5.02  5.21&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 3       5.27  5.25&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 4       5.60  5.51&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 5       5.28  5.24&lt;/span&gt;

&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# summarize performance&lt;/span&gt;
test_results &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;metrics&lt;/span&gt;(truth &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; Sale_Price, estimate &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; .pred) 
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 3 x 3&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   .metric .estimator .estimate&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 1 rmse    standard      0.0914&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 2 rsq     standard      0.717 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 3 mae     standard      0.0662&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Note that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If the model required indicator variables, we would have to create them manually prior to using &lt;code&gt;fit()&lt;/code&gt; (perhaps using the recipes package).&lt;/li&gt;
&lt;li&gt;We had to manually log the outcome prior to modeling.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now, for illustration, let&amp;rsquo;s use the formula method using some new parameter values:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;rand_forest&lt;/span&gt;(mode &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;regression&amp;#34;&lt;/span&gt;, mtry &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;3&lt;/span&gt;, trees &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;1000&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;set_engine&lt;/span&gt;(&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;ranger&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;fit&lt;/span&gt;(
    &lt;span style=&#34;color:#00f&#34;&gt;log10&lt;/span&gt;(Sale_Price) &lt;span style=&#34;color:#666&#34;&gt;~&lt;/span&gt; Longitude &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; Latitude &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; Lot_Area &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; Neighborhood &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; Year_Sold,
    data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; ames_train
  )
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; parsnip model object&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Fit time:  2.6s &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Ranger result&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Call:&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  ranger::ranger(formula = formula, data = data, mtry = ~3, num.trees = ~1000,      num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1)) &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Type:                             Regression &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Number of trees:                  1000 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Sample size:                      2199 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Number of independent variables:  5 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Mtry:                             3 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Target node size:                 5 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Variable importance mode:         none &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Splitrule:                        variance &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; OOB prediction error (MSE):       0.00848 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; R squared (OOB):                  0.735&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Suppose that we would like to use the randomForest package instead of ranger. To do so, the only part of the syntax that needs to change is the &lt;code&gt;set_engine()&lt;/code&gt; argument:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;rand_forest&lt;/span&gt;(mode &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;regression&amp;#34;&lt;/span&gt;, mtry &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;3&lt;/span&gt;, trees &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;1000&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;set_engine&lt;/span&gt;(&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;randomForest&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;fit&lt;/span&gt;(
    &lt;span style=&#34;color:#00f&#34;&gt;log10&lt;/span&gt;(Sale_Price) &lt;span style=&#34;color:#666&#34;&gt;~&lt;/span&gt; Longitude &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; Latitude &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; Lot_Area &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; Neighborhood &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; Year_Sold,
    data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; ames_train
  )
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; parsnip model object&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Fit time:  2.1s &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Call:&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  randomForest(x = as.data.frame(x), y = y, ntree = ~1000, mtry = ~3) &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;                Type of random forest: regression&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;                      Number of trees: 1000&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; No. of variables tried at each split: 3&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;           Mean of squared residuals: 0.013&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;                     % Var explained: 59.4&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Look at the formula code that was printed out; one function uses the argument name &lt;code&gt;ntree&lt;/code&gt; and the other uses &lt;code&gt;num.trees&lt;/code&gt;. The parsnip models don&amp;rsquo;t require you to know the specific names of the main arguments.&lt;/p&gt;
&lt;p&gt;Now suppose that we want to modify the value of &lt;code&gt;mtry&lt;/code&gt; based on the number of predictors in the data. Usually, a good default value is &lt;code&gt;floor(sqrt(num_predictors))&lt;/code&gt; but a pure bagging model requires an &lt;code&gt;mtry&lt;/code&gt; value equal to the total number of parameters. There may be cases where you may not know how many predictors are going to be present when the model will be fit (perhaps due to the generation of indicator variables or a variable filter) so this might be difficult to know exactly ahead of time when you write your code.&lt;/p&gt;
&lt;p&gt;When the model it being fit by parsnip, 
&lt;a href=&#34;https://tidymodels.github.io/parsnip/reference/descriptors.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;data descriptors&lt;/em&gt;&lt;/a&gt; are made available. These attempt to let you know what you will have available when the model is fit. When a model object is created (say using &lt;code&gt;rand_forest()&lt;/code&gt;), the values of the arguments that you give it are &lt;em&gt;immediately evaluated&lt;/em&gt; unless you delay them. To delay the evaluation of any argument, you can used &lt;code&gt;rlang::expr()&lt;/code&gt; to make an expression.&lt;/p&gt;
&lt;p&gt;Two relevant data descriptors for our example model are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;.preds()&lt;/code&gt;: the number of predictor &lt;em&gt;variables&lt;/em&gt; in the data set that are associated with the predictors &lt;strong&gt;prior to dummy variable creation&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;.cols()&lt;/code&gt;: the number of predictor &lt;em&gt;columns&lt;/em&gt; after dummy variables (or other encodings) are created.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Since ranger won&amp;rsquo;t create indicator values, &lt;code&gt;.preds()&lt;/code&gt; would be appropriate for &lt;code&gt;mtry&lt;/code&gt; for a bagging model.&lt;/p&gt;
&lt;p&gt;For example, let&amp;rsquo;s use an expression with the &lt;code&gt;.preds()&lt;/code&gt; descriptor to fit a bagging model:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;rand_forest&lt;/span&gt;(mode &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;regression&amp;#34;&lt;/span&gt;, mtry &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;.preds&lt;/span&gt;(), trees &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;1000&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;set_engine&lt;/span&gt;(&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;ranger&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;fit&lt;/span&gt;(
    &lt;span style=&#34;color:#00f&#34;&gt;log10&lt;/span&gt;(Sale_Price) &lt;span style=&#34;color:#666&#34;&gt;~&lt;/span&gt; Longitude &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; Latitude &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; Lot_Area &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; Neighborhood &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; Year_Sold,
    data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; ames_train
  )
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; parsnip model object&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Fit time:  3.6s &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Ranger result&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Call:&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  ranger::ranger(formula = formula, data = data, mtry = ~.preds(),      num.trees = ~1000, num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1)) &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Type:                             Regression &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Number of trees:                  1000 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Sample size:                      2199 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Number of independent variables:  5 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Mtry:                             5 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Target node size:                 5 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Variable importance mode:         none &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Splitrule:                        variance &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; OOB prediction error (MSE):       0.00869 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; R squared (OOB):                  0.728&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;regularized-regression&#34;&gt;Regularized regression&lt;/h2&gt;
&lt;p&gt;A linear model might work for this data set as well. We can use the &lt;code&gt;linear_reg()&lt;/code&gt; parsnip model. There are two engines that can perform regularization/penalization, the glmnet and sparklyr packages. Let&amp;rsquo;s use the former here. The glmnet package only implements a non-formula method, but parsnip will allow either one to be used.&lt;/p&gt;
&lt;p&gt;When regularization is used, the predictors should first be centered and scaled before being passed to the model. The formula method won&amp;rsquo;t do that automatically so we will need to do this ourselves. We&amp;rsquo;ll use the 
&lt;a href=&#34;https://tidymodels.github.io/recipes/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;recipes&lt;/a&gt; package for these steps.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;norm_recipe &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;recipe&lt;/span&gt;(
    Sale_Price &lt;span style=&#34;color:#666&#34;&gt;~&lt;/span&gt; Longitude &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; Latitude &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; Lot_Area &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; Neighborhood &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; Year_Sold, 
    data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; ames_train
  ) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;step_other&lt;/span&gt;(Neighborhood) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;step_dummy&lt;/span&gt;(&lt;span style=&#34;color:#00f&#34;&gt;all_nominal&lt;/span&gt;()) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;step_center&lt;/span&gt;(&lt;span style=&#34;color:#00f&#34;&gt;all_predictors&lt;/span&gt;()) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;step_scale&lt;/span&gt;(&lt;span style=&#34;color:#00f&#34;&gt;all_predictors&lt;/span&gt;()) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;step_log&lt;/span&gt;(Sale_Price, base &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;10&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# estimate the means and standard deviations&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;prep&lt;/span&gt;(training &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; ames_train, retain &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;TRUE&lt;/span&gt;)

&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# Now let&amp;#39;s fit the model using the processed version of the data&lt;/span&gt;

glmn_fit &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;linear_reg&lt;/span&gt;(penalty &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;0.001&lt;/span&gt;, mixture &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;0.5&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;set_engine&lt;/span&gt;(&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;glmnet&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;fit&lt;/span&gt;(Sale_Price &lt;span style=&#34;color:#666&#34;&gt;~&lt;/span&gt; ., data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;juice&lt;/span&gt;(norm_recipe))
glmn_fit
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; parsnip model object&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Fit time:  13ms &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Call:  glmnet::glmnet(x = as.matrix(x), y = y, family = &amp;#34;gaussian&amp;#34;,      alpha = ~0.5) &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;    Df  %Dev Lambda&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 1   0 0.000 0.1370&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 2   1 0.019 0.1250&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 3   1 0.036 0.1140&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 4   1 0.050 0.1040&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 5   2 0.068 0.0946&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 6   4 0.093 0.0862&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 7   5 0.125 0.0785&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 8   5 0.153 0.0716&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 9   7 0.184 0.0652&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 10  7 0.214 0.0594&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 11  7 0.240 0.0541&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 12  8 0.262 0.0493&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 13  8 0.286 0.0449&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 14  8 0.306 0.0409&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 15  8 0.323 0.0373&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 16  8 0.338 0.0340&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 17  8 0.350 0.0310&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 18  8 0.361 0.0282&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 19  9 0.370 0.0257&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 20  9 0.379 0.0234&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 21  9 0.386 0.0213&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 22  9 0.392 0.0195&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 23  9 0.397 0.0177&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 24  9 0.401 0.0161&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 25  9 0.405 0.0147&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 26  9 0.408 0.0134&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 27 10 0.410 0.0122&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 28 11 0.413 0.0111&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 29 11 0.415 0.0101&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 30 11 0.417 0.0092&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 31 12 0.418 0.0084&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 32 12 0.420 0.0077&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 33 12 0.421 0.0070&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 34 12 0.422 0.0064&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 35 12 0.423 0.0058&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 36 12 0.423 0.0053&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 37 12 0.424 0.0048&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 38 12 0.425 0.0044&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 39 12 0.425 0.0040&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 40 12 0.425 0.0036&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 41 12 0.426 0.0033&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 42 12 0.426 0.0030&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 43 12 0.426 0.0028&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 44 12 0.426 0.0025&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 45 12 0.426 0.0023&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 46 12 0.426 0.0021&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 47 12 0.427 0.0019&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 48 12 0.427 0.0017&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 49 12 0.427 0.0016&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 50 12 0.427 0.0014&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 51 12 0.427 0.0013&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 52 12 0.427 0.0012&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 53 12 0.427 0.0011&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 54 12 0.427 0.0010&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 55 12 0.427 0.0009&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 56 12 0.427 0.0008&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 57 12 0.427 0.0008&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 58 12 0.427 0.0007&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 59 12 0.427 0.0006&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 60 12 0.427 0.0006&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 61 12 0.427 0.0005&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 62 12 0.427 0.0005&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 63 12 0.427 0.0004&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 64 12 0.427 0.0004&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 65 12 0.427 0.0004&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If &lt;code&gt;penalty&lt;/code&gt; were not specified, all of the &lt;code&gt;lambda&lt;/code&gt; values would be computed.&lt;/p&gt;
&lt;p&gt;To get the predictions for this specific value of &lt;code&gt;lambda&lt;/code&gt; (aka &lt;code&gt;penalty&lt;/code&gt;):&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# First, get the processed version of the test set predictors:&lt;/span&gt;
test_normalized &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;bake&lt;/span&gt;(norm_recipe, new_data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; ames_test, &lt;span style=&#34;color:#00f&#34;&gt;all_predictors&lt;/span&gt;())

test_results &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  test_results &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;rename&lt;/span&gt;(`random forest` &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; .pred) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;bind_cols&lt;/span&gt;(
    &lt;span style=&#34;color:#00f&#34;&gt;predict&lt;/span&gt;(glmn_fit, new_data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; test_normalized) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
      &lt;span style=&#34;color:#00f&#34;&gt;rename&lt;/span&gt;(glmnet &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; .pred)
  )
test_results
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 731 x 3&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;    Sale_Price `random forest` glmnet&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;         &amp;lt;dbl&amp;gt;           &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  1       5.33            5.22   5.27&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  2       5.02            5.21   5.17&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  3       5.27            5.25   5.23&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  4       5.60            5.51   5.25&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  5       5.28            5.24   5.25&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  6       5.17            5.19   5.19&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  7       5.02            4.97   5.19&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  8       5.46            5.50   5.49&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  9       5.44            5.46   5.48&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 10       5.33            5.50   5.47&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # … with 721 more rows&lt;/span&gt;

test_results &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;metrics&lt;/span&gt;(truth &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; Sale_Price, estimate &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; glmnet) 
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 3 x 3&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   .metric .estimator .estimate&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 1 rmse    standard      0.132 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 2 rsq     standard      0.410 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 3 mae     standard      0.0956&lt;/span&gt;

test_results &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;gather&lt;/span&gt;(model, prediction, &lt;span style=&#34;color:#666&#34;&gt;-&lt;/span&gt;Sale_Price) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;ggplot&lt;/span&gt;(&lt;span style=&#34;color:#00f&#34;&gt;aes&lt;/span&gt;(x &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; prediction, y &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; Sale_Price)) &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;geom_abline&lt;/span&gt;(col &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;green&amp;#34;&lt;/span&gt;, lty &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;2&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;geom_point&lt;/span&gt;(alpha &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;.4&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;facet_wrap&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;~&lt;/span&gt;model) &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;coord_fixed&lt;/span&gt;()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;figs/glmn-pred-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This final plot compares the performance of the random forest and regularized regression models.&lt;/p&gt;
&lt;h2 id=&#34;session-information&#34;&gt;Session information&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;#&amp;gt; ─ Session info ───────────────────────────────────────────────────────────────
#&amp;gt;  setting  value                       
#&amp;gt;  version  R version 3.6.2 (2019-12-12)
#&amp;gt;  os       macOS Mojave 10.14.6        
#&amp;gt;  system   x86_64, darwin15.6.0        
#&amp;gt;  ui       X11                         
#&amp;gt;  language (EN)                        
#&amp;gt;  collate  en_US.UTF-8                 
#&amp;gt;  ctype    en_US.UTF-8                 
#&amp;gt;  tz       America/Denver              
#&amp;gt;  date     2020-04-17                  
#&amp;gt; 
#&amp;gt; ─ Packages ───────────────────────────────────────────────────────────────────
#&amp;gt;  package      * version date       lib source        
#&amp;gt;  AmesHousing  * 0.0.3   2017-12-17 [1] CRAN (R 3.6.0)
#&amp;gt;  broom        * 0.5.5   2020-02-29 [1] CRAN (R 3.6.0)
#&amp;gt;  dials        * 0.0.6   2020-04-03 [1] CRAN (R 3.6.2)
#&amp;gt;  dplyr        * 0.8.5   2020-03-07 [1] CRAN (R 3.6.0)
#&amp;gt;  ggplot2      * 3.3.0   2020-03-05 [1] CRAN (R 3.6.0)
#&amp;gt;  glmnet       * 3.0-2   2019-12-11 [1] CRAN (R 3.6.0)
#&amp;gt;  infer        * 0.5.1   2019-11-19 [1] CRAN (R 3.6.0)
#&amp;gt;  parsnip      * 0.1.0   2020-04-09 [1] CRAN (R 3.6.2)
#&amp;gt;  purrr        * 0.3.3   2019-10-18 [1] CRAN (R 3.6.0)
#&amp;gt;  randomForest * 4.6-14  2018-03-25 [1] CRAN (R 3.6.0)
#&amp;gt;  ranger       * 0.12.1  2020-01-10 [1] CRAN (R 3.6.0)
#&amp;gt;  recipes      * 0.1.10  2020-03-18 [1] CRAN (R 3.6.0)
#&amp;gt;  rlang          0.4.5   2020-03-01 [1] CRAN (R 3.6.0)
#&amp;gt;  rsample      * 0.0.6   2020-03-31 [1] CRAN (R 3.6.2)
#&amp;gt;  tibble       * 2.1.3   2019-06-06 [1] CRAN (R 3.6.2)
#&amp;gt;  tidymodels   * 0.1.0   2020-02-16 [1] CRAN (R 3.6.0)
#&amp;gt;  tune         * 0.1.0   2020-04-02 [1] CRAN (R 3.6.2)
#&amp;gt;  workflows    * 0.1.1   2020-03-17 [1] CRAN (R 3.6.0)
#&amp;gt;  yardstick    * 0.0.6   2020-03-17 [1] CRAN (R 3.6.0)
#&amp;gt; 
#&amp;gt; [1] /Library/Frameworks/R.framework/Versions/3.6/Resources/library
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Classification models using a neural network</title>
      <link>https://nutriverse.io/learn/models/parsnip-nnet/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://nutriverse.io/learn/models/parsnip-nnet/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;To use the code in this article, you will need to install the following packages: keras and tidymodels. You will also need the python keras library installed (see &lt;code&gt;?keras::install_keras()&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;We can create classification models with the tidymodels package 
&lt;a href=&#34;https://tidymodels.github.io/parsnip/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;parsnip&lt;/a&gt; to predict categorical quantities or class labels. Here, let&amp;rsquo;s fit a single classification model using a neural network and evaluate using a validation set. While the 
&lt;a href=&#34;https://tidymodels.github.io/tune/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tune&lt;/a&gt; package has functionality to also do this, the parsnip package is the center of attention in this article so that we can better understand its usage.&lt;/p&gt;
&lt;h2 id=&#34;fitting-a-neural-network&#34;&gt;Fitting a neural network&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s fit a model to a small, two predictor classification data set. The data are in the modeldata package (part of tidymodels) and have been split into training, validation, and test data sets. In this analysis, the test set is left untouched; this article tries to emulate a good data usage methodology where the test set would only be evaluated once at the end after a variety of models have been considered.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;data&lt;/span&gt;(bivariate)
&lt;span style=&#34;color:#00f&#34;&gt;nrow&lt;/span&gt;(bivariate_train)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; [1] 1009&lt;/span&gt;
&lt;span style=&#34;color:#00f&#34;&gt;nrow&lt;/span&gt;(bivariate_val)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; [1] 300&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;A plot of the data shows two right-skewed predictors:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;ggplot&lt;/span&gt;(bivariate_train, &lt;span style=&#34;color:#00f&#34;&gt;aes&lt;/span&gt;(x &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; A, y &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; B, col &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; Class)) &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;geom_point&lt;/span&gt;(alpha &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;.2&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;figs/biv-plot-1.svg&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s use a single hidden layer neural network to predict the outcome. To do this, we transform the predictor columns to be more symmetric (via the &lt;code&gt;step_BoxCox()&lt;/code&gt; function) and on a common scale (using &lt;code&gt;step_normalize()&lt;/code&gt;). We can use 
&lt;a href=&#34;https://tidymodels.github.io/recipes/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;recipes&lt;/a&gt; to do so:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;biv_rec &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;recipe&lt;/span&gt;(Class &lt;span style=&#34;color:#666&#34;&gt;~&lt;/span&gt; ., data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; bivariate_train) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;step_BoxCox&lt;/span&gt;(&lt;span style=&#34;color:#00f&#34;&gt;all_predictors&lt;/span&gt;())&lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;step_normalize&lt;/span&gt;(&lt;span style=&#34;color:#00f&#34;&gt;all_predictors&lt;/span&gt;()) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;prep&lt;/span&gt;(training &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; bivariate_train, retain &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;TRUE&lt;/span&gt;)

&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# We will juice() to get the processed training set back&lt;/span&gt;

&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# For validation:&lt;/span&gt;
val_normalized &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;bake&lt;/span&gt;(biv_rec, new_data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; bivariate_val, &lt;span style=&#34;color:#00f&#34;&gt;all_predictors&lt;/span&gt;())
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# For testing when we arrive at a final model: &lt;/span&gt;
test_normalized &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;bake&lt;/span&gt;(biv_rec, new_data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; bivariate_test, &lt;span style=&#34;color:#00f&#34;&gt;all_predictors&lt;/span&gt;())
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We can use the keras package to fit a model with 5 hidden units and a 10% dropout rate, to regularize the model:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;set.seed&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;57974&lt;/span&gt;)
nnet_fit &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;mlp&lt;/span&gt;(epochs &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;100&lt;/span&gt;, hidden_units &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;5&lt;/span&gt;, dropout &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;0.1&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;set_mode&lt;/span&gt;(&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;classification&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# Also set engine-specific `verbose` argument to prevent logging the results: &lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;set_engine&lt;/span&gt;(&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;keras&amp;#34;&lt;/span&gt;, verbose &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;0&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;fit&lt;/span&gt;(Class &lt;span style=&#34;color:#666&#34;&gt;~&lt;/span&gt; ., data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;juice&lt;/span&gt;(biv_rec))

nnet_fit
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; parsnip model object&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Fit time:  8.7s &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Model: &amp;#34;sequential&amp;#34;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ________________________________________________________________________________&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Layer (type)                        Output Shape                    Param #     &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ================================================================================&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; dense (Dense)                       (None, 5)                       15          &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ________________________________________________________________________________&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; dense_1 (Dense)                     (None, 5)                       30          &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ________________________________________________________________________________&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; dropout (Dropout)                   (None, 5)                       0           &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ________________________________________________________________________________&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; dense_2 (Dense)                     (None, 2)                       12          &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ================================================================================&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Total params: 57&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Trainable params: 57&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Non-trainable params: 0&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ________________________________________________________________________________&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;model-performance&#34;&gt;Model performance&lt;/h2&gt;
&lt;p&gt;In parsnip, the &lt;code&gt;predict()&lt;/code&gt; function can be used to characterize performance on the validation set. Since parsnip always produces tibble outputs, these can just be column bound to the original data:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;val_results &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  bivariate_val &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;bind_cols&lt;/span&gt;(
    &lt;span style=&#34;color:#00f&#34;&gt;predict&lt;/span&gt;(nnet_fit, new_data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; val_normalized),
    &lt;span style=&#34;color:#00f&#34;&gt;predict&lt;/span&gt;(nnet_fit, new_data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; val_normalized, type &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;prob&amp;#34;&lt;/span&gt;)
  )
val_results &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;slice&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;5&lt;/span&gt;)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 5 x 6&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;       A     B Class .pred_class .pred_One .pred_Two&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt;           &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 1 1061.  74.5 One   Two             0.473    0.527 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 2 1241.  83.4 One   Two             0.484    0.516 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 3  939.  71.9 One   One             0.636    0.364 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 4  813.  77.1 One   One             0.925    0.0746&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 5 1706.  92.8 Two   Two             0.355    0.645&lt;/span&gt;

val_results &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;roc_auc&lt;/span&gt;(truth &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; Class, .pred_One)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 1 x 3&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   .metric .estimator .estimate&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 1 roc_auc binary         0.815&lt;/span&gt;

val_results &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;accuracy&lt;/span&gt;(truth &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; Class, .pred_class)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 1 x 3&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   .metric  .estimator .estimate&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 1 accuracy binary         0.737&lt;/span&gt;

val_results &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;conf_mat&lt;/span&gt;(truth &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; Class, .pred_class)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;           Truth&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Prediction One Two&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;        One 150  27&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;        Two  52  71&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Let&amp;rsquo;s also create a grid to get a visual sense of the class boundary for the validation set.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;a_rng &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;range&lt;/span&gt;(bivariate_train&lt;span style=&#34;color:#666&#34;&gt;$&lt;/span&gt;A)
b_rng &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;range&lt;/span&gt;(bivariate_train&lt;span style=&#34;color:#666&#34;&gt;$&lt;/span&gt;B)
x_grid &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;expand.grid&lt;/span&gt;(A &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;seq&lt;/span&gt;(a_rng[1], a_rng[2], length.out &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;100&lt;/span&gt;),
              B &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;seq&lt;/span&gt;(b_rng[1], b_rng[2], length.out &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;100&lt;/span&gt;))
x_grid_trans &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;bake&lt;/span&gt;(biv_rec, x_grid)

&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# Make predictions using the transformed predictors but &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# attach them to the predictors in the original units: &lt;/span&gt;
x_grid &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  x_grid &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;bind_cols&lt;/span&gt;(&lt;span style=&#34;color:#00f&#34;&gt;predict&lt;/span&gt;(nnet_fit, x_grid_trans, type &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;prob&amp;#34;&lt;/span&gt;))

&lt;span style=&#34;color:#00f&#34;&gt;ggplot&lt;/span&gt;(x_grid, &lt;span style=&#34;color:#00f&#34;&gt;aes&lt;/span&gt;(x &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; A, y &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; B)) &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;geom_contour&lt;/span&gt;(&lt;span style=&#34;color:#00f&#34;&gt;aes&lt;/span&gt;(z &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; .pred_One), breaks &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;.5&lt;/span&gt;, col &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;black&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;geom_point&lt;/span&gt;(data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; bivariate_val, &lt;span style=&#34;color:#00f&#34;&gt;aes&lt;/span&gt;(col &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; Class), alpha &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;0.3&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;figs/biv-boundary-1.svg&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;session-information&#34;&gt;Session information&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;#&amp;gt; ─ Session info ───────────────────────────────────────────────────────────────
#&amp;gt;  setting  value                       
#&amp;gt;  version  R version 3.6.2 (2019-12-12)
#&amp;gt;  os       macOS Mojave 10.14.6        
#&amp;gt;  system   x86_64, darwin15.6.0        
#&amp;gt;  ui       X11                         
#&amp;gt;  language (EN)                        
#&amp;gt;  collate  en_US.UTF-8                 
#&amp;gt;  ctype    en_US.UTF-8                 
#&amp;gt;  tz       America/Denver              
#&amp;gt;  date     2020-04-17                  
#&amp;gt; 
#&amp;gt; ─ Packages ───────────────────────────────────────────────────────────────────
#&amp;gt;  package    * version date       lib source        
#&amp;gt;  broom      * 0.5.5   2020-02-29 [1] CRAN (R 3.6.0)
#&amp;gt;  dials      * 0.0.6   2020-04-03 [1] CRAN (R 3.6.2)
#&amp;gt;  dplyr      * 0.8.5   2020-03-07 [1] CRAN (R 3.6.0)
#&amp;gt;  ggplot2    * 3.3.0   2020-03-05 [1] CRAN (R 3.6.0)
#&amp;gt;  infer      * 0.5.1   2019-11-19 [1] CRAN (R 3.6.0)
#&amp;gt;  keras        2.2.5.0 2019-10-08 [1] CRAN (R 3.6.0)
#&amp;gt;  parsnip    * 0.1.0   2020-04-09 [1] CRAN (R 3.6.2)
#&amp;gt;  purrr      * 0.3.3   2019-10-18 [1] CRAN (R 3.6.0)
#&amp;gt;  recipes    * 0.1.10  2020-03-18 [1] CRAN (R 3.6.0)
#&amp;gt;  rlang        0.4.5   2020-03-01 [1] CRAN (R 3.6.0)
#&amp;gt;  rsample    * 0.0.6   2020-03-31 [1] CRAN (R 3.6.2)
#&amp;gt;  tibble     * 2.1.3   2019-06-06 [1] CRAN (R 3.6.2)
#&amp;gt;  tidymodels * 0.1.0   2020-02-16 [1] CRAN (R 3.6.0)
#&amp;gt;  tune       * 0.1.0   2020-04-02 [1] CRAN (R 3.6.2)
#&amp;gt;  workflows  * 0.1.1   2020-03-17 [1] CRAN (R 3.6.0)
#&amp;gt;  yardstick  * 0.0.6   2020-03-17 [1] CRAN (R 3.6.0)
#&amp;gt; 
#&amp;gt; [1] /Library/Frameworks/R.framework/Versions/3.6/Resources/library
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Nested resampling</title>
      <link>https://nutriverse.io/learn/work/nested-resampling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://nutriverse.io/learn/work/nested-resampling/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;To use the code in this article, you will need to install the following packages: furrr, kernlab, mlbench, scales, and tidymodels.&lt;/p&gt;
&lt;p&gt;In this article, we discuss an alternative method for evaluating and tuning models, called 
&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C7&amp;amp;q=%22nested&amp;#43;resampling%22&amp;#43;inner&amp;#43;outer&amp;amp;btnG=&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;nested resampling&lt;/a&gt;. While it is more computationally taxing and challenging to implement than other resampling methods, it has the potential to produce better estimates of model performance.&lt;/p&gt;
&lt;h2 id=&#34;resampling-models&#34;&gt;Resampling models&lt;/h2&gt;
&lt;p&gt;A typical scheme for splitting the data when developing a predictive model is to create an initial split of the data into a training and test set. If resampling is used, it is executed on the training set. A series of binary splits is created. In rsample, we use the term &lt;em&gt;analysis set&lt;/em&gt; for the data that are used to fit the model and the term &lt;em&gt;assessment set&lt;/em&gt; for the set used to compute performance:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;figs/resampling.svg&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A common method for tuning models is 
&lt;a href=&#34;https://nutriverse.io/learn/work/tune-svm/&#34;&gt;grid search&lt;/a&gt; where a candidate set of tuning parameters is created. The full set of models for every combination of the tuning parameter grid and the resamples is fitted. Each time, the assessment data are used to measure performance and the average value is determined for each tuning parameter.&lt;/p&gt;
&lt;p&gt;The potential problem is that once we pick the tuning parameter associated with the best performance, this performance value is usually quoted as the performance of the model. There is serious potential for &lt;em&gt;optimization bias&lt;/em&gt; since we use the same data to tune the model and to assess performance. This would result in an optimistic estimate of performance.&lt;/p&gt;
&lt;p&gt;Nested resampling uses an additional layer of resampling that separates the tuning activities from the process used to estimate the efficacy of the model. An &lt;em&gt;outer&lt;/em&gt; resampling scheme is used and, for every split in the outer resample, another full set of resampling splits are created on the original analysis set. For example, if 10-fold cross-validation is used on the outside and 5-fold cross-validation on the inside, a total of 500 models will be fit. The parameter tuning will be conducted 10 times and the best parameters are determined from the average of the 5 assessment sets. This process occurs 10 times.&lt;/p&gt;
&lt;p&gt;Once the tuning results are complete, a model is fit to each of the outer resampling splits using the best parameter associated with that resample. The average of the outer method&amp;rsquo;s assessment sets are a unbiased estimate of the model.&lt;/p&gt;
&lt;p&gt;We will simulate some regression data to illustrate the methods. The mlbench package has a function &lt;code&gt;mlbench::mlbench.friedman1()&lt;/code&gt; that can simulate a complex regression data structure from the 
&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;q=%22Multivariate&amp;#43;adaptive&amp;#43;regression&amp;#43;splines%22&amp;amp;btnG=&amp;amp;as_sdt=1%2C7&amp;amp;as_sdtp=&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;original MARS publication&lt;/a&gt;. A training set size of 100 data points are generated as well as a large set that will be used to characterize how well the resampling procedure performed.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;library&lt;/span&gt;(mlbench)
sim_data &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;function&lt;/span&gt;(n) {
  tmp &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;mlbench.friedman1&lt;/span&gt;(n, sd &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;)
  tmp &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;cbind&lt;/span&gt;(tmp&lt;span style=&#34;color:#666&#34;&gt;$&lt;/span&gt;x, tmp&lt;span style=&#34;color:#666&#34;&gt;$&lt;/span&gt;y)
  tmp &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;as.data.frame&lt;/span&gt;(tmp)
  &lt;span style=&#34;color:#00f&#34;&gt;names&lt;/span&gt;(tmp)&lt;span style=&#34;color:#00f&#34;&gt;[ncol&lt;/span&gt;(tmp)] &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;y&amp;#34;&lt;/span&gt;
  tmp
}

&lt;span style=&#34;color:#00f&#34;&gt;set.seed&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;9815&lt;/span&gt;)
train_dat &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;sim_data&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;100&lt;/span&gt;)
large_dat &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;sim_data&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;10&lt;/span&gt;^5)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;nested-resampling&#34;&gt;Nested resampling&lt;/h2&gt;
&lt;p&gt;To get started, the types of resampling methods need to be specified. This isn&amp;rsquo;t a large data set, so 5 repeats of 10-fold cross validation will be used as the &lt;em&gt;outer&lt;/em&gt; resampling method for generating the estimate of overall performance. To tune the model, it would be good to have precise estimates for each of the values of the tuning parameter so let&amp;rsquo;s use 25 iterations of the bootstrap. This means that there will eventually be &lt;code&gt;5 * 10 * 25 = 1250&lt;/code&gt; models that are fit to the data &lt;em&gt;per tuning parameter&lt;/em&gt;. These models will be discarded once the performance of the model has been quantified.&lt;/p&gt;
&lt;p&gt;To create the tibble with the resampling specifications:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;library&lt;/span&gt;(tidymodels)
results &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;nested_cv&lt;/span&gt;(train_dat, 
                     outside &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;vfold_cv&lt;/span&gt;(repeats &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;5&lt;/span&gt;), 
                     inside &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;bootstraps&lt;/span&gt;(times &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;25&lt;/span&gt;))
results
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; [1] &amp;#34;nested_cv&amp;#34;  &amp;#34;vfold_cv&amp;#34;   &amp;#34;rset&amp;#34;       &amp;#34;tbl_df&amp;#34;     &amp;#34;tbl&amp;#34;       &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; [6] &amp;#34;data.frame&amp;#34;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # Nested resampling:&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; #  outer: 10-fold cross-validation repeated 5 times&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; #  inner: Bootstrap sampling&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 50 x 4&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;    splits          id      id2    inner_resamples  &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;    &amp;lt;named list&amp;gt;    &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;  &amp;lt;named list&amp;gt;     &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  1 &amp;lt;split [90/10]&amp;gt; Repeat1 Fold01 &amp;lt;tibble [25 × 2]&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  2 &amp;lt;split [90/10]&amp;gt; Repeat1 Fold02 &amp;lt;tibble [25 × 2]&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  3 &amp;lt;split [90/10]&amp;gt; Repeat1 Fold03 &amp;lt;tibble [25 × 2]&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  4 &amp;lt;split [90/10]&amp;gt; Repeat1 Fold04 &amp;lt;tibble [25 × 2]&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  5 &amp;lt;split [90/10]&amp;gt; Repeat1 Fold05 &amp;lt;tibble [25 × 2]&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  6 &amp;lt;split [90/10]&amp;gt; Repeat1 Fold06 &amp;lt;tibble [25 × 2]&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  7 &amp;lt;split [90/10]&amp;gt; Repeat1 Fold07 &amp;lt;tibble [25 × 2]&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  8 &amp;lt;split [90/10]&amp;gt; Repeat1 Fold08 &amp;lt;tibble [25 × 2]&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  9 &amp;lt;split [90/10]&amp;gt; Repeat1 Fold09 &amp;lt;tibble [25 × 2]&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 10 &amp;lt;split [90/10]&amp;gt; Repeat1 Fold10 &amp;lt;tibble [25 × 2]&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # … with 40 more rows&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The splitting information for each resample is contained in the &lt;code&gt;split&lt;/code&gt; objects. Focusing on the second fold of the first repeat:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;results&lt;span style=&#34;color:#666&#34;&gt;$&lt;/span&gt;splits[[2]]
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &amp;lt;Training/Validation/Total&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &amp;lt;90/10/100&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;&amp;lt;90/10/100&amp;gt;&lt;/code&gt; indicates the number of observations in the analysis set, assessment set, and the original data.&lt;/p&gt;
&lt;p&gt;Each element of &lt;code&gt;inner_resamples&lt;/code&gt; has its own tibble with the bootstrapping splits.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;results&lt;span style=&#34;color:#666&#34;&gt;$&lt;/span&gt;inner_resamples[[5]]
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # Bootstrap sampling &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 25 x 2&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;    splits          id         &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;    &amp;lt;list&amp;gt;          &amp;lt;chr&amp;gt;      &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  1 &amp;lt;split [90/31]&amp;gt; Bootstrap01&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  2 &amp;lt;split [90/33]&amp;gt; Bootstrap02&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  3 &amp;lt;split [90/37]&amp;gt; Bootstrap03&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  4 &amp;lt;split [90/31]&amp;gt; Bootstrap04&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  5 &amp;lt;split [90/32]&amp;gt; Bootstrap05&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  6 &amp;lt;split [90/32]&amp;gt; Bootstrap06&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  7 &amp;lt;split [90/36]&amp;gt; Bootstrap07&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  8 &amp;lt;split [90/34]&amp;gt; Bootstrap08&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  9 &amp;lt;split [90/29]&amp;gt; Bootstrap09&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 10 &amp;lt;split [90/31]&amp;gt; Bootstrap10&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # … with 15 more rows&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;These are self-contained, meaning that the bootstrap sample is aware that it is a sample of a specific 90% of the data:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;results&lt;span style=&#34;color:#666&#34;&gt;$&lt;/span&gt;inner_resamples[[5]]&lt;span style=&#34;color:#666&#34;&gt;$&lt;/span&gt;splits[[1]]
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &amp;lt;Training/Validation/Total&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &amp;lt;90/31/90&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;To start, we need to define how the model will be created and measured. Let&amp;rsquo;s use a radial basis support vector machine model via the function &lt;code&gt;kernlab::ksvm&lt;/code&gt;. This model is generally considered to have &lt;em&gt;two&lt;/em&gt; tuning parameters: the SVM cost value and the kernel parameter &lt;code&gt;sigma&lt;/code&gt;. For illustration purposes here, only the cost value will be tuned and the function &lt;code&gt;kernlab::sigest&lt;/code&gt; will be used to estimate &lt;code&gt;sigma&lt;/code&gt; during each model fit. This is automatically done by &lt;code&gt;ksvm&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;After the model is fit to the analysis set, the root-mean squared error (RMSE) is computed on the assessment set. &lt;strong&gt;One important note:&lt;/strong&gt; for this model, it is critical to center and scale the predictors before computing dot products. We don&amp;rsquo;t do this operation here because &lt;code&gt;mlbench.friedman1&lt;/code&gt; simulates all of the predictors to be standardized uniform random variables.&lt;/p&gt;
&lt;p&gt;Our function to fit the model and compute the RMSE is:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;library&lt;/span&gt;(kernlab)

&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# `object` will be an `rsplit` object from our `results` tibble&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# `cost` is the tuning parameter&lt;/span&gt;
svm_rmse &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;function&lt;/span&gt;(object, cost &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;) {
  y_col &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;ncol&lt;/span&gt;(object&lt;span style=&#34;color:#666&#34;&gt;$&lt;/span&gt;data)
  mod &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
    &lt;span style=&#34;color:#00f&#34;&gt;svm_rbf&lt;/span&gt;(mode &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;regression&amp;#34;&lt;/span&gt;, cost &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; cost) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
    &lt;span style=&#34;color:#00f&#34;&gt;set_engine&lt;/span&gt;(&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;kernlab&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
    &lt;span style=&#34;color:#00f&#34;&gt;fit&lt;/span&gt;(y &lt;span style=&#34;color:#666&#34;&gt;~&lt;/span&gt; ., data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;analysis&lt;/span&gt;(object))
  
  holdout_pred &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
    &lt;span style=&#34;color:#00f&#34;&gt;predict&lt;/span&gt;(mod, &lt;span style=&#34;color:#00f&#34;&gt;assessment&lt;/span&gt;(object) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; dplyr&lt;span style=&#34;color:#666&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#00f&#34;&gt;select&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;-&lt;/span&gt;y)) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
    &lt;span style=&#34;color:#00f&#34;&gt;bind_cols&lt;/span&gt;(&lt;span style=&#34;color:#00f&#34;&gt;assessment&lt;/span&gt;(object) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; dplyr&lt;span style=&#34;color:#666&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#00f&#34;&gt;select&lt;/span&gt;(y))
  &lt;span style=&#34;color:#00f&#34;&gt;rmse&lt;/span&gt;(holdout_pred, truth &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; y, estimate &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; .pred)&lt;span style=&#34;color:#666&#34;&gt;$&lt;/span&gt;.estimate
}

&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# In some case, we want to parameterize the function over the tuning parameter:&lt;/span&gt;
rmse_wrapper &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;function&lt;/span&gt;(cost, object) &lt;span style=&#34;color:#00f&#34;&gt;svm_rmse&lt;/span&gt;(object, cost)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;For the nested resampling, a model needs to be fit for each tuning parameter and each bootstrap split. To do this, create a wrapper:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# `object` will be an `rsplit` object for the bootstrap samples&lt;/span&gt;
tune_over_cost &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;function&lt;/span&gt;(object) {
  &lt;span style=&#34;color:#00f&#34;&gt;tibble&lt;/span&gt;(cost &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;2&lt;/span&gt; ^ &lt;span style=&#34;color:#00f&#34;&gt;seq&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;-2&lt;/span&gt;, &lt;span style=&#34;color:#666&#34;&gt;8&lt;/span&gt;, by &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;)) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
    &lt;span style=&#34;color:#00f&#34;&gt;mutate&lt;/span&gt;(RMSE &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;map_dbl&lt;/span&gt;(cost, rmse_wrapper, object &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; object))
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Since this will be called across the set of outer cross-validation splits, another wrapper is required:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# `object` is an `rsplit` object in `results$inner_resamples` &lt;/span&gt;
summarize_tune_results &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;function&lt;/span&gt;(object) {
  &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# Return row-bound tibble that has the 25 bootstrap results&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;map_df&lt;/span&gt;(object&lt;span style=&#34;color:#666&#34;&gt;$&lt;/span&gt;splits, tune_over_cost) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
    &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# For each value of the tuning parameter, compute the &lt;/span&gt;
    &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# average RMSE which is the inner bootstrap estimate. &lt;/span&gt;
    &lt;span style=&#34;color:#00f&#34;&gt;group_by&lt;/span&gt;(cost) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
    &lt;span style=&#34;color:#00f&#34;&gt;summarize&lt;/span&gt;(mean_RMSE &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;mean&lt;/span&gt;(RMSE, na.rm &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;TRUE&lt;/span&gt;),
              n &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;length&lt;/span&gt;(RMSE))
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now that those functions are defined, we can execute all the inner resampling loops:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;tuning_results &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;map&lt;/span&gt;(results&lt;span style=&#34;color:#666&#34;&gt;$&lt;/span&gt;inner_resamples, summarize_tune_results) 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Alternatively, since these computations can be run in parallel, we can use the furrr package. Instead of using &lt;code&gt;map()&lt;/code&gt;, the function &lt;code&gt;future_map()&lt;/code&gt; parallelizes the iterations using the 
&lt;a href=&#34;https://cran.r-project.org/web/packages/future/vignettes/future-1-overview.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;future package&lt;/a&gt;. The &lt;code&gt;multisession&lt;/code&gt; plan uses the local cores to process the inner resampling loop. The end results are the same as the sequential computations.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;library&lt;/span&gt;(furrr)
&lt;span style=&#34;color:#00f&#34;&gt;plan&lt;/span&gt;(multisession)

tuning_results &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;future_map&lt;/span&gt;(results&lt;span style=&#34;color:#666&#34;&gt;$&lt;/span&gt;inner_resamples, summarize_tune_results) 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The object &lt;code&gt;tuning_results&lt;/code&gt; is a list of data frames for each of the 50 outer resamples.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s make a plot of the averaged results to see what the relationship is between the RMSE and the tuning parameters for each of the inner bootstrapping operations:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;library&lt;/span&gt;(scales)

pooled_inner &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; tuning_results &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; bind_rows

best_cost &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;function&lt;/span&gt;(dat) dat&lt;span style=&#34;color:#00f&#34;&gt;[which.min&lt;/span&gt;(dat&lt;span style=&#34;color:#666&#34;&gt;$&lt;/span&gt;mean_RMSE),]

p &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;ggplot&lt;/span&gt;(pooled_inner, &lt;span style=&#34;color:#00f&#34;&gt;aes&lt;/span&gt;(x &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; cost, y &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; mean_RMSE)) &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;scale_x_continuous&lt;/span&gt;(trans &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;log2&amp;#39;&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;xlab&lt;/span&gt;(&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;SVM Cost&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;ylab&lt;/span&gt;(&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;Inner RMSE&amp;#34;&lt;/span&gt;)

&lt;span style=&#34;color:#00f&#34;&gt;for &lt;/span&gt;(i in &lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#00f&#34;&gt;length&lt;/span&gt;(tuning_results))
  p &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; p  &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;geom_line&lt;/span&gt;(data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; tuning_results[[i]], alpha &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;.2&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;geom_point&lt;/span&gt;(data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;best_cost&lt;/span&gt;(tuning_results[[i]]), pch &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;16&lt;/span&gt;, alpha &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;3&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;4&lt;/span&gt;)

p &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; p &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;geom_smooth&lt;/span&gt;(data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; pooled_inner, se &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;FALSE&lt;/span&gt;)
p
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;figs/rmse-plot-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Each gray line is a separate bootstrap resampling curve created from a different 90% of the data. The blue line is a LOESS smooth of all the results pooled together.&lt;/p&gt;
&lt;p&gt;To determine the best parameter estimate for each of the outer resampling iterations:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;cost_vals &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  tuning_results &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;map_df&lt;/span&gt;(best_cost) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;select&lt;/span&gt;(cost)

results &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;bind_cols&lt;/span&gt;(results, cost_vals) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;mutate&lt;/span&gt;(cost &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;factor&lt;/span&gt;(cost, levels &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;paste&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;2&lt;/span&gt; ^ &lt;span style=&#34;color:#00f&#34;&gt;seq&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;-2&lt;/span&gt;, &lt;span style=&#34;color:#666&#34;&gt;8&lt;/span&gt;, by &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;))))

&lt;span style=&#34;color:#00f&#34;&gt;ggplot&lt;/span&gt;(results, &lt;span style=&#34;color:#00f&#34;&gt;aes&lt;/span&gt;(x &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; cost)) &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;geom_bar&lt;/span&gt;() &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;xlab&lt;/span&gt;(&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;SVM Cost&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;scale_x_discrete&lt;/span&gt;(drop &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;FALSE&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;figs/choose-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Most of the resamples produced an optimal cost value of 2.0, but the distribution is right-skewed due to the flat trend in the resampling profile once the cost value becomes 10 or larger.&lt;/p&gt;
&lt;p&gt;Now that we have these estimates, we can compute the outer resampling results for each of the 50 splits using the corresponding tuning parameter value:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;results &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  results &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;mutate&lt;/span&gt;(RMSE &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;map2_dbl&lt;/span&gt;(splits, cost, svm_rmse))

&lt;span style=&#34;color:#00f&#34;&gt;summary&lt;/span&gt;(results&lt;span style=&#34;color:#666&#34;&gt;$&lt;/span&gt;RMSE)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;    1.57    2.09    2.68    2.69    3.25    4.25&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The estimated RMSE for the model tuning process is 2.69.&lt;/p&gt;
&lt;p&gt;What is the RMSE estimate for the non-nested procedure when only the outer resampling method is used? For each cost value in the tuning grid, 50 SVM models are fit and their RMSE values are averaged. The table of cost values and mean RMSE estimates is used to determine the best cost value. The associated RMSE is the biased estimate.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;not_nested &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;map&lt;/span&gt;(results&lt;span style=&#34;color:#666&#34;&gt;$&lt;/span&gt;splits, tune_over_cost) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  bind_rows

outer_summary &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; not_nested &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;group_by&lt;/span&gt;(cost) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;summarize&lt;/span&gt;(outer_RMSE &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;mean&lt;/span&gt;(RMSE), n &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;length&lt;/span&gt;(RMSE))

outer_summary
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 11 x 3&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;      cost outer_RMSE     n&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;     &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  1   0.25       3.54    50&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  2   0.5        3.11    50&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  3   1          2.77    50&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  4   2          2.62    50&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  5   4          2.65    50&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  6   8          2.75    50&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  7  16          2.82    50&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  8  32          2.82    50&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  9  64          2.83    50&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 10 128          2.83    50&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 11 256          2.82    50&lt;/span&gt;

&lt;span style=&#34;color:#00f&#34;&gt;ggplot&lt;/span&gt;(outer_summary, &lt;span style=&#34;color:#00f&#34;&gt;aes&lt;/span&gt;(x &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; cost, y &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; outer_RMSE)) &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;geom_point&lt;/span&gt;() &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;geom_line&lt;/span&gt;() &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;scale_x_continuous&lt;/span&gt;(trans &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;log2&amp;#39;&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;xlab&lt;/span&gt;(&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;SVM Cost&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;ylab&lt;/span&gt;(&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;RMSE&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;figs/not-nested-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The non-nested procedure estimates the RMSE to be 2.62. Both estimates are fairly close.&lt;/p&gt;
&lt;p&gt;The approximately true RMSE for an SVM model with a cost value of 2.0 can be approximated with the large sample that was simulated at the beginning.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;finalModel &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;ksvm&lt;/span&gt;(y &lt;span style=&#34;color:#666&#34;&gt;~&lt;/span&gt; ., data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; train_dat, C &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;2&lt;/span&gt;)
large_pred &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;predict&lt;/span&gt;(finalModel, large_dat[, &lt;span style=&#34;color:#666&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#00f&#34;&gt;ncol&lt;/span&gt;(large_dat)])
&lt;span style=&#34;color:#00f&#34;&gt;sqrt&lt;/span&gt;(&lt;span style=&#34;color:#00f&#34;&gt;mean&lt;/span&gt;((large_dat&lt;span style=&#34;color:#666&#34;&gt;$&lt;/span&gt;y &lt;span style=&#34;color:#666&#34;&gt;-&lt;/span&gt; large_pred) ^ &lt;span style=&#34;color:#666&#34;&gt;2&lt;/span&gt;, na.rm &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;TRUE&lt;/span&gt;))
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; [1] 2.71&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The nested procedure produces a closer estimate to the approximate truth but the non-nested estimate is very similar.&lt;/p&gt;
&lt;h2 id=&#34;session-information&#34;&gt;Session information&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;#&amp;gt; ─ Session info ───────────────────────────────────────────────────────────────
#&amp;gt;  setting  value                       
#&amp;gt;  version  R version 3.6.2 (2019-12-12)
#&amp;gt;  os       macOS Mojave 10.14.6        
#&amp;gt;  system   x86_64, darwin15.6.0        
#&amp;gt;  ui       X11                         
#&amp;gt;  language (EN)                        
#&amp;gt;  collate  en_US.UTF-8                 
#&amp;gt;  ctype    en_US.UTF-8                 
#&amp;gt;  tz       America/Denver              
#&amp;gt;  date     2020-04-17                  
#&amp;gt; 
#&amp;gt; ─ Packages ───────────────────────────────────────────────────────────────────
#&amp;gt;  package    * version date       lib source        
#&amp;gt;  broom      * 0.5.5   2020-02-29 [1] CRAN (R 3.6.0)
#&amp;gt;  dials      * 0.0.6   2020-04-03 [1] CRAN (R 3.6.2)
#&amp;gt;  dplyr      * 0.8.5   2020-03-07 [1] CRAN (R 3.6.0)
#&amp;gt;  furrr      * 0.1.0   2018-05-16 [1] CRAN (R 3.6.0)
#&amp;gt;  ggplot2    * 3.3.0   2020-03-05 [1] CRAN (R 3.6.0)
#&amp;gt;  infer      * 0.5.1   2019-11-19 [1] CRAN (R 3.6.0)
#&amp;gt;  kernlab    * 0.9-29  2019-11-12 [1] CRAN (R 3.6.0)
#&amp;gt;  mlbench    * 2.1-1   2012-07-10 [1] CRAN (R 3.6.0)
#&amp;gt;  parsnip    * 0.1.0   2020-04-09 [1] CRAN (R 3.6.2)
#&amp;gt;  purrr      * 0.3.3   2019-10-18 [1] CRAN (R 3.6.0)
#&amp;gt;  recipes    * 0.1.10  2020-03-18 [1] CRAN (R 3.6.0)
#&amp;gt;  rlang        0.4.5   2020-03-01 [1] CRAN (R 3.6.0)
#&amp;gt;  rsample    * 0.0.6   2020-03-31 [1] CRAN (R 3.6.2)
#&amp;gt;  scales     * 1.1.0   2019-11-18 [1] CRAN (R 3.6.0)
#&amp;gt;  tibble     * 2.1.3   2019-06-06 [1] CRAN (R 3.6.2)
#&amp;gt;  tidymodels * 0.1.0   2020-02-16 [1] CRAN (R 3.6.0)
#&amp;gt;  tune       * 0.1.0   2020-04-02 [1] CRAN (R 3.6.2)
#&amp;gt;  workflows  * 0.1.1   2020-03-17 [1] CRAN (R 3.6.0)
#&amp;gt;  yardstick  * 0.0.6   2020-03-17 [1] CRAN (R 3.6.0)
#&amp;gt; 
#&amp;gt; [1] /Library/Frameworks/R.framework/Versions/3.6/Resources/library
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Preprocess your data with recipes</title>
      <link>https://nutriverse.io/start/recipes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://nutriverse.io/start/recipes/</guid>
      <description>&lt;h2 id=&#34;intro&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In our 
&lt;a href=&#34;https://nutriverse.io/start/models/&#34;&gt;&lt;em&gt;Build a Model&lt;/em&gt;&lt;/a&gt; article, we learned how to specify and train models with different engines using the 
&lt;a href=&#34;https://tidymodels.github.io/parsnip/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;parsnip package&lt;/a&gt;. In this article, we&amp;rsquo;ll explore another tidymodels package, 
&lt;a href=&#34;https://tidymodels.github.io/recipes/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;recipes&lt;/a&gt;, which is designed to help you preprocess your data &lt;em&gt;before&lt;/em&gt; training your model. Recipes are built as a series of preprocessing steps, such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;converting qualitative predictors to indicator variables (also known as dummy variables),&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;transforming data to be on a different scale (e.g., taking the logarithm of a variable),&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;transforming whole groups of predictors together,&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;extracting key features from raw variables (e.g., getting the day of the week out of a date variable),&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;and so on. If you are familiar with R&amp;rsquo;s formula interface, a lot of this might sound familiar and like what a formula already does. Recipes can be used to do many of the same things, but they have a much wider range of possibilities. This article shows how to use recipes for modeling.&lt;/p&gt;
&lt;p&gt;To use code in this article,  you will need to install the following packages: nycflights13, skimr, and tidymodels.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;library&lt;/span&gt;(tidymodels)      &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# for the recipes package, along with the rest of tidymodels&lt;/span&gt;

&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# Helper packages&lt;/span&gt;
&lt;span style=&#34;color:#00f&#34;&gt;library&lt;/span&gt;(nycflights13)    &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# for flight data&lt;/span&gt;
&lt;span style=&#34;color:#00f&#34;&gt;library&lt;/span&gt;(skimr)           &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# for variable summaries&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;data&#34;&gt;The New York City flight data&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s use the 
&lt;a href=&#34;https://github.com/hadley/nycflights13&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;nycflights13 data&lt;/a&gt; to predict whether a plane arrives more than 30 minutes late. This data set contains information on 325,819 flights departing near New York City in 2013. Let&amp;rsquo;s start by loading the data and making a few changes to the variables:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;set.seed&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;123&lt;/span&gt;)

flight_data &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  flights &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;mutate&lt;/span&gt;(
    &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# Convert the arrival delay to a factor&lt;/span&gt;
    arr_delay &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;ifelse&lt;/span&gt;(arr_delay &lt;span style=&#34;color:#666&#34;&gt;&amp;gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;30&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;late&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;on_time&amp;#34;&lt;/span&gt;),
    arr_delay &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;factor&lt;/span&gt;(arr_delay),
    &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# We will use the date (not date-time) in the recipe below&lt;/span&gt;
    date &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;as.Date&lt;/span&gt;(time_hour)
  ) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# Include the weather data&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;inner_join&lt;/span&gt;(weather, by &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;origin&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;time_hour&amp;#34;&lt;/span&gt;)) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# Only retain the specific columns we will use&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;select&lt;/span&gt;(dep_time, flight, origin, dest, air_time, distance, 
         carrier, date, arr_delay, time_hour) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# Exclude missing data&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;na.omit&lt;/span&gt;() &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# For creating models, it is better to have qualitative columns&lt;/span&gt;
  &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# encoded as factors (instead of character strings)&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;mutate_if&lt;/span&gt;(is.character, as.factor)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We can see that about 16% of the flights in this data set arrived more than 30 minutes late.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;flight_data &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;count&lt;/span&gt;(arr_delay) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;mutate&lt;/span&gt;(prop &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; n&lt;span style=&#34;color:#666&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#00f&#34;&gt;sum&lt;/span&gt;(n))
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 2 x 3&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   arr_delay      n  prop&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   &amp;lt;fct&amp;gt;      &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 1 late       52540 0.161&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 2 on_time   273279 0.839&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Before we start building up our recipe, let&amp;rsquo;s take a quick look at a few specific variables that will be important for both preprocessing and modeling.&lt;/p&gt;
&lt;p&gt;First, notice that the variable we created called &lt;code&gt;arr_delay&lt;/code&gt; is a factor variable; it is important that our outcome variable for training a logistic regression model is a factor.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;glimpse&lt;/span&gt;(flight_data)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Observations: 325,819&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Variables: 10&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; $ dep_time  &amp;lt;int&amp;gt; 517, 533, 542, 544, 554, 554, 555, 557, 557, 558, 558, 558,…&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; $ flight    &amp;lt;int&amp;gt; 1545, 1714, 1141, 725, 461, 1696, 507, 5708, 79, 301, 49, 7…&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; $ origin    &amp;lt;fct&amp;gt; EWR, LGA, JFK, JFK, LGA, EWR, EWR, LGA, JFK, LGA, JFK, JFK,…&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; $ dest      &amp;lt;fct&amp;gt; IAH, IAH, MIA, BQN, ATL, ORD, FLL, IAD, MCO, ORD, PBI, TPA,…&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; $ air_time  &amp;lt;dbl&amp;gt; 227, 227, 160, 183, 116, 150, 158, 53, 140, 138, 149, 158, …&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; $ distance  &amp;lt;dbl&amp;gt; 1400, 1416, 1089, 1576, 762, 719, 1065, 229, 944, 733, 1028…&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; $ carrier   &amp;lt;fct&amp;gt; UA, UA, AA, B6, DL, UA, B6, EV, B6, AA, B6, B6, UA, UA, AA,…&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; $ date      &amp;lt;date&amp;gt; 2013-01-01, 2013-01-01, 2013-01-01, 2013-01-01, 2013-01-01…&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; $ arr_delay &amp;lt;fct&amp;gt; on_time, on_time, late, on_time, on_time, on_time, on_time,…&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; $ time_hour &amp;lt;dttm&amp;gt; 2013-01-01 05:00:00, 2013-01-01 05:00:00, 2013-01-01 05:00…&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Second, there are two variables that we don&amp;rsquo;t want to use as predictors in our model, but that we would like to retain as identification variables that can be used to troubleshoot poorly predicted data points. These are &lt;code&gt;flight&lt;/code&gt;, a numeric value, and &lt;code&gt;time_hour&lt;/code&gt;, a date-time value.&lt;/p&gt;
&lt;p&gt;Third, there are 104 flight destinations contained in &lt;code&gt;dest&lt;/code&gt; and 16 distinct &lt;code&gt;carrier&lt;/code&gt;s.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;flight_data &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  skimr&lt;span style=&#34;color:#666&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#00f&#34;&gt;skim&lt;/span&gt;(dest, carrier) 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;table style=&#39;width: auto;&#39;
        class=&#39;table table-condensed&#39;&gt;
&lt;caption&gt;Table 1: Data summary&lt;/caption&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt;   &lt;/th&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt;   &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Name &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Piped data &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Number of rows &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 325819 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Number of columns &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 10 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; _______________________ &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt;  &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Column type frequency: &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt;  &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; factor &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 2 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; ________________________ &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt;  &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Group variables &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; None &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Variable type: factor&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; skim_variable &lt;/th&gt;
   &lt;th style=&#34;text-align:right;&#34;&gt; n_missing &lt;/th&gt;
   &lt;th style=&#34;text-align:right;&#34;&gt; complete_rate &lt;/th&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; ordered &lt;/th&gt;
   &lt;th style=&#34;text-align:right;&#34;&gt; n_unique &lt;/th&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; top_counts &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; dest &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 1 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; FALSE &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 104 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; ATL: 16771, ORD: 16507, LAX: 15942, BOS: 14948 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; carrier &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 1 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; FALSE &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 16 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; UA: 57489, B6: 53715, EV: 50868, DL: 47465 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Because we&amp;rsquo;ll be using a simple logistic regression model, the variables &lt;code&gt;dest&lt;/code&gt; and &lt;code&gt;carrier&lt;/code&gt; will be converted to 
&lt;a href=&#34;https://bookdown.org/max/FES/creating-dummy-variables-for-unordered-categories.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;dummy variables&lt;/a&gt;. However, some of these values do not occur very frequently and this could complicate our analysis. We&amp;rsquo;ll discuss specific steps later in this article that we can add to our recipe to address this issue before modeling.&lt;/p&gt;
&lt;h2 id=&#34;data-split&#34;&gt;Data splitting&lt;/h2&gt;
&lt;p&gt;To get started, let&amp;rsquo;s split this single dataset into two: a &lt;em&gt;training&lt;/em&gt; set and a &lt;em&gt;testing&lt;/em&gt; set. We&amp;rsquo;ll keep most of the rows in the original dataset (subset chosen randomly) in the &lt;em&gt;training&lt;/em&gt; set. The training data will be used to &lt;em&gt;fit&lt;/em&gt; the model, and the &lt;em&gt;testing&lt;/em&gt; set will be used to measure model performance.&lt;/p&gt;
&lt;p&gt;To do this, we can use the 
&lt;a href=&#34;https://tidymodels.github.io/rsample/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;rsample&lt;/a&gt; package to create an object that contains the information on &lt;em&gt;how&lt;/em&gt; to split the data, and then two more rsample functions to create data frames for the training and testing sets:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# Fix the random numbers by setting the seed &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# This enables the analysis to be reproducible when random numbers are used &lt;/span&gt;
&lt;span style=&#34;color:#00f&#34;&gt;set.seed&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;555&lt;/span&gt;)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# Put 3/4 of the data into the training set &lt;/span&gt;
data_split &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;initial_split&lt;/span&gt;(flight_data, prop &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;3&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;4&lt;/span&gt;)

&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# Create data frames for the two sets:&lt;/span&gt;
train_data &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;training&lt;/span&gt;(data_split)
test_data  &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;testing&lt;/span&gt;(data_split)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;recipe&#34;&gt;Create recipe and roles&lt;/h2&gt;
&lt;p&gt;To get started, let&amp;rsquo;s create a recipe for a simple logistic regression model. Before training the model, we can use a recipe to create a few new predictors and conduct some preprocessing required by the model.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s initiate a new recipe:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;flights_rec &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;recipe&lt;/span&gt;(arr_delay &lt;span style=&#34;color:#666&#34;&gt;~&lt;/span&gt; ., data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; train_data) 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The 
&lt;a href=&#34;https://tidymodels.github.io/recipes/reference/recipe.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;recipe()&lt;/code&gt; function&lt;/a&gt; as we used it here has two arguments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A &lt;strong&gt;formula&lt;/strong&gt;. Any variable on the left-hand side of the tilde (&lt;code&gt;~&lt;/code&gt;) is considered the model outcome (here, &lt;code&gt;arr_delay&lt;/code&gt;). On the right-hand side of the tilde are the predictors. Variables may be listed by name, or you can use the dot (&lt;code&gt;.&lt;/code&gt;) to indicate all other variables as predictors.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;strong&gt;data&lt;/strong&gt;. A recipe is associated with the data set used to create the model. This will typically be the &lt;em&gt;training&lt;/em&gt; set, so &lt;code&gt;data = train_data&lt;/code&gt; here. Naming a data set doesn&amp;rsquo;t actually change the data itself; it is only used to catalog the names of the variables and their types, like factors, integers, dates, etc.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now we can add 
&lt;a href=&#34;https://tidymodels.github.io/recipes/reference/roles.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;roles&lt;/a&gt; to this recipe. We can use the 
&lt;a href=&#34;https://tidymodels.github.io/recipes/reference/roles.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;update_role()&lt;/code&gt; function&lt;/a&gt; to let recipes know that &lt;code&gt;flight&lt;/code&gt; and &lt;code&gt;time_hour&lt;/code&gt; are variables with a custom role that we called &lt;code&gt;&amp;quot;ID&amp;quot;&lt;/code&gt; (a role can have any character value). Whereas our formula included all variables in the training set other than &lt;code&gt;arr_delay&lt;/code&gt; as predictors, this tells the recipe to keep these two variables but not use them as either outcomes or predictors.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;flights_rec &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;recipe&lt;/span&gt;(arr_delay &lt;span style=&#34;color:#666&#34;&gt;~&lt;/span&gt; ., data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; train_data) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;update_role&lt;/span&gt;(flight, time_hour, new_role &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;ID&amp;#34;&lt;/span&gt;) 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This step of adding roles to a recipe is optional; the purpose of using it here is that those two variables can be retained in the data but not included in the model. This can be convenient when, after the model is fit, we want to investigate some poorly predicted value. These ID columns will be available and can be used to try to understand what went wrong.&lt;/p&gt;
&lt;p&gt;To get the current set of variables and roles, use the &lt;code&gt;summary()&lt;/code&gt; function:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;summary&lt;/span&gt;(flights_rec)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 10 x 4&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;    variable  type    role      source  &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;    &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;   &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  1 dep_time  numeric predictor original&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  2 flight    numeric ID        original&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  3 origin    nominal predictor original&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  4 dest      nominal predictor original&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  5 air_time  numeric predictor original&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  6 distance  numeric predictor original&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  7 carrier   nominal predictor original&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  8 date      date    predictor original&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  9 time_hour date    ID        original&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 10 arr_delay nominal outcome   original&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;features&#34;&gt;Create features&lt;/h2&gt;
&lt;p&gt;Now we can start adding steps onto our recipe using the pipe operator. Perhaps it is reasonable for the date of the flight to have an effect on the likelihood of a late arrival. A little bit of &lt;strong&gt;feature engineering&lt;/strong&gt; might go a long way to improving our model. How should the date be encoded into the model? The &lt;code&gt;date&lt;/code&gt; column has an R &lt;code&gt;date&lt;/code&gt; object so including that column &amp;ldquo;as is&amp;rdquo; will mean that the model will convert it to a numeric format equal to the number of days after a reference date:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;flight_data &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;distinct&lt;/span&gt;(date) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;mutate&lt;/span&gt;(numeric_date &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;as.numeric&lt;/span&gt;(date)) 
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 364 x 2&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   date       numeric_date&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   &amp;lt;date&amp;gt;            &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 1 2013-01-01        15706&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 2 2013-01-02        15707&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 3 2013-01-03        15708&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 4 2013-01-04        15709&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 5 2013-01-05        15710&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # … with 359 more rows&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;It&amp;rsquo;s possible that the numeric date variable is a good option for modeling; perhaps the model would benefit from a linear trend between the log-odds of a late arrival and the numeric date variable. However, it might be better to add model terms &lt;em&gt;derived&lt;/em&gt; from the date that have a better potential to be important to the model. For example, we could derive the following meaningful features from the single &lt;code&gt;date&lt;/code&gt; variable:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;the day of the week,&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;the month, and&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;whether or not the date corresponds to a holiday.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s do all three of these by adding steps to our recipe:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;flights_rec &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;recipe&lt;/span&gt;(arr_delay &lt;span style=&#34;color:#666&#34;&gt;~&lt;/span&gt; ., data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; train_data) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;update_role&lt;/span&gt;(flight, time_hour, new_role &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;ID&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;step_date&lt;/span&gt;(date, features &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;dow&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;month&amp;#34;&lt;/span&gt;)) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;               
  &lt;span style=&#34;color:#00f&#34;&gt;step_holiday&lt;/span&gt;(date, holidays &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; timeDate&lt;span style=&#34;color:#666&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#00f&#34;&gt;listHolidays&lt;/span&gt;(&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;US&amp;#34;&lt;/span&gt;)) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;step_rm&lt;/span&gt;(date)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;What do each of these steps do?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;With 
&lt;a href=&#34;https://tidymodels.github.io/recipes/reference/step_date.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;step_date()&lt;/code&gt;&lt;/a&gt;, we created two new factor columns with the appropriate day of the week and the month.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;With 
&lt;a href=&#34;https://tidymodels.github.io/recipes/reference/step_holiday.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;step_holiday()&lt;/code&gt;&lt;/a&gt;, we created a binary variable indicating whether the current date is a holiday or not. The argument value of &lt;code&gt;timeDate::listHolidays(&amp;quot;US&amp;quot;)&lt;/code&gt; uses the 
&lt;a href=&#34;https://cran.r-project.org/web/packages/timeDate/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;timeDate package&lt;/a&gt; to list the 17 standard US holidays.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;With 
&lt;a href=&#34;https://tidymodels.github.io/recipes/reference/step_rm.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;step_rm()&lt;/code&gt;&lt;/a&gt;, we remove the original &lt;code&gt;date&lt;/code&gt; variable since we no longer want it in the model.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Next, we&amp;rsquo;ll turn our attention to the variable types of our predictors. Because we plan to train a logistic regression model, we know that predictors will ultimately need to be numeric, as opposed to factor variables. In other words, there may be a difference in how we store our data (in factors inside a data frame), and how the underlying equations require them (a purely numeric matrix).&lt;/p&gt;
&lt;p&gt;For factors like &lt;code&gt;dest&lt;/code&gt; and &lt;code&gt;origin&lt;/code&gt;, 
&lt;a href=&#34;https://bookdown.org/max/FES/creating-dummy-variables-for-unordered-categories.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;standard practice&lt;/a&gt; is to convert them into &lt;em&gt;dummy&lt;/em&gt; or &lt;em&gt;indicator&lt;/em&gt; variables to make them numeric. These are binary values for each level of the factor. For example, our &lt;code&gt;origin&lt;/code&gt; variable has values of &lt;code&gt;&amp;quot;EWR&amp;quot;&lt;/code&gt;, &lt;code&gt;&amp;quot;JFK&amp;quot;&lt;/code&gt;, and &lt;code&gt;&amp;quot;LGA&amp;quot;&lt;/code&gt;. The standard dummy variable encoding, shown below, will create &lt;em&gt;two&lt;/em&gt; numeric columns of the data that are 1 when the originating airport is &lt;code&gt;&amp;quot;JFK&amp;quot;&lt;/code&gt; or &lt;code&gt;&amp;quot;LGA&amp;quot;&lt;/code&gt; and zero otherwise, respectively.&lt;/p&gt;
&lt;table class=&#34;table&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; origin &lt;/th&gt;
   &lt;th style=&#34;text-align:right;&#34;&gt; origin_JFK &lt;/th&gt;
   &lt;th style=&#34;text-align:right;&#34;&gt; origin_LGA &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; EWR &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; JFK &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 1 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; LGA &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 1 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;But, unlike the standard model formula methods in R, a recipe &lt;strong&gt;does not&lt;/strong&gt; automatically create these dummy variables for you; you&amp;rsquo;ll need to tell your recipe to add this step. This is for two reasons. First, many models do not require 
&lt;a href=&#34;https://bookdown.org/max/FES/categorical-trees.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;numeric predictors&lt;/a&gt;, so dummy variables may not always be preferred. Second, recipes can also be used for purposes outside of modeling, where non-dummy versions of the variables may work better. For example, you may want to make a table or a plot with a variable as a single factor. For those reasons, you need to explicitly tell recipes to create dummy variables using &lt;code&gt;step_dummy()&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;flights_rec &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;recipe&lt;/span&gt;(arr_delay &lt;span style=&#34;color:#666&#34;&gt;~&lt;/span&gt; ., data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; train_data) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;update_role&lt;/span&gt;(flight, time_hour, new_role &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;ID&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;step_date&lt;/span&gt;(date, features &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;dow&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;month&amp;#34;&lt;/span&gt;)) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;step_holiday&lt;/span&gt;(date, holidays &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; timeDate&lt;span style=&#34;color:#666&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#00f&#34;&gt;listHolidays&lt;/span&gt;(&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;US&amp;#34;&lt;/span&gt;)) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;step_rm&lt;/span&gt;(date) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;step_dummy&lt;/span&gt;(&lt;span style=&#34;color:#00f&#34;&gt;all_nominal&lt;/span&gt;(), &lt;span style=&#34;color:#666&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#00f&#34;&gt;all_outcomes&lt;/span&gt;())
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Here, we did something different than before: instead of applying a step to an individual variable, we used 
&lt;a href=&#34;https://tidymodels.github.io/recipes/reference/selections.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;selectors&lt;/a&gt; to apply this recipe step to several variables at once.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The first selector, &lt;code&gt;all_nominal()&lt;/code&gt;, selects all variables that are either factors or characters.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The second selector, &lt;code&gt;-all_outcomes()&lt;/code&gt; removes any outcome variables from this recipe step.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With these two selectors together, our recipe step above translates to:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Create dummy variables for all of the factor or character columns &lt;em&gt;unless&lt;/em&gt; they are outcomes.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;At this stage in the recipe, this step selects the &lt;code&gt;origin&lt;/code&gt;, &lt;code&gt;dest&lt;/code&gt;, and &lt;code&gt;carrier&lt;/code&gt; variables. It also includes two new variables, &lt;code&gt;date_dow&lt;/code&gt; and &lt;code&gt;date_month&lt;/code&gt;, that were created by the earlier &lt;code&gt;step_date()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;More generally, the recipe selectors mean that you don&amp;rsquo;t always have to apply steps to individual variables one at a time. Since a recipe knows the &lt;em&gt;variable type&lt;/em&gt; and &lt;em&gt;role&lt;/em&gt; of each column, they can also be selected (or dropped) using this information.&lt;/p&gt;
&lt;p&gt;We need one final step to add to our recipe. Since &lt;code&gt;carrier&lt;/code&gt; and &lt;code&gt;dest&lt;/code&gt; have some infrequently occurring values, it is possible that dummy variables might be created for values that don&amp;rsquo;t exist in the training set. For example, there is one destination that is only in the test set:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;test_data &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;distinct&lt;/span&gt;(dest) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;anti_join&lt;/span&gt;(train_data)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Joining, by = &amp;#34;dest&amp;#34;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 1 x 1&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   dest &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   &amp;lt;fct&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 1 LEX&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;When the recipe is applied to the training set, a column is made for LEX but it will contain all zeros. This is a &amp;ldquo;zero-variance predictor&amp;rdquo; that has no information within the column. While some R functions will not produce an error for such predictors, it usually causes warnings and other issues. &lt;code&gt;step_zv()&lt;/code&gt; will remove columns from the data when the training set data have a single value, so it is added to the recipe &lt;em&gt;after&lt;/em&gt; &lt;code&gt;step_dummy()&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;flights_rec &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;recipe&lt;/span&gt;(arr_delay &lt;span style=&#34;color:#666&#34;&gt;~&lt;/span&gt; ., data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; train_data) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;update_role&lt;/span&gt;(flight, time_hour, new_role &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;ID&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;step_date&lt;/span&gt;(date, features &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;dow&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;month&amp;#34;&lt;/span&gt;)) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;step_holiday&lt;/span&gt;(date, holidays &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; timeDate&lt;span style=&#34;color:#666&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#00f&#34;&gt;listHolidays&lt;/span&gt;(&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;US&amp;#34;&lt;/span&gt;)) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;step_rm&lt;/span&gt;(date) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;step_dummy&lt;/span&gt;(&lt;span style=&#34;color:#00f&#34;&gt;all_nominal&lt;/span&gt;(), &lt;span style=&#34;color:#666&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#00f&#34;&gt;all_outcomes&lt;/span&gt;()) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;step_zv&lt;/span&gt;(&lt;span style=&#34;color:#00f&#34;&gt;all_predictors&lt;/span&gt;())
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now we&amp;rsquo;ve created a &lt;em&gt;specification&lt;/em&gt; of what should be done with the data. How do we use the recipe we made?&lt;/p&gt;
&lt;h2 id=&#34;fit-workflow&#34;&gt;Fit a model with a recipe&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s use logistic regression to model the flight data. As we saw in 
&lt;a href=&#34;https://nutriverse.io/start/models/&#34;&gt;&lt;em&gt;Build a Model&lt;/em&gt;&lt;/a&gt;, we start by 
&lt;a href=&#34;https://nutriverse.io/start/models/#build-model&#34;&gt;building a model specification&lt;/a&gt; using the parsnip package:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;lr_mod &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;logistic_reg&lt;/span&gt;() &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;set_engine&lt;/span&gt;(&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;glm&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We will want to use our recipe across several steps as we train and test our model. We will:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Process the recipe using the training set&lt;/strong&gt;: This involves any estimation or calculations based on the training set. For our recipe, the training set will be used to determine which predictors should be converted to dummy variables and which predictors will have zero-variance in the training set, and should be slated for removal.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Apply the recipe to the training set&lt;/strong&gt;: We create the final predictor set on the training set.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Apply the recipe to the test set&lt;/strong&gt;: We create the final predictor set on the test set. Nothing is recomputed and no information from the test set is used here; the dummy variable and zero-variance results from the training set are applied to the test set.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To simplify this process, we can use a &lt;em&gt;model workflow&lt;/em&gt;, which pairs a model and recipe together. This is a straightforward approach because different recipes are often needed for different models, so when a model and recipe are bundled, it becomes easier to train and test &lt;em&gt;workflows&lt;/em&gt;. We&amp;rsquo;ll use the 
&lt;a href=&#34;https://tidymodels.github.io/workflows/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;workflows package&lt;/a&gt; from tidymodels to bundle our parsnip model (&lt;code&gt;lr_mod&lt;/code&gt;) with our recipe (&lt;code&gt;flights_rec&lt;/code&gt;).&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;flights_wflow &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;workflow&lt;/span&gt;() &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;add_model&lt;/span&gt;(lr_mod) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;add_recipe&lt;/span&gt;(flights_rec)
flights_wflow
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ══ Workflow ═════════════════════════════════════════════════════════════&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Preprocessor: Recipe&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Model: logistic_reg()&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ── Preprocessor ─────────────────────────────────────────────────────────&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 5 Recipe Steps&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ● step_date()&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ● step_holiday()&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ● step_rm()&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ● step_dummy()&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ● step_zv()&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ── Model ────────────────────────────────────────────────────────────────&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Logistic Regression Model Specification (classification)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Computational engine: glm&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now, there is a single function that can be used to prepare the recipe and train the model from the resulting predictors:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;flights_fit &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  flights_wflow &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;fit&lt;/span&gt;(data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; train_data)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This object has the finalized recipe and fitted model objects inside. You may want to extract the model or recipe objects from the workflow. To do this, you can use the helper functions &lt;code&gt;pull_workflow_fit()&lt;/code&gt; and &lt;code&gt;pull_workflow_prepped_recipe()&lt;/code&gt;. For example, here we pull the fitted model object then use the &lt;code&gt;broom::tidy()&lt;/code&gt; function to get a tidy tibble of model coefficients:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;flights_fit &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;pull_workflow_fit&lt;/span&gt;() &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;tidy&lt;/span&gt;()
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 157 x 5&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   term                estimate std.error statistic  p.value&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   &amp;lt;chr&amp;gt;                  &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 1 (Intercept)          3.91    2.73           1.43 1.51e- 1&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 2 dep_time            -0.00167 0.0000141   -118.   0.      &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 3 air_time            -0.0439  0.000561     -78.4  0.      &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 4 distance             0.00686 0.00150        4.57 4.84e- 6&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 5 date_USChristmasDay  1.12    0.173          6.49 8.45e-11&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # … with 152 more rows&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;predict-workflow&#34;&gt;Use a trained workflow to predict&lt;/h2&gt;
&lt;p&gt;Our goal was to predict whether a plane arrives more than 30 minutes late. We have just:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Built the model (&lt;code&gt;lr_mod&lt;/code&gt;),&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Created a preprocessing recipe (&lt;code&gt;flights_rec&lt;/code&gt;),&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Bundled the model and recipe (&lt;code&gt;flights_wflow&lt;/code&gt;), and&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Trained our workflow using a single call to &lt;code&gt;fit()&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The next step is to use the trained workflow (&lt;code&gt;flights_fit&lt;/code&gt;) to predict with the unseen test data, which we will do with a single call to &lt;code&gt;predict()&lt;/code&gt;. The &lt;code&gt;predict()&lt;/code&gt; method applies the recipe to the new data, then passes them to the fitted model.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;predict&lt;/span&gt;(flights_fit, test_data)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 81,454 x 1&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   .pred_class&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   &amp;lt;fct&amp;gt;      &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 1 on_time    &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 2 on_time    &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 3 on_time    &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 4 on_time    &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 5 on_time    &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # … with 8.145e+04 more rows&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Because our outcome variable here is a factor, the output from &lt;code&gt;predict()&lt;/code&gt; returns the predicted class: &lt;code&gt;late&lt;/code&gt; versus &lt;code&gt;on_time&lt;/code&gt;. But, let&amp;rsquo;s say we want the predicted class probabilities for each flight instead. To return those, we can specify &lt;code&gt;type = &amp;quot;prob&amp;quot;&lt;/code&gt; when we use &lt;code&gt;predict()&lt;/code&gt;. We&amp;rsquo;ll also bind the output with some variables from the test data and save them together:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;flights_pred &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;predict&lt;/span&gt;(flights_fit, test_data, type &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;prob&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;bind_cols&lt;/span&gt;(test_data &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;select&lt;/span&gt;(arr_delay, time_hour, flight)) 

&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# The data look like: &lt;/span&gt;
flights_pred
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 81,454 x 5&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   .pred_late .pred_on_time arr_delay time_hour           flight&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;        &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt; &amp;lt;fct&amp;gt;     &amp;lt;dttm&amp;gt;               &amp;lt;int&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 1     0.0565         0.944 on_time   2013-01-01 05:00:00   1714&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 2     0.0264         0.974 on_time   2013-01-01 06:00:00     79&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 3     0.0481         0.952 on_time   2013-01-01 06:00:00    301&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 4     0.0325         0.967 on_time   2013-01-01 06:00:00     49&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 5     0.0711         0.929 on_time   2013-01-01 06:00:00   1187&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # … with 8.145e+04 more rows&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now that we have a tibble with our predicted class probabilities, how will we evaluate the performance of our workflow? We can see from these first few rows that our model predicted these 5 on time flights correctly because the values of &lt;code&gt;.pred_on_time&lt;/code&gt; are &lt;em&gt;p&lt;/em&gt; &amp;gt; .50. But we also know that we have 81,454 rows total to predict. We would like to calculate a metric that tells how well our model predicted late arrivals, compared to the true status of our outcome variable, &lt;code&gt;arr_delay&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s use the area under the 
&lt;a href=&#34;https://bookdown.org/max/FES/measuring-performance.html#class-metrics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ROC curve&lt;/a&gt; as our metric, computed using &lt;code&gt;roc_curve()&lt;/code&gt; and &lt;code&gt;roc_auc()&lt;/code&gt; from the 
&lt;a href=&#34;https://tidymodels.github.io/yardstick/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;yardstick package&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To generate a ROC curve, we need the predicted class probabilities for &lt;code&gt;late&lt;/code&gt; and &lt;code&gt;on_time&lt;/code&gt;, which we just calculated in the code chunk above. We can create the ROC curve with these values, using &lt;code&gt;roc_curve()&lt;/code&gt; and then piping to the &lt;code&gt;autoplot()&lt;/code&gt; method:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;flights_pred &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;roc_curve&lt;/span&gt;(truth &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; arr_delay, .pred_late) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;autoplot&lt;/span&gt;()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;figs/roc-plot-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Similarly, &lt;code&gt;roc_auc()&lt;/code&gt; estimates the area under the curve:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;flights_pred &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;roc_auc&lt;/span&gt;(truth &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; arr_delay, .pred_late)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 1 x 3&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   .metric .estimator .estimate&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 1 roc_auc binary         0.765&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Not too bad! We leave it to the reader to test out this workflow 
&lt;a href=&#34;https://tidymodels.github.io/workflows/reference/add_formula.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;without&lt;/em&gt;&lt;/a&gt; this recipe. You can use &lt;code&gt;workflows::add_formula(arr_delay ~ .)&lt;/code&gt; instead of &lt;code&gt;add_recipe()&lt;/code&gt; (remember to remove the identification variables first!), and see whether our recipe improved our model&amp;rsquo;s ability to predict late arrivals.&lt;/p&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session information&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;#&amp;gt; ─ Session info ───────────────────────────────────────────────────────────────
#&amp;gt;  setting  value                       
#&amp;gt;  version  R version 3.6.2 (2019-12-12)
#&amp;gt;  os       macOS Mojave 10.14.6        
#&amp;gt;  system   x86_64, darwin15.6.0        
#&amp;gt;  ui       X11                         
#&amp;gt;  language (EN)                        
#&amp;gt;  collate  en_US.UTF-8                 
#&amp;gt;  ctype    en_US.UTF-8                 
#&amp;gt;  tz       America/Denver              
#&amp;gt;  date     2020-04-20                  
#&amp;gt; 
#&amp;gt; ─ Packages ───────────────────────────────────────────────────────────────────
#&amp;gt;  package      * version date       lib source        
#&amp;gt;  broom        * 0.5.5   2020-02-29 [1] CRAN (R 3.6.0)
#&amp;gt;  dials        * 0.0.6   2020-04-03 [1] CRAN (R 3.6.2)
#&amp;gt;  dplyr        * 0.8.5   2020-03-07 [1] CRAN (R 3.6.0)
#&amp;gt;  ggplot2      * 3.3.0   2020-03-05 [1] CRAN (R 3.6.0)
#&amp;gt;  infer        * 0.5.1   2019-11-19 [1] CRAN (R 3.6.0)
#&amp;gt;  nycflights13 * 1.0.1   2019-09-16 [1] CRAN (R 3.6.0)
#&amp;gt;  parsnip      * 0.1.0   2020-04-09 [1] CRAN (R 3.6.2)
#&amp;gt;  purrr        * 0.3.4   2020-04-17 [1] CRAN (R 3.6.2)
#&amp;gt;  recipes      * 0.1.10  2020-03-18 [1] CRAN (R 3.6.0)
#&amp;gt;  rlang          0.4.5   2020-03-01 [1] CRAN (R 3.6.0)
#&amp;gt;  rsample      * 0.0.6   2020-03-31 [1] CRAN (R 3.6.2)
#&amp;gt;  skimr        * 2.1.1   2020-04-16 [1] CRAN (R 3.6.2)
#&amp;gt;  tibble       * 2.1.3   2019-06-06 [1] CRAN (R 3.6.2)
#&amp;gt;  tidymodels   * 0.1.0   2020-02-16 [1] CRAN (R 3.6.0)
#&amp;gt;  tune         * 0.1.0   2020-04-02 [1] CRAN (R 3.6.2)
#&amp;gt;  workflows    * 0.1.1   2020-03-17 [1] CRAN (R 3.6.0)
#&amp;gt;  yardstick    * 0.0.6   2020-03-17 [1] CRAN (R 3.6.0)
#&amp;gt; 
#&amp;gt; [1] /Library/Frameworks/R.framework/Versions/3.6/Resources/library
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Bootstrap resampling and tidy regression models</title>
      <link>https://nutriverse.io/learn/statistics/bootstrap/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://nutriverse.io/learn/statistics/bootstrap/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;To use the code in this article, you will need to install the following packages: tidymodels and tidyr.&lt;/p&gt;
&lt;p&gt;Combining fitted models in a tidy way is useful for performing bootstrapping or permutation tests. These approaches have been explored before, for instance by 
&lt;a href=&#34;https://rstudio-pubs-static.s3.amazonaws.com/19698_a4c472606e3c43e4b94720506e49bb7b.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Andrew MacDonald here&lt;/a&gt;, and 
&lt;a href=&#34;https://github.com/hadley/dplyr/issues/269&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hadley has explored efficient support for bootstrapping&lt;/a&gt; as a potential enhancement to dplyr. The tidymodels package 
&lt;a href=&#34;https://broom.tidyverse.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;broom&lt;/a&gt; fits naturally with 
&lt;a href=&#34;https://dplyr.tidyverse.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;dplyr&lt;/a&gt; in performing these analyses.&lt;/p&gt;
&lt;p&gt;Bootstrapping consists of randomly sampling a data set with replacement, then performing the analysis individually on each bootstrapped replicate. The variation in the resulting estimate is then a reasonable approximation of the variance in our estimate.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s say we want to fit a nonlinear model to the weight/mileage relationship in the &lt;code&gt;mtcars&lt;/code&gt; data set.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;library&lt;/span&gt;(tidymodels)

&lt;span style=&#34;color:#00f&#34;&gt;ggplot&lt;/span&gt;(mtcars, &lt;span style=&#34;color:#00f&#34;&gt;aes&lt;/span&gt;(mpg, wt)) &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; 
    &lt;span style=&#34;color:#00f&#34;&gt;geom_point&lt;/span&gt;()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;figs/unnamed-chunk-1-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We might use the method of nonlinear least squares (via the &lt;code&gt;nls()&lt;/code&gt; function) to fit a model.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;nlsfit &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;nls&lt;/span&gt;(mpg &lt;span style=&#34;color:#666&#34;&gt;~&lt;/span&gt; k &lt;span style=&#34;color:#666&#34;&gt;/&lt;/span&gt; wt &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; b, mtcars, start &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;list&lt;/span&gt;(k &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;, b &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;0&lt;/span&gt;))
&lt;span style=&#34;color:#00f&#34;&gt;summary&lt;/span&gt;(nlsfit)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Formula: mpg ~ k/wt + b&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Parameters:&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   Estimate Std. Error t value Pr(&amp;gt;|t|)    &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; k    45.83       4.25   10.79  7.6e-12 ***&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; b     4.39       1.54    2.85   0.0077 ** &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ---&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Residual standard error: 2.77 on 30 degrees of freedom&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Number of iterations to convergence: 1 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Achieved convergence tolerance: 2.88e-08&lt;/span&gt;

&lt;span style=&#34;color:#00f&#34;&gt;ggplot&lt;/span&gt;(mtcars, &lt;span style=&#34;color:#00f&#34;&gt;aes&lt;/span&gt;(wt, mpg)) &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt;
    &lt;span style=&#34;color:#00f&#34;&gt;geom_point&lt;/span&gt;() &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt;
    &lt;span style=&#34;color:#00f&#34;&gt;geom_line&lt;/span&gt;(&lt;span style=&#34;color:#00f&#34;&gt;aes&lt;/span&gt;(y &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;predict&lt;/span&gt;(nlsfit)))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;figs/unnamed-chunk-2-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;While this does provide a p-value and confidence intervals for the parameters, these are based on model assumptions that may not hold in real data. Bootstrapping is a popular method for providing confidence intervals and predictions that are more robust to the nature of the data.&lt;/p&gt;
&lt;h2 id=&#34;bootstrapping-models&#34;&gt;Bootstrapping models&lt;/h2&gt;
&lt;p&gt;We can use the &lt;code&gt;bootstraps()&lt;/code&gt; function in the rsample package to sample bootstrap replications. First, we construct 2000 bootstrap replicates of the data, each of which has been randomly sampled with replacement. The resulting object is an &lt;code&gt;rset&lt;/code&gt;, which is a data frame with a column of &lt;code&gt;rsplit&lt;/code&gt; objects.&lt;/p&gt;
&lt;p&gt;An &lt;code&gt;rsplit&lt;/code&gt; object has two main components: an analysis data set and an assessment data set, accessible via &lt;code&gt;analysis(rsplit)&lt;/code&gt; and &lt;code&gt;assessment(rsplit)&lt;/code&gt; respectively. For bootstrap samples, the analysis data set is the bootstrap sample itself, and the assessment data set consists of all the out-of-bag samples.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;set.seed&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;27&lt;/span&gt;)
boots &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;bootstraps&lt;/span&gt;(mtcars, times &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;2000&lt;/span&gt;, apparent &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;TRUE&lt;/span&gt;)
boots
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # Bootstrap sampling with apparent sample &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 2,001 x 2&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;    splits          id           &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;    &amp;lt;list&amp;gt;          &amp;lt;chr&amp;gt;        &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  1 &amp;lt;split [32/13]&amp;gt; Bootstrap0001&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  2 &amp;lt;split [32/10]&amp;gt; Bootstrap0002&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  3 &amp;lt;split [32/13]&amp;gt; Bootstrap0003&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  4 &amp;lt;split [32/11]&amp;gt; Bootstrap0004&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  5 &amp;lt;split [32/9]&amp;gt;  Bootstrap0005&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  6 &amp;lt;split [32/10]&amp;gt; Bootstrap0006&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  7 &amp;lt;split [32/11]&amp;gt; Bootstrap0007&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  8 &amp;lt;split [32/13]&amp;gt; Bootstrap0008&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  9 &amp;lt;split [32/11]&amp;gt; Bootstrap0009&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 10 &amp;lt;split [32/11]&amp;gt; Bootstrap0010&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # … with 1,991 more rows&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Let&amp;rsquo;s create a helper function to fit an &lt;code&gt;nls()&lt;/code&gt; model on each bootstrap sample, and then use &lt;code&gt;purrr::map()&lt;/code&gt; to apply this function to all the bootstrap samples at once. Similarly, we create a column of tidy coefficient information by unnesting.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;fit_nls_on_bootstrap &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;function&lt;/span&gt;(split) {
    &lt;span style=&#34;color:#00f&#34;&gt;nls&lt;/span&gt;(mpg &lt;span style=&#34;color:#666&#34;&gt;~&lt;/span&gt; k &lt;span style=&#34;color:#666&#34;&gt;/&lt;/span&gt; wt &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; b, &lt;span style=&#34;color:#00f&#34;&gt;analysis&lt;/span&gt;(split), start &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;list&lt;/span&gt;(k &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;, b &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;0&lt;/span&gt;))
}

boot_models &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt;
  boots &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;mutate&lt;/span&gt;(model &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;map&lt;/span&gt;(splits, fit_nls_on_bootstrap),
         coef_info &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;map&lt;/span&gt;(model, tidy))

&lt;span style=&#34;color:#00f&#34;&gt;library&lt;/span&gt;(tidyr)
boot_coefs &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  boot_models &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;unnest&lt;/span&gt;(coef_info)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The unnested coefficient information contains a summary of each replication combined in a single data frame:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;boot_coefs
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 4,002 x 8&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;    splits         id           model term  estimate std.error statistic  p.value&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;    &amp;lt;list&amp;gt;         &amp;lt;chr&amp;gt;        &amp;lt;lis&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  1 &amp;lt;split [32/13… Bootstrap00… &amp;lt;nls&amp;gt; k        42.1       4.05     10.4  1.91e-11&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  2 &amp;lt;split [32/13… Bootstrap00… &amp;lt;nls&amp;gt; b         5.39      1.43      3.78 6.93e- 4&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  3 &amp;lt;split [32/10… Bootstrap00… &amp;lt;nls&amp;gt; k        49.9       5.66      8.82 7.82e-10&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  4 &amp;lt;split [32/10… Bootstrap00… &amp;lt;nls&amp;gt; b         3.73      1.92      1.94 6.13e- 2&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  5 &amp;lt;split [32/13… Bootstrap00… &amp;lt;nls&amp;gt; k        37.8       2.68     14.1  9.01e-15&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  6 &amp;lt;split [32/13… Bootstrap00… &amp;lt;nls&amp;gt; b         6.73      1.17      5.75 2.78e- 6&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  7 &amp;lt;split [32/11… Bootstrap00… &amp;lt;nls&amp;gt; k        45.6       4.45     10.2  2.70e-11&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  8 &amp;lt;split [32/11… Bootstrap00… &amp;lt;nls&amp;gt; b         4.75      1.62      2.93 6.38e- 3&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  9 &amp;lt;split [32/9]&amp;gt; Bootstrap00… &amp;lt;nls&amp;gt; k        43.6       4.63      9.41 1.85e-10&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 10 &amp;lt;split [32/9]&amp;gt; Bootstrap00… &amp;lt;nls&amp;gt; b         5.89      1.68      3.51 1.44e- 3&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # … with 3,992 more rows&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;confidence-intervals&#34;&gt;Confidence intervals&lt;/h2&gt;
&lt;p&gt;We can then calculate confidence intervals (using what is called the 
&lt;a href=&#34;https://www.uvm.edu/~dhowell/StatPages/Randomization%20Tests/ResamplingWithR/BootstMeans/bootstrapping_means.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;percentile method&lt;/a&gt;):&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;percentile_intervals &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;int_pctl&lt;/span&gt;(boot_models, coef_info)
percentile_intervals
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 2 x 6&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   term   .lower .estimate .upper .alpha .method   &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   &amp;lt;chr&amp;gt;   &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;     &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 1 b      0.0475      4.12   7.31   0.05 percentile&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 2 k     37.6        46.7   59.8    0.05 percentile&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Or we can use histograms to get a more detailed idea of the uncertainty in each estimate:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;ggplot&lt;/span&gt;(boot_coefs, &lt;span style=&#34;color:#00f&#34;&gt;aes&lt;/span&gt;(estimate)) &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;geom_histogram&lt;/span&gt;(bins &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;30&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;facet_wrap&lt;/span&gt;( &lt;span style=&#34;color:#666&#34;&gt;~&lt;/span&gt; term, scales &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;free&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;geom_vline&lt;/span&gt;(&lt;span style=&#34;color:#00f&#34;&gt;aes&lt;/span&gt;(xintercept &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; .lower), data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; percentile_intervals, col &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;blue&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;geom_vline&lt;/span&gt;(&lt;span style=&#34;color:#00f&#34;&gt;aes&lt;/span&gt;(xintercept &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; .upper), data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; percentile_intervals, col &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;blue&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;figs/unnamed-chunk-6-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The rsample package also has functions for 
&lt;a href=&#34;https://tidymodels.github.io/rsample/reference/int_pctl.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;other types of confidence intervals&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;possible-model-fits&#34;&gt;Possible model fits&lt;/h2&gt;
&lt;p&gt;We can use &lt;code&gt;augment()&lt;/code&gt; to visualize the uncertainty in the fitted curve. Since there are so many bootstrap samples, we&amp;rsquo;ll only show a sample of the model fits in our visualization:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;boot_aug &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  boot_models &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;sample_n&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;200&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;mutate&lt;/span&gt;(augmented &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;map&lt;/span&gt;(model, augment)) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;unnest&lt;/span&gt;(augmented)

boot_aug
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 6,400 x 8&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;    splits         id            model coef_info         mpg    wt .fitted .resid&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;    &amp;lt;list&amp;gt;         &amp;lt;chr&amp;gt;         &amp;lt;lis&amp;gt; &amp;lt;list&amp;gt;          &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  1 &amp;lt;split [32/11… Bootstrap1644 &amp;lt;nls&amp;gt; &amp;lt;tibble [2 × 5…  16.4  4.07    15.6  0.829&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  2 &amp;lt;split [32/11… Bootstrap1644 &amp;lt;nls&amp;gt; &amp;lt;tibble [2 × 5…  19.7  2.77    21.9 -2.21 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  3 &amp;lt;split [32/11… Bootstrap1644 &amp;lt;nls&amp;gt; &amp;lt;tibble [2 × 5…  19.2  3.84    16.4  2.84 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  4 &amp;lt;split [32/11… Bootstrap1644 &amp;lt;nls&amp;gt; &amp;lt;tibble [2 × 5…  21.4  2.78    21.8 -0.437&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  5 &amp;lt;split [32/11… Bootstrap1644 &amp;lt;nls&amp;gt; &amp;lt;tibble [2 × 5…  26    2.14    27.8 -1.75 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  6 &amp;lt;split [32/11… Bootstrap1644 &amp;lt;nls&amp;gt; &amp;lt;tibble [2 × 5…  33.9  1.84    32.0  1.88 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  7 &amp;lt;split [32/11… Bootstrap1644 &amp;lt;nls&amp;gt; &amp;lt;tibble [2 × 5…  32.4  2.2     27.0  5.35 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  8 &amp;lt;split [32/11… Bootstrap1644 &amp;lt;nls&amp;gt; &amp;lt;tibble [2 × 5…  30.4  1.62    36.1 -5.70 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  9 &amp;lt;split [32/11… Bootstrap1644 &amp;lt;nls&amp;gt; &amp;lt;tibble [2 × 5…  21.5  2.46    24.4 -2.86 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 10 &amp;lt;split [32/11… Bootstrap1644 &amp;lt;nls&amp;gt; &amp;lt;tibble [2 × 5…  26    2.14    27.8 -1.75 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # … with 6,390 more rows&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;ggplot&lt;/span&gt;(boot_aug, &lt;span style=&#34;color:#00f&#34;&gt;aes&lt;/span&gt;(wt, mpg)) &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;geom_line&lt;/span&gt;(&lt;span style=&#34;color:#00f&#34;&gt;aes&lt;/span&gt;(y &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; .fitted, group &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; id), alpha &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;.2&lt;/span&gt;, col &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;blue&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;geom_point&lt;/span&gt;()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;figs/unnamed-chunk-8-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;With only a few small changes, we could easily perform bootstrapping with other kinds of predictive or hypothesis testing models, since the &lt;code&gt;tidy()&lt;/code&gt; and &lt;code&gt;augment()&lt;/code&gt; functions works for many statistical outputs. As another example, we could use &lt;code&gt;smooth.spline()&lt;/code&gt;, which fits a cubic smoothing spline to data:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;fit_spline_on_bootstrap &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;function&lt;/span&gt;(split) {
    data &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;analysis&lt;/span&gt;(split)
    &lt;span style=&#34;color:#00f&#34;&gt;smooth.spline&lt;/span&gt;(data&lt;span style=&#34;color:#666&#34;&gt;$&lt;/span&gt;wt, data&lt;span style=&#34;color:#666&#34;&gt;$&lt;/span&gt;mpg, df &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;4&lt;/span&gt;)
}

boot_splines &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  boots &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;sample_n&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;200&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;mutate&lt;/span&gt;(spline &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;map&lt;/span&gt;(splits, fit_spline_on_bootstrap),
         aug_train &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;map&lt;/span&gt;(spline, augment))

splines_aug &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  boot_splines &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;unnest&lt;/span&gt;(aug_train)

&lt;span style=&#34;color:#00f&#34;&gt;ggplot&lt;/span&gt;(splines_aug, &lt;span style=&#34;color:#00f&#34;&gt;aes&lt;/span&gt;(x, y)) &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;geom_line&lt;/span&gt;(&lt;span style=&#34;color:#00f&#34;&gt;aes&lt;/span&gt;(y &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; .fitted, group &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; id), alpha &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;0.2&lt;/span&gt;, col &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;blue&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;geom_point&lt;/span&gt;()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;figs/unnamed-chunk-9-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;session-information&#34;&gt;Session information&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;#&amp;gt; ─ Session info ───────────────────────────────────────────────────────────────
#&amp;gt;  setting  value                       
#&amp;gt;  version  R version 3.6.2 (2019-12-12)
#&amp;gt;  os       macOS Mojave 10.14.6        
#&amp;gt;  system   x86_64, darwin15.6.0        
#&amp;gt;  ui       X11                         
#&amp;gt;  language (EN)                        
#&amp;gt;  collate  en_US.UTF-8                 
#&amp;gt;  ctype    en_US.UTF-8                 
#&amp;gt;  tz       America/Denver              
#&amp;gt;  date     2020-04-17                  
#&amp;gt; 
#&amp;gt; ─ Packages ───────────────────────────────────────────────────────────────────
#&amp;gt;  package    * version date       lib source        
#&amp;gt;  broom      * 0.5.5   2020-02-29 [1] CRAN (R 3.6.0)
#&amp;gt;  dials      * 0.0.6   2020-04-03 [1] CRAN (R 3.6.2)
#&amp;gt;  dplyr      * 0.8.5   2020-03-07 [1] CRAN (R 3.6.0)
#&amp;gt;  ggplot2    * 3.3.0   2020-03-05 [1] CRAN (R 3.6.0)
#&amp;gt;  infer      * 0.5.1   2019-11-19 [1] CRAN (R 3.6.0)
#&amp;gt;  parsnip    * 0.1.0   2020-04-09 [1] CRAN (R 3.6.2)
#&amp;gt;  purrr      * 0.3.3   2019-10-18 [1] CRAN (R 3.6.0)
#&amp;gt;  recipes    * 0.1.10  2020-03-18 [1] CRAN (R 3.6.0)
#&amp;gt;  rlang        0.4.5   2020-03-01 [1] CRAN (R 3.6.0)
#&amp;gt;  rsample    * 0.0.6   2020-03-31 [1] CRAN (R 3.6.2)
#&amp;gt;  tibble     * 2.1.3   2019-06-06 [1] CRAN (R 3.6.2)
#&amp;gt;  tidymodels * 0.1.0   2020-02-16 [1] CRAN (R 3.6.0)
#&amp;gt;  tidyr      * 1.0.2   2020-01-24 [1] CRAN (R 3.6.0)
#&amp;gt;  tune       * 0.1.0   2020-04-02 [1] CRAN (R 3.6.2)
#&amp;gt;  workflows  * 0.1.1   2020-03-17 [1] CRAN (R 3.6.0)
#&amp;gt;  yardstick  * 0.0.6   2020-03-17 [1] CRAN (R 3.6.0)
#&amp;gt; 
#&amp;gt; [1] /Library/Frameworks/R.framework/Versions/3.6/Resources/library
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Evaluate your model with resampling</title>
      <link>https://nutriverse.io/start/resampling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://nutriverse.io/start/resampling/</guid>
      <description>&lt;h2 id=&#34;intro&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;So far, we have 
&lt;a href=&#34;https://nutriverse.io/start/models/&#34;&gt;built a model&lt;/a&gt; and 
&lt;a href=&#34;https://nutriverse.io/start/recipes/&#34;&gt;preprocessed data with a recipe&lt;/a&gt;. We also introduced 
&lt;a href=&#34;https://nutriverse.io/start/recipes/#fit-workflow&#34;&gt;workflows&lt;/a&gt; as a way to bundle a 
&lt;a href=&#34;https://tidymodels.github.io/parsnip/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;parsnip model&lt;/a&gt; and 
&lt;a href=&#34;https://tidymodels.github.io/recipes/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;recipe&lt;/a&gt; together. Once we have a model trained, we need a way to measure how well that model predicts new data. This tutorial explains how to characterize model performance based on &lt;strong&gt;resampling&lt;/strong&gt; statistics.&lt;/p&gt;
&lt;p&gt;To use code in this article,  you will need to install the following packages: modeldata, ranger, and tidymodels.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;library&lt;/span&gt;(tidymodels) &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# for the rsample package, along with the rest of tidymodels&lt;/span&gt;

&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# Helper packages&lt;/span&gt;
&lt;span style=&#34;color:#00f&#34;&gt;library&lt;/span&gt;(modeldata)  &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# for the cells data&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;data&#34;&gt;The cell image data&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s use data from 
&lt;a href=&#34;http://www.biomedcentral.com/1471-2105/8/340&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hill, LaPan, Li, and Haney (2007)&lt;/a&gt;, available in the 
&lt;a href=&#34;https://cran.r-project.org/web/packages/modeldata/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;modeldata package&lt;/a&gt;, to predict cell image segmentation quality with resampling. To start, we load this data into R:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;data&lt;/span&gt;(cells, package &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;modeldata&amp;#34;&lt;/span&gt;)
cells
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 2,019 x 58&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   case  class angle_ch_1 area_ch_1 avg_inten_ch_1 avg_inten_ch_2 avg_inten_ch_3&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt;      &amp;lt;dbl&amp;gt;     &amp;lt;int&amp;gt;          &amp;lt;dbl&amp;gt;          &amp;lt;dbl&amp;gt;          &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 1 Test  PS        143.         185           15.7           4.95           9.55&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 2 Train PS        134.         819           31.9         207.            69.9 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 3 Train WS        107.         431           28.0         116.            63.9 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 4 Train PS         69.2        298           19.5         102.            28.2 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 5 Test  PS          2.89       285           24.3         112.            20.5 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # … with 2,014 more rows, and 51 more variables: avg_inten_ch_4 &amp;lt;dbl&amp;gt;,&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; #   convex_hull_area_ratio_ch_1 &amp;lt;dbl&amp;gt;, convex_hull_perim_ratio_ch_1 &amp;lt;dbl&amp;gt;,&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; #   diff_inten_density_ch_1 &amp;lt;dbl&amp;gt;, diff_inten_density_ch_3 &amp;lt;dbl&amp;gt;, …&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We have data for 2019 cells, with 58 variables. The main outcome variable of interest for us here is called &lt;code&gt;class&lt;/code&gt;, which you can see is a factor. But before we jump into predicting the &lt;code&gt;class&lt;/code&gt; variable, we need to understand it better. Below is a brief primer on cell image segmentation.&lt;/p&gt;
&lt;h3 id=&#34;predicting-image-segmentation-quality&#34;&gt;Predicting image segmentation quality&lt;/h3&gt;
&lt;p&gt;Some biologists conduct experiments on cells. In drug discovery, a particular type of cell can be treated with either a drug or control and then observed to see what the effect is (if any). A common approach for this kind of measurement is cell imaging. Different parts of the cells can be colored so that the locations of a cell can be determined.&lt;/p&gt;
&lt;p&gt;For example, in top panel of this image of five cells, the green color is meant to define the boundary of the cell (coloring something called the cytoskeleton) while the blue color defines the nucleus of the cell.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/cells.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Using these colors, the cells in an image can be &lt;em&gt;segmented&lt;/em&gt; so that we know which pixels belong to which cell. If this is done well, the cell can be measured in different ways that are important to the biology. Sometimes the shape of the cell matters and different mathematical tools are used to summarize characteristics like the size or &amp;ldquo;oblongness&amp;rdquo; of the cell.&lt;/p&gt;
&lt;p&gt;The bottom panel shows some segmentation results. Cells 1 and 5 are fairly well segmented. However, cells 2 to 4 are bunched up together because the segmentation was not very good. The consequence of bad segmentation is data contamination; when the biologist analyzes the shape or size of these cells, the data are inaccurate and could lead to the wrong conclusion.&lt;/p&gt;
&lt;p&gt;A cell-based experiment might involve millions of cells so it is unfeasible to visually assess them all. Instead, a subsample can be created and these cells can be manually labeled by experts as either poorly segmented (&lt;code&gt;PS&lt;/code&gt;) or well-segmented (&lt;code&gt;WS&lt;/code&gt;). If we can predict these labels accurately, the larger data set can be improved by filtering out the cells most likely to be poorly segmented.&lt;/p&gt;
&lt;h3 id=&#34;back-to-the-cells-data&#34;&gt;Back to the cells data&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;cells&lt;/code&gt; data has &lt;code&gt;class&lt;/code&gt; labels for 2019 cells — each cell is labeled as either poorly segmented (&lt;code&gt;PS&lt;/code&gt;) or well-segmented (&lt;code&gt;WS&lt;/code&gt;). Each also has a total of 56 predictors based on automated image analysis measurements. For example, &lt;code&gt;avg_inten_ch_1&lt;/code&gt; is the mean intensity of the data contained in the nucleus, &lt;code&gt;area_ch_1&lt;/code&gt; is the total size of the cell, and so on (some predictors are fairly arcane in nature).&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;cells
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 2,019 x 58&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   case  class angle_ch_1 area_ch_1 avg_inten_ch_1 avg_inten_ch_2 avg_inten_ch_3&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt;      &amp;lt;dbl&amp;gt;     &amp;lt;int&amp;gt;          &amp;lt;dbl&amp;gt;          &amp;lt;dbl&amp;gt;          &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 1 Test  PS        143.         185           15.7           4.95           9.55&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 2 Train PS        134.         819           31.9         207.            69.9 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 3 Train WS        107.         431           28.0         116.            63.9 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 4 Train PS         69.2        298           19.5         102.            28.2 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 5 Test  PS          2.89       285           24.3         112.            20.5 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # … with 2,014 more rows, and 51 more variables: avg_inten_ch_4 &amp;lt;dbl&amp;gt;,&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; #   convex_hull_area_ratio_ch_1 &amp;lt;dbl&amp;gt;, convex_hull_perim_ratio_ch_1 &amp;lt;dbl&amp;gt;,&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; #   diff_inten_density_ch_1 &amp;lt;dbl&amp;gt;, diff_inten_density_ch_3 &amp;lt;dbl&amp;gt;, …&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The rates of the classes are somewhat imbalanced; there are more poorly segmented cells than well-segmented cells:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;cells &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;count&lt;/span&gt;(class) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;mutate&lt;/span&gt;(prop &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; n&lt;span style=&#34;color:#666&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#00f&#34;&gt;sum&lt;/span&gt;(n))
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 2 x 3&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   class     n  prop&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   &amp;lt;fct&amp;gt; &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 1 PS     1300 0.644&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 2 WS      719 0.356&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;data-split&#34;&gt;Data splitting&lt;/h2&gt;
&lt;p&gt;In our previous 
&lt;a href=&#34;https://nutriverse.io/start/recipes/#data-split&#34;&gt;&lt;em&gt;Preprocess your data with recipes&lt;/em&gt;&lt;/a&gt; article, we started by splitting our data. It is common when beginning a modeling project to 
&lt;a href=&#34;https://bookdown.org/max/FES/data-splitting.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;separate the data set&lt;/a&gt; into two partitions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The &lt;em&gt;training set&lt;/em&gt; is used to estimate parameters, compare models and feature engineering techniques, tune models, etc.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;em&gt;test set&lt;/em&gt; is held in reserve until the end of the project, at which point there should only be one or two models under serious consideration. It is used as an unbiased source for measuring final model performance.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are different ways to create these partitions of the data. The most common approach is to use a random sample. Suppose that one quarter of the data were reserved for the test set. Random sampling would randomly select 25% for the test set and use the remainder for the training set. We can use the 
&lt;a href=&#34;https://tidymodels.github.io/rsample/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;rsample&lt;/a&gt; package for this purpose.&lt;/p&gt;
&lt;p&gt;Since random sampling uses random numbers, it is important to set the random number seed. This ensures that the random numbers can be reproduced at a later time (if needed).&lt;/p&gt;
&lt;p&gt;The function &lt;code&gt;rsample::initial_split()&lt;/code&gt; takes the original data and saves the information on how to make the partitions. In the original analysis, the authors made their own training/test set and that information is contained in the column &lt;code&gt;case&lt;/code&gt;. To demonstrate how to make a split, we&amp;rsquo;ll remove this column before we make our own split:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;set.seed&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;123&lt;/span&gt;)
cell_split &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;initial_split&lt;/span&gt;(cells &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;select&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;-&lt;/span&gt;case), 
                            strata &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; class)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Here we used the 
&lt;a href=&#34;https://tidymodels.github.io/rsample/reference/initial_split.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;strata&lt;/code&gt; argument&lt;/a&gt;, which conducts a stratified split. This ensures that, despite the imbalance we noticed in our &lt;code&gt;class&lt;/code&gt; variable, our training and test data sets will keep roughly the same proportions of poorly and well-segmented cells as in the original data. After the &lt;code&gt;initial_split&lt;/code&gt;, the &lt;code&gt;training()&lt;/code&gt; and &lt;code&gt;testing()&lt;/code&gt; functions return the actual data sets.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;cell_train &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;training&lt;/span&gt;(cell_split)
cell_test  &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;testing&lt;/span&gt;(cell_split)

&lt;span style=&#34;color:#00f&#34;&gt;nrow&lt;/span&gt;(cell_train)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; [1] 1515&lt;/span&gt;
&lt;span style=&#34;color:#00f&#34;&gt;nrow&lt;/span&gt;(cell_train)&lt;span style=&#34;color:#666&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#00f&#34;&gt;nrow&lt;/span&gt;(cells)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; [1] 0.7503715&lt;/span&gt;

&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# training set proportions by class&lt;/span&gt;
cell_train &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;count&lt;/span&gt;(class) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;mutate&lt;/span&gt;(prop &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; n&lt;span style=&#34;color:#666&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#00f&#34;&gt;sum&lt;/span&gt;(n))
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 2 x 3&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   class     n  prop&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   &amp;lt;fct&amp;gt; &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 1 PS      975 0.644&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 2 WS      540 0.356&lt;/span&gt;

&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# test set proportions by class&lt;/span&gt;
cell_test &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;count&lt;/span&gt;(class) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;mutate&lt;/span&gt;(prop &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; n&lt;span style=&#34;color:#666&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#00f&#34;&gt;sum&lt;/span&gt;(n))
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 2 x 3&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   class     n  prop&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   &amp;lt;fct&amp;gt; &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 1 PS      325 0.645&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 2 WS      179 0.355&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The majority of the modeling work is then conducted on the training set data.&lt;/p&gt;
&lt;h2 id=&#34;modeling&#34;&gt;Modeling&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://en.wikipedia.org/wiki/Random_forest&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Random forest models&lt;/a&gt; are 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Ensemble_learning&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ensembles&lt;/a&gt; of 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Decision_tree&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;decision trees&lt;/a&gt;. A large number of decision tree models are created for the ensemble based on slightly different versions of the training set. When creating the individual decision trees, the fitting process encourages them to be as diverse as possible. The collection of trees are combined into the random forest model and, when a new sample is predicted, the votes from each tree are used to calculate the final predicted value for the new sample. For categorical outcome variables like &lt;code&gt;class&lt;/code&gt; in our &lt;code&gt;cells&lt;/code&gt; data example, the majority vote across all the trees in the random forest determines the predicted class for the new sample.&lt;/p&gt;
&lt;p&gt;One of the benefits of a random forest model is that it is very low maintenance;  it requires very little preprocessing of the data and the default parameters tend to give reasonable results. For that reason, we won&amp;rsquo;t create a recipe for the &lt;code&gt;cells&lt;/code&gt; data.&lt;/p&gt;
&lt;p&gt;At the same time, the number of trees in the ensemble should be large (in the thousands) and this makes the model moderately expensive to compute.&lt;/p&gt;
&lt;p&gt;To fit a random forest model on the training set, let&amp;rsquo;s use the 
&lt;a href=&#34;https://tidymodels.github.io/parsnip/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;parsnip&lt;/a&gt; package with the 
&lt;a href=&#34;https://cran.r-project.org/web/packages/ranger/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ranger&lt;/a&gt; engine. We first define the model that we want to create:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;rf_mod &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;rand_forest&lt;/span&gt;(trees &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;1000&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;set_engine&lt;/span&gt;(&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;ranger&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;set_mode&lt;/span&gt;(&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;classification&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Starting with this parsnip model object, the &lt;code&gt;fit()&lt;/code&gt; function can be used with a model formula. Since random forest models use random numbers, we again set the seed prior to computing:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;set.seed&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;234&lt;/span&gt;)
rf_fit &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  rf_mod &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;fit&lt;/span&gt;(class &lt;span style=&#34;color:#666&#34;&gt;~&lt;/span&gt; ., data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; cell_train)
rf_fit
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; parsnip model object&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Fit time:  2.4s &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Ranger result&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Call:&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  ranger::ranger(formula = formula, data = data, num.trees = ~1000,      num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1), probability = TRUE) &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Type:                             Probability estimation &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Number of trees:                  1000 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Sample size:                      1515 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Number of independent variables:  56 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Mtry:                             7 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Target node size:                 10 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Variable importance mode:         none &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Splitrule:                        gini &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; OOB prediction error (Brier s.):  0.1218873&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This new &lt;code&gt;rf_fit&lt;/code&gt; object is our fitted model, trained on our training data set.&lt;/p&gt;
&lt;h2 id=&#34;performance&#34;&gt;Estimating performance&lt;/h2&gt;
&lt;p&gt;During a modeling project, we might create a variety of different models. To choose between them, we need to consider how well these models do, as measured by some performance statistics. In our example in this article, some options we could use are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;the area under the Receiver Operating Characteristic (ROC) curve, and&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;overall classification accuracy.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The ROC curve uses the class probability estimates to give us a sense of performance across the entire set of potential probability cutoffs. Overall accuracy uses the hard class predictions to measure performance. The hard class predictions tell us whether our model predicted &lt;code&gt;PS&lt;/code&gt; or &lt;code&gt;WS&lt;/code&gt; for each cell. But, behind those predictions, the model is actually estimating a probability. A simple 50% probability cutoff is used to categorize a cell as poorly segmented.&lt;/p&gt;
&lt;p&gt;The 
&lt;a href=&#34;https://tidymodels.github.io/yardstick/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;yardstick package&lt;/a&gt; has functions for computing both of these measures called &lt;code&gt;roc_auc()&lt;/code&gt; and &lt;code&gt;accuracy()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;At first glance, it might seem like a good idea to use the training set data to compute these statistics. (This is actually a very bad idea.) Let&amp;rsquo;s see what happens if we try this. To evaluate performance based on the training set, we call the &lt;code&gt;predict()&lt;/code&gt; method to get both types of predictions (i.e. probabilities and hard class predictions).&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;rf_training_pred &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;predict&lt;/span&gt;(rf_fit, cell_train) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;bind_cols&lt;/span&gt;(&lt;span style=&#34;color:#00f&#34;&gt;predict&lt;/span&gt;(rf_fit, cell_train, type &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;prob&amp;#34;&lt;/span&gt;)) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# Add the true outcome data back in&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;bind_cols&lt;/span&gt;(cell_train &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
              &lt;span style=&#34;color:#00f&#34;&gt;select&lt;/span&gt;(class))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Using the yardstick functions, this model has spectacular results, so spectacular that you might be starting to get suspicious:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;rf_training_pred &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;                &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# training set predictions&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;roc_auc&lt;/span&gt;(truth &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; class, .pred_PS)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 1 x 3&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   .metric .estimator .estimate&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 1 roc_auc binary          1.00&lt;/span&gt;
rf_training_pred &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;                &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# training set predictions&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;accuracy&lt;/span&gt;(truth &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; class, .pred_class)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 1 x 3&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   .metric  .estimator .estimate&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 1 accuracy binary         0.993&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now that we have this model with exceptional performance, we proceed to the test set. Unfortunately, we discover that, although our results aren&amp;rsquo;t bad, they are certainly worse than what we initially thought based on predicting the training set:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;rf_testing_pred &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;predict&lt;/span&gt;(rf_fit, cell_test) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;bind_cols&lt;/span&gt;(&lt;span style=&#34;color:#00f&#34;&gt;predict&lt;/span&gt;(rf_fit, cell_test, type &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;prob&amp;#34;&lt;/span&gt;)) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;bind_cols&lt;/span&gt;(cell_test &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;select&lt;/span&gt;(class))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;rf_testing_pred &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;                   &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# test set predictions&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;roc_auc&lt;/span&gt;(truth &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; class, .pred_PS)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 1 x 3&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   .metric .estimator .estimate&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 1 roc_auc binary         0.909&lt;/span&gt;
rf_testing_pred &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;                   &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# test set predictions&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;accuracy&lt;/span&gt;(truth &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; class, .pred_class)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 1 x 3&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   .metric  .estimator .estimate&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 1 accuracy binary         0.837&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;what-happened-here&#34;&gt;What happened here?&lt;/h3&gt;
&lt;p&gt;There are several reasons why training set statistics like the ones shown in this section can be unrealistically optimistic:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Models like random forests, neural networks, and other black-box methods can essentially memorize the training set. Re-predicting that same set should always result in nearly perfect results.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The training set does not have the capacity to be a good arbiter of performance. It is not an independent piece of information; predicting the training set can only reflect what the model already knows.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To understand that second point better, think about an analogy from teaching. Suppose you give a class a test, then give them the answers, then provide the same test. The student scores on the &lt;em&gt;second&lt;/em&gt; test do not accurately reflect what they know about the subject; these scores would probably be higher than their results on the first test.&lt;/p&gt;
&lt;h2 id=&#34;resampling&#34;&gt;Resampling to the rescue&lt;/h2&gt;
&lt;p&gt;Resampling methods, such as cross-validation and the bootstrap, are empirical simulation systems. They create a series of data sets similar to the training/testing split discussed previously; a subset of the data are used for creating the model and a different subset is used to measure performance. Resampling is always used with the &lt;em&gt;training set&lt;/em&gt;. This schematic from 
&lt;a href=&#34;https://bookdown.org/max/FES/resampling.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kuhn and Johnson (2019)&lt;/a&gt; illustrates data usage for resampling methods:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/resampling.svg&#34; width=&#34;85%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the first level of this diagram, you see what happens when you use &lt;code&gt;rsample::initial_split()&lt;/code&gt;, which splits the original data into training and test sets. Then, the training set is chosen for resampling, and the test set is held out.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s use 10-fold cross-validation (CV) in this example. This method randomly allocates the 1515 cells in the training set to 10 groups of roughly equal size, called &amp;ldquo;folds&amp;rdquo;. For the first iteration of resampling, the first fold of about 151 cells are held out for the purpose of measuring performance. This is similar to a test set but, to avoid confusion, we call these data the &lt;em&gt;assessment set&lt;/em&gt; in the tidymodels framework.&lt;/p&gt;
&lt;p&gt;The other 90% of the data (about 1363 cells) are used to fit the model. Again, this sounds similar to a training set, so in tidymodels we call this data the &lt;em&gt;analysis set&lt;/em&gt;. This model, trained on the analysis set, is applied to the assessment set to generate predictions, and performance statistics are computed based on those predictions.&lt;/p&gt;
&lt;p&gt;In this example, 10-fold CV moves iteratively through the folds and leaves a different 10% out each time for model assessment. At the end of this process, there are 10 sets of performance statistics that were created on 10 data sets that were not used in the modeling process. For the cell example, this means 10 accuracies and 10 areas under the ROC curve. While 10 models were created, these are not used further; we do not keep the models themselves trained on these folds because their only purpose is calculating performance metrics.&lt;/p&gt;
&lt;p&gt;The final resampling estimates for the model are the &lt;strong&gt;averages&lt;/strong&gt; of the performance statistics replicates. For example, suppose for our data the results were:&lt;/p&gt;
&lt;table class=&#34;table&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; resample &lt;/th&gt;
   &lt;th style=&#34;text-align:right;&#34;&gt; accuracy &lt;/th&gt;
   &lt;th style=&#34;text-align:right;&#34;&gt; roc_auc &lt;/th&gt;
   &lt;th style=&#34;text-align:right;&#34;&gt; assessment size &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Fold01 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.7828947 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.8419206 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 152 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Fold02 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.8092105 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.8939982 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 152 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Fold03 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.8486842 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.9174923 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 152 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Fold04 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.8355263 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.8941946 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 152 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Fold05 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.8684211 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.9063232 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 152 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Fold06 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.8410596 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.9136661 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 151 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Fold07 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.8807947 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.9368932 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 151 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Fold08 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.7814570 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.8890798 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 151 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Fold09 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.8145695 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.9075369 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 151 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Fold10 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.8675497 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.9310806 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 151 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;From these resampling statistics, the final estimate of performance for this random forest model would be 0.903 for the area under the ROC curve and 0.833 for accuracy.&lt;/p&gt;
&lt;p&gt;These resampling statistics are an effective method for measuring model performance &lt;em&gt;without&lt;/em&gt; predicting the training set directly as a whole.&lt;/p&gt;
&lt;h2 id=&#34;fit-resamples&#34;&gt;Fit a model with resampling&lt;/h2&gt;
&lt;p&gt;To generate these results, the first step is to create a resampling object using rsample. There are 
&lt;a href=&#34;https://tidymodels.github.io/rsample/reference/index.html#section-resampling-methods&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;several resampling methods&lt;/a&gt; implemented in rsample; cross-validation folds can be created using &lt;code&gt;vfold_cv()&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;set.seed&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;345&lt;/span&gt;)
folds &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;vfold_cv&lt;/span&gt;(cell_train, v &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;10&lt;/span&gt;)
folds
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; #  10-fold cross-validation &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 10 x 2&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;    splits             id    &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;    &amp;lt;named list&amp;gt;       &amp;lt;chr&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  1 &amp;lt;split [1.4K/152]&amp;gt; Fold01&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  2 &amp;lt;split [1.4K/152]&amp;gt; Fold02&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  3 &amp;lt;split [1.4K/152]&amp;gt; Fold03&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  4 &amp;lt;split [1.4K/152]&amp;gt; Fold04&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  5 &amp;lt;split [1.4K/152]&amp;gt; Fold05&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  6 &amp;lt;split [1.4K/151]&amp;gt; Fold06&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  7 &amp;lt;split [1.4K/151]&amp;gt; Fold07&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  8 &amp;lt;split [1.4K/151]&amp;gt; Fold08&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  9 &amp;lt;split [1.4K/151]&amp;gt; Fold09&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 10 &amp;lt;split [1.4K/151]&amp;gt; Fold10&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The list column for &lt;code&gt;splits&lt;/code&gt; contains the information on which rows belong in the analysis and assessment sets. There are functions that can be used to extract the individual resampled data called &lt;code&gt;analysis()&lt;/code&gt; and &lt;code&gt;assessment()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;However, the tune package contains high-level functions that can do the required computations to resample a model for the purpose of measuring performance. You have several options for building an object for resampling:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Resample a model specification preprocessed with a formula or 
&lt;a href=&#34;https://nutriverse.io/start/recipes/&#34;&gt;recipe&lt;/a&gt;, or&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Resample a 
&lt;a href=&#34;https://tidymodels.github.io/workflows/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;workflow()&lt;/code&gt;&lt;/a&gt; that bundles together a model specification and formula/recipe.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For this example, let&amp;rsquo;s use a &lt;code&gt;workflow()&lt;/code&gt; that bundles together the random forest model and a formula, since we are not using a recipe. Whichever of these options you use, the syntax to &lt;code&gt;fit_resamples()&lt;/code&gt; is very similar to &lt;code&gt;fit()&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;rf_wf &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;workflow&lt;/span&gt;() &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;add_model&lt;/span&gt;(rf_mod) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;add_formula&lt;/span&gt;(class &lt;span style=&#34;color:#666&#34;&gt;~&lt;/span&gt; .)

&lt;span style=&#34;color:#00f&#34;&gt;set.seed&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;456&lt;/span&gt;)
rf_fit_rs &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  rf_wf &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;fit_resamples&lt;/span&gt;(folds)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;rf_fit_rs
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; #  10-fold cross-validation &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 10 x 4&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;    splits             id     .metrics         .notes          &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  * &amp;lt;list&amp;gt;             &amp;lt;chr&amp;gt;  &amp;lt;list&amp;gt;           &amp;lt;list&amp;gt;          &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  1 &amp;lt;split [1.4K/152]&amp;gt; Fold01 &amp;lt;tibble [2 × 3]&amp;gt; &amp;lt;tibble [0 × 1]&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  2 &amp;lt;split [1.4K/152]&amp;gt; Fold02 &amp;lt;tibble [2 × 3]&amp;gt; &amp;lt;tibble [0 × 1]&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  3 &amp;lt;split [1.4K/152]&amp;gt; Fold03 &amp;lt;tibble [2 × 3]&amp;gt; &amp;lt;tibble [0 × 1]&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  4 &amp;lt;split [1.4K/152]&amp;gt; Fold04 &amp;lt;tibble [2 × 3]&amp;gt; &amp;lt;tibble [0 × 1]&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  5 &amp;lt;split [1.4K/152]&amp;gt; Fold05 &amp;lt;tibble [2 × 3]&amp;gt; &amp;lt;tibble [0 × 1]&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  6 &amp;lt;split [1.4K/151]&amp;gt; Fold06 &amp;lt;tibble [2 × 3]&amp;gt; &amp;lt;tibble [0 × 1]&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  7 &amp;lt;split [1.4K/151]&amp;gt; Fold07 &amp;lt;tibble [2 × 3]&amp;gt; &amp;lt;tibble [0 × 1]&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  8 &amp;lt;split [1.4K/151]&amp;gt; Fold08 &amp;lt;tibble [2 × 3]&amp;gt; &amp;lt;tibble [0 × 1]&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  9 &amp;lt;split [1.4K/151]&amp;gt; Fold09 &amp;lt;tibble [2 × 3]&amp;gt; &amp;lt;tibble [0 × 1]&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 10 &amp;lt;split [1.4K/151]&amp;gt; Fold10 &amp;lt;tibble [2 × 3]&amp;gt; &amp;lt;tibble [0 × 1]&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The results are similar to the &lt;code&gt;folds&lt;/code&gt; results with some extra columns. The column &lt;code&gt;.metrics&lt;/code&gt; contains the performance statistics created from the 10 assessment sets. These can be manually unnested but the tune package contains a number of simple functions that can extract these data:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;collect_metrics&lt;/span&gt;(rf_fit_rs)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 2 x 5&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   .metric  .estimator  mean     n std_err&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 1 accuracy binary     0.833    10 0.0111 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 2 roc_auc  binary     0.903    10 0.00842&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Think about these values we now have for accuracy and AUC. These performance metrics are now more realistic (i.e. lower) than our ill-advised first attempt at computing performance metrics in the section above. If we wanted to try different model types for this data set, we could more confidently compare performance metrics computed using resampling to choose between models. Also, remember that at the end of our project, we return to our test set to estimate final model performance. We have looked at this once already before we started using resampling, but let&amp;rsquo;s remind ourselves of the results:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;rf_testing_pred &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;                   &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# test set predictions&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;roc_auc&lt;/span&gt;(truth &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; class, .pred_PS)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 1 x 3&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   .metric .estimator .estimate&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 1 roc_auc binary         0.909&lt;/span&gt;
rf_testing_pred &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;                   &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# test set predictions&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;accuracy&lt;/span&gt;(truth &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; class, .pred_class)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 1 x 3&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   .metric  .estimator .estimate&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 1 accuracy binary         0.837&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The performance metrics from the test set are much closer to the performance metrics computed using resampling than our first (&amp;ldquo;bad idea&amp;rdquo;) attempt. Resampling allows us to simulate how well our model will perform on new data, and the test set acts as the final, unbiased check for our model&amp;rsquo;s performance.&lt;/p&gt;
&lt;h2 id=&#34;session-information&#34;&gt;Session information&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;#&amp;gt; ─ Session info ───────────────────────────────────────────────────────────────
#&amp;gt;  setting  value                       
#&amp;gt;  version  R version 3.6.2 (2019-12-12)
#&amp;gt;  os       macOS Mojave 10.14.6        
#&amp;gt;  system   x86_64, darwin15.6.0        
#&amp;gt;  ui       X11                         
#&amp;gt;  language (EN)                        
#&amp;gt;  collate  en_US.UTF-8                 
#&amp;gt;  ctype    en_US.UTF-8                 
#&amp;gt;  tz       America/Denver              
#&amp;gt;  date     2020-04-21                  
#&amp;gt; 
#&amp;gt; ─ Packages ───────────────────────────────────────────────────────────────────
#&amp;gt;  package    * version date       lib source        
#&amp;gt;  broom      * 0.5.5   2020-02-29 [1] CRAN (R 3.6.0)
#&amp;gt;  dials      * 0.0.6   2020-04-03 [1] CRAN (R 3.6.2)
#&amp;gt;  dplyr      * 0.8.5   2020-03-07 [1] CRAN (R 3.6.0)
#&amp;gt;  ggplot2    * 3.3.0   2020-03-05 [1] CRAN (R 3.6.0)
#&amp;gt;  infer      * 0.5.1   2019-11-19 [1] CRAN (R 3.6.0)
#&amp;gt;  modeldata  * 0.0.1   2019-12-06 [1] CRAN (R 3.6.0)
#&amp;gt;  parsnip    * 0.1.0   2020-04-09 [1] CRAN (R 3.6.2)
#&amp;gt;  purrr      * 0.3.4   2020-04-17 [1] CRAN (R 3.6.2)
#&amp;gt;  ranger     * 0.12.1  2020-01-10 [1] CRAN (R 3.6.0)
#&amp;gt;  recipes    * 0.1.10  2020-03-18 [1] CRAN (R 3.6.0)
#&amp;gt;  rlang        0.4.5   2020-03-01 [1] CRAN (R 3.6.0)
#&amp;gt;  rsample    * 0.0.6   2020-03-31 [1] CRAN (R 3.6.2)
#&amp;gt;  tibble     * 2.1.3   2019-06-06 [1] CRAN (R 3.6.2)
#&amp;gt;  tidymodels * 0.1.0   2020-02-16 [1] CRAN (R 3.6.0)
#&amp;gt;  tune       * 0.1.0   2020-04-02 [1] CRAN (R 3.6.2)
#&amp;gt;  workflows  * 0.1.1   2020-03-17 [1] CRAN (R 3.6.0)
#&amp;gt;  yardstick  * 0.0.6   2020-03-17 [1] CRAN (R 3.6.0)
#&amp;gt; 
#&amp;gt; [1] /Library/Frameworks/R.framework/Versions/3.6/Resources/library
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Iterative Bayesian optimization of a classification model</title>
      <link>https://nutriverse.io/learn/work/bayes-opt/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://nutriverse.io/learn/work/bayes-opt/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;To use the code in this article, you will need to install the following packages: kernlab, modeldata, tidymodels, and tidyr.&lt;/p&gt;
&lt;p&gt;Many of the examples for model tuning focus on 
&lt;a href=&#34;https://nutriverse.io/learn/work/tune-svm/&#34;&gt;grid search&lt;/a&gt;. For that method, all the candidate tuning parameter combinations are defined prior to evaluation. Alternatively, &lt;em&gt;iterative search&lt;/em&gt; can be used to analyze the existing tuning parameter results and then &lt;em&gt;predict&lt;/em&gt; which tuning parameters to try next.&lt;/p&gt;
&lt;p&gt;There are a variety of methods for iterative search and the focus in this article is on &lt;em&gt;Bayesian optimization&lt;/em&gt;. For more information on this method, these resources might be helpful:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C7&amp;amp;q=Practical&amp;#43;Bayesian&amp;#43;Optimization&amp;#43;of&amp;#43;Machine&amp;#43;Learning&amp;#43;Algorithms&amp;amp;btnG=&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Practical bayesian optimization of machine learning algorithms&lt;/em&gt;&lt;/a&gt; (2012). J Snoek, H Larochelle, and RP Adams. Advances in neural information.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.cs.toronto.edu/~rgrosse/courses/csc411_f18/tutorials/tut8_adams_slides.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;A Tutorial on Bayesian Optimization for Machine Learning&lt;/em&gt;&lt;/a&gt; (2018). R Adams.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;http://www.gaussianprocess.org/gpml/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Gaussian Processes for Machine Learning&lt;/em&gt;&lt;/a&gt; (2006). C E Rasmussen and C Williams.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C7&amp;amp;q=%22Bayesian&amp;#43;Optimization%22&amp;amp;btnG=&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Other articles!&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;cell-segmenting-revisited&#34;&gt;Cell segmenting revisited&lt;/h2&gt;
&lt;p&gt;To demonstrate this approach to tuning models, let&amp;rsquo;s return to the cell segmentation data from the 
&lt;a href=&#34;https://nutriverse.io/start/resampling/&#34;&gt;Getting Started&lt;/a&gt; article on resampling:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;library&lt;/span&gt;(tidymodels)
&lt;span style=&#34;color:#00f&#34;&gt;library&lt;/span&gt;(modeldata)

&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# Load data&lt;/span&gt;
&lt;span style=&#34;color:#00f&#34;&gt;data&lt;/span&gt;(cells)

&lt;span style=&#34;color:#00f&#34;&gt;set.seed&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;2369&lt;/span&gt;)
tr_te_split &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;initial_split&lt;/span&gt;(cells &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;select&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;-&lt;/span&gt;case), prop &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;3&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;4&lt;/span&gt;)
cell_train &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;training&lt;/span&gt;(tr_te_split)
cell_test  &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;testing&lt;/span&gt;(tr_te_split)

&lt;span style=&#34;color:#00f&#34;&gt;set.seed&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;1697&lt;/span&gt;)
folds &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;vfold_cv&lt;/span&gt;(cell_train, v &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;10&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;the-tuning-scheme&#34;&gt;The tuning scheme&lt;/h2&gt;
&lt;p&gt;Since the predictors are highly correlated, we can used a recipe to convert the original predictors to principal component scores. There is also slight class imbalance in these data; about 64% of the data are poorly segmented. To mitigate this, the data will be down-sampled at the end of the pre-processing so that the number of poorly and well segmented cells occur with equal frequency. We can use a recipe for all this pre-processing, but the number of principal components will need to be &lt;em&gt;tuned&lt;/em&gt; so that we have enough (but not too many) representations of the data.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;cell_pre_proc &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;recipe&lt;/span&gt;(class &lt;span style=&#34;color:#666&#34;&gt;~&lt;/span&gt; ., data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; cell_train) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;step_YeoJohnson&lt;/span&gt;(&lt;span style=&#34;color:#00f&#34;&gt;all_predictors&lt;/span&gt;()) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;step_normalize&lt;/span&gt;(&lt;span style=&#34;color:#00f&#34;&gt;all_predictors&lt;/span&gt;()) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;step_pca&lt;/span&gt;(&lt;span style=&#34;color:#00f&#34;&gt;all_predictors&lt;/span&gt;(), num_comp &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;tune&lt;/span&gt;()) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;step_downsample&lt;/span&gt;(class)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In this analysis, we will use a support vector machine to model the data. Let&amp;rsquo;s use a radial basis function (RBF) kernel and tune its main parameter ($\sigma$). Additionally, the main SVM parameter, the cost value, also needs optimization.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;svm_mod &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;svm_rbf&lt;/span&gt;(mode &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;classification&amp;#34;&lt;/span&gt;, cost &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;tune&lt;/span&gt;(), rbf_sigma &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;tune&lt;/span&gt;()) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;set_engine&lt;/span&gt;(&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;kernlab&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;These two objects (the recipe and model) will be combined into a single object via the &lt;code&gt;workflow()&lt;/code&gt; function from the 
&lt;a href=&#34;https://tidymodels.github.io/workflows/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;workflows&lt;/a&gt; package; this object will be used in the optimization process.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;svm_wflow &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;workflow&lt;/span&gt;() &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;add_model&lt;/span&gt;(svm_mod) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;add_recipe&lt;/span&gt;(cell_pre_proc)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;From this object, we can derive information about what parameters are slated to be tuned. A parameter set is derived by:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;svm_set &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;parameters&lt;/span&gt;(svm_wflow)
svm_set
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Collection of 3 parameters for tuning&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;         id parameter type object class&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;       cost           cost    nparam[+]&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  rbf_sigma      rbf_sigma    nparam[+]&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   num_comp       num_comp    nparam[+]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The default range for the number of PCA components is rather small for this data set. A member of the parameter set can be modified using the &lt;code&gt;update()&lt;/code&gt; function. Let&amp;rsquo;s constrain the search to one to twenty components by updating the &lt;code&gt;num_comp&lt;/code&gt; parameter. Additionally, the lower bound of this parameter is set to zero which specifies that the original predictor set should also be evaluated (i.e., with no PCA step at all):&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;svm_set &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  svm_set &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;update&lt;/span&gt;(num_comp &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;num_comp&lt;/span&gt;(&lt;span style=&#34;color:#00f&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;0L&lt;/span&gt;, &lt;span style=&#34;color:#666&#34;&gt;20L&lt;/span&gt;)))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;sequential-tuning&#34;&gt;Sequential tuning&lt;/h2&gt;
&lt;p&gt;Bayesian optimization is a sequential method that uses a model to predict new candidate parameters for assessment. When scoring potential parameter value, the mean and variance of performance are predicted. The strategy used to define how these two statistical quantities are used is defined by an &lt;em&gt;acquisition function&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;For example, one approach for scoring new candidates is to use a confidence bound. Suppose accuracy is being optimized. For a metric that we want to maximize, a lower confidence bound can be used. The multiplier on the standard error (denoted as &lt;code&gt;\(\kappa\)&lt;/code&gt;) is a value that can be used to make trade-offs between &lt;strong&gt;exploration&lt;/strong&gt; and &lt;strong&gt;exploitation&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Exploration&lt;/strong&gt; means that the search will consider candidates in untested space.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Exploitation&lt;/strong&gt; focuses in areas where the previous best results occurred.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The variance predicted by the Bayesian model is mostly spatial variation; the value will be large for candidate values that are not close to values that have already been evaluated. If the standard error multiplier is high, the search process will be more likely to avoid areas without candidate values in the vicinity.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll use another acquisition function, &lt;em&gt;expected improvement&lt;/em&gt;, that determines which candidates are likely to be helpful relative to the current best results. This is the default acquisition function. More information on these functions can be found in the 
&lt;a href=&#34;https://tidymodels.github.io/tune/articles/acquisition_functions.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;package vignette for acquisition functions&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;set.seed&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;12&lt;/span&gt;)
search_res &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt;
  svm_wflow &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;tune_bayes&lt;/span&gt;(
    resamples &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; folds,
    &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# To use non-default parameter ranges&lt;/span&gt;
    param_info &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; svm_set,
    &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# Generate five at semi-random to start&lt;/span&gt;
    initial &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;5&lt;/span&gt;,
    iter &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;50&lt;/span&gt;,
    &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# How to measure performance?&lt;/span&gt;
    metrics &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;metric_set&lt;/span&gt;(roc_auc),
    control &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;control_bayes&lt;/span&gt;(no_improve &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;30&lt;/span&gt;, verbose &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;TRUE&lt;/span&gt;)
  )
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &amp;gt;  Generating a set of 5 initial parameter results&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Initialization complete&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Optimizing roc_auc using the expected improvement&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ── Iteration 1 ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Current best:		roc_auc=0.8655 (@iter 0)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Generating 5000 candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Predicted candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i cost=0.58, rbf_sigma=1.54e-09, num_comp=12&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ⓧ Newest results:	roc_auc=0.8624 (+/-0.00897)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ── Iteration 2 ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Current best:		roc_auc=0.8655 (@iter 0)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Generating 5000 candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Predicted candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i cost=0.0251, rbf_sigma=6.36e-06, num_comp=16&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ⓧ Newest results:	roc_auc=0.8606 (+/-0.00908)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ── Iteration 3 ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Current best:		roc_auc=0.8655 (@iter 0)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Generating 5000 candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Predicted candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i cost=23, rbf_sigma=1.02e-10, num_comp=7&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ⓧ Newest results:	roc_auc=0.8634 (+/-0.00923)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ── Iteration 4 ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Current best:		roc_auc=0.8655 (@iter 0)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Generating 5000 candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Predicted candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i cost=0.0894, rbf_sigma=1.09e-10, num_comp=0&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ⓧ Newest results:	roc_auc=0.8494 (+/-0.0116)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ── Iteration 5 ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Current best:		roc_auc=0.8655 (@iter 0)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Generating 5000 candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Predicted candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i cost=0.402, rbf_sigma=0.413, num_comp=20&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ⓧ Newest results:	roc_auc=0.8236 (+/-0.00885)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ── Iteration 6 ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Current best:		roc_auc=0.8655 (@iter 0)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Generating 5000 candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Predicted candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i cost=24, rbf_sigma=0.942, num_comp=8&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ⓧ Newest results:	roc_auc=0.8054 (+/-0.0114)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ── Iteration 7 ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Current best:		roc_auc=0.8655 (@iter 0)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Generating 5000 candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Predicted candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i cost=30.3, rbf_sigma=2.25e-06, num_comp=13&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ⓧ Newest results:	roc_auc=0.8622 (+/-0.009)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ── Iteration 8 ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Current best:		roc_auc=0.8655 (@iter 0)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Generating 5000 candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Predicted candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i cost=25, rbf_sigma=1.07e-10, num_comp=20&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ⓧ Newest results:	roc_auc=0.8655 (+/-0.00848)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ── Iteration 9 ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Current best:		roc_auc=0.8655 (@iter 0)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Generating 5000 candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Predicted candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i cost=2.1, rbf_sigma=5.29e-06, num_comp=0&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ⓧ Newest results:	roc_auc=0.8494 (+/-0.0116)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ── Iteration 10 ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Current best:		roc_auc=0.8655 (@iter 0)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Generating 5000 candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Predicted candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i cost=9.87, rbf_sigma=0.000395, num_comp=20&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ♥ Newest results:	roc_auc=0.8681 (+/-0.00898)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ── Iteration 11 ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Current best:		roc_auc=0.8681 (@iter 10)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Generating 5000 candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Predicted candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i cost=0.073, rbf_sigma=0.000585, num_comp=0&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ⓧ Newest results:	roc_auc=0.8509 (+/-0.0116)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ── Iteration 12 ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Current best:		roc_auc=0.8681 (@iter 10)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Generating 5000 candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Predicted candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i cost=0.00101, rbf_sigma=1.29e-07, num_comp=0&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ⓧ Newest results:	roc_auc=0.8494 (+/-0.0116)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ── Iteration 13 ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Current best:		roc_auc=0.8681 (@iter 10)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Generating 5000 candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Predicted candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i cost=0.0553, rbf_sigma=0.000291, num_comp=12&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ⓧ Newest results:	roc_auc=0.8625 (+/-0.00898)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ── Iteration 14 ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Current best:		roc_auc=0.8681 (@iter 10)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Generating 5000 candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Predicted candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i cost=11.8, rbf_sigma=0.00143, num_comp=20&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ♥ Newest results:	roc_auc=0.8691 (+/-0.00837)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ── Iteration 15 ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Current best:		roc_auc=0.8691 (@iter 14)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Generating 5000 candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Predicted candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i cost=0.0915, rbf_sigma=0.03, num_comp=20&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ♥ Newest results:	roc_auc=0.8728 (+/-0.00842)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ── Iteration 16 ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Current best:		roc_auc=0.8728 (@iter 15)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Generating 5000 candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Predicted candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i cost=0.0289, rbf_sigma=8.48e-09, num_comp=20&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ⓧ Newest results:	roc_auc=0.8655 (+/-0.00848)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ── Iteration 17 ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Current best:		roc_auc=0.8728 (@iter 15)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Generating 5000 candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Predicted candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i cost=0.0021, rbf_sigma=0.0109, num_comp=20&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ⓧ Newest results:	roc_auc=0.8696 (+/-0.00881)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ── Iteration 18 ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Current best:		roc_auc=0.8728 (@iter 15)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Generating 5000 candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Predicted candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i cost=0.461, rbf_sigma=0.908, num_comp=0&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ⓧ Newest results:	roc_auc=0.7732 (+/-0.0168)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ── Iteration 19 ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Current best:		roc_auc=0.8728 (@iter 15)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Generating 5000 candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Predicted candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i cost=0.00132, rbf_sigma=8.1e-08, num_comp=11&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ⓧ Newest results:	roc_auc=0.8621 (+/-0.00933)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ── Iteration 20 ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Current best:		roc_auc=0.8728 (@iter 15)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Generating 5000 candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Predicted candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i cost=20.2, rbf_sigma=1.64e-09, num_comp=0&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ⓧ Newest results:	roc_auc=0.8494 (+/-0.0116)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ── Iteration 21 ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Current best:		roc_auc=0.8728 (@iter 15)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Generating 5000 candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Predicted candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i cost=0.00173, rbf_sigma=0.126, num_comp=11&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ⓧ Newest results:	roc_auc=0.8721 (+/-0.00749)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ── Iteration 22 ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Current best:		roc_auc=0.8728 (@iter 15)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Generating 5000 candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Predicted candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i cost=0.00853, rbf_sigma=0.989, num_comp=0&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ⓧ Newest results:	roc_auc=0.7369 (+/-0.0313)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ── Iteration 23 ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Current best:		roc_auc=0.8728 (@iter 15)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Generating 5000 candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Predicted candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i cost=0.00673, rbf_sigma=1.55e-10, num_comp=17&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ⓧ Newest results:	roc_auc=0.787 (+/-0.0485)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ── Iteration 24 ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Current best:		roc_auc=0.8728 (@iter 15)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Generating 5000 candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Predicted candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i cost=0.871, rbf_sigma=1.72e-10, num_comp=19&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ⓧ Newest results:	roc_auc=0.864 (+/-0.00842)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ── Iteration 25 ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Current best:		roc_auc=0.8728 (@iter 15)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Generating 5000 candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Predicted candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i cost=3.8, rbf_sigma=6.24e-10, num_comp=19&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ⓧ Newest results:	roc_auc=0.864 (+/-0.00842)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ── Iteration 26 ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Current best:		roc_auc=0.8728 (@iter 15)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Generating 5000 candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Predicted candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i cost=5.2, rbf_sigma=0.791, num_comp=1&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ⓧ Newest results:	roc_auc=0.7319 (+/-0.0173)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ── Iteration 27 ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Current best:		roc_auc=0.8728 (@iter 15)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Generating 5000 candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Predicted candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i cost=0.213, rbf_sigma=9.11e-10, num_comp=20&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ⓧ Newest results:	roc_auc=0.8655 (+/-0.00848)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ── Iteration 28 ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Current best:		roc_auc=0.8728 (@iter 15)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Generating 5000 candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Predicted candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i cost=6.99, rbf_sigma=3.03e-10, num_comp=0&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ⓧ Newest results:	roc_auc=0.8494 (+/-0.0116)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ── Iteration 29 ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Current best:		roc_auc=0.8728 (@iter 15)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Generating 5000 candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Predicted candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i cost=0.00102, rbf_sigma=0.344, num_comp=5&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ⓧ Newest results:	roc_auc=0.8631 (+/-0.0079)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ── Iteration 30 ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Current best:		roc_auc=0.8728 (@iter 15)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Generating 5000 candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Predicted candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i cost=20.3, rbf_sigma=1.28e-05, num_comp=3&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ⓧ Newest results:	roc_auc=0.8503 (+/-0.0112)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ── Iteration 31 ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Current best:		roc_auc=0.8728 (@iter 15)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Generating 5000 candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Predicted candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i cost=0.0012, rbf_sigma=3.75e-05, num_comp=7&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ⓧ Newest results:	roc_auc=0.8634 (+/-0.00923)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ── Iteration 32 ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Current best:		roc_auc=0.8728 (@iter 15)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Generating 5000 candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Predicted candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i cost=0.0142, rbf_sigma=2.58e-10, num_comp=1&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ⓧ Newest results:	roc_auc=0.7015 (+/-0.0374)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ── Iteration 33 ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Current best:		roc_auc=0.8728 (@iter 15)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Generating 5000 candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Predicted candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i cost=0.00411, rbf_sigma=0.557, num_comp=1&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ⓧ Newest results:	roc_auc=0.747 (+/-0.0173)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ── Iteration 34 ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Current best:		roc_auc=0.8728 (@iter 15)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Generating 5000 candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Predicted candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i cost=0.161, rbf_sigma=0.167, num_comp=1&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ⓧ Newest results:	roc_auc=0.7541 (+/-0.0177)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ── Iteration 35 ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Current best:		roc_auc=0.8728 (@iter 15)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Generating 5000 candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Predicted candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i cost=2.48, rbf_sigma=0.783, num_comp=19&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ⓧ Newest results:	roc_auc=0.7748 (+/-0.014)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ── Iteration 36 ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Current best:		roc_auc=0.8728 (@iter 15)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Generating 5000 candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Predicted candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i cost=0.0138, rbf_sigma=0.624, num_comp=19&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ⓧ Newest results:	roc_auc=0.7938 (+/-0.0117)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ── Iteration 37 ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Current best:		roc_auc=0.8728 (@iter 15)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Generating 5000 candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Predicted candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i cost=0.00341, rbf_sigma=1.11e-10, num_comp=2&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ⓧ Newest results:	roc_auc=0.7311 (+/-0.0404)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ── Iteration 38 ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Current best:		roc_auc=0.8728 (@iter 15)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Generating 5000 candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Predicted candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i cost=0.00113, rbf_sigma=1.48e-10, num_comp=14&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ⓧ Newest results:	roc_auc=0.7888 (+/-0.0489)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ── Iteration 39 ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Current best:		roc_auc=0.8728 (@iter 15)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Generating 5000 candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Predicted candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i cost=17.1, rbf_sigma=1.71e-10, num_comp=9&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ⓧ Newest results:	roc_auc=0.8638 (+/-0.00874)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ── Iteration 40 ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Current best:		roc_auc=0.8728 (@iter 15)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Generating 5000 candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Predicted candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i cost=13.3, rbf_sigma=0.968, num_comp=17&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ⓧ Newest results:	roc_auc=0.7691 (+/-0.0158)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ── Iteration 41 ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Current best:		roc_auc=0.8728 (@iter 15)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Generating 5000 candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Predicted candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i cost=0.0026, rbf_sigma=0.995, num_comp=3&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ⓧ Newest results:	roc_auc=0.8496 (+/-0.0093)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ── Iteration 42 ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Current best:		roc_auc=0.8728 (@iter 15)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Generating 5000 candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Predicted candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i cost=23.6, rbf_sigma=0.856, num_comp=13&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ⓧ Newest results:	roc_auc=0.7972 (+/-0.0144)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ── Iteration 43 ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Current best:		roc_auc=0.8728 (@iter 15)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Generating 5000 candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Predicted candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i cost=0.00142, rbf_sigma=7.1e-06, num_comp=18&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ⓧ Newest results:	roc_auc=0.8593 (+/-0.00882)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ── Iteration 44 ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Current best:		roc_auc=0.8728 (@iter 15)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Generating 5000 candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Predicted candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i cost=31.4, rbf_sigma=1.7e-10, num_comp=15&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ⓧ Newest results:	roc_auc=0.8616 (+/-0.00899)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ── Iteration 45 ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Current best:		roc_auc=0.8728 (@iter 15)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Generating 5000 candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Predicted candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i cost=31.4, rbf_sigma=0.0118, num_comp=6&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ♥ Newest results:	roc_auc=0.8779 (+/-0.00726)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ── Iteration 46 ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Current best:		roc_auc=0.8779 (@iter 45)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Generating 5000 candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Predicted candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i cost=11, rbf_sigma=0.718, num_comp=10&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ⓧ Newest results:	roc_auc=0.8247 (+/-0.0103)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ── Iteration 47 ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Current best:		roc_auc=0.8779 (@iter 45)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Generating 5000 candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Predicted candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i cost=27.1, rbf_sigma=3.61e-06, num_comp=8&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ⓧ Newest results:	roc_auc=0.8645 (+/-0.00874)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ── Iteration 48 ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Current best:		roc_auc=0.8779 (@iter 45)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Generating 5000 candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Predicted candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i cost=20.4, rbf_sigma=1.23e-10, num_comp=4&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ⓧ Newest results:	roc_auc=0.8513 (+/-0.0109)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ── Iteration 49 ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Current best:		roc_auc=0.8779 (@iter 45)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Generating 5000 candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Predicted candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i cost=0.0011, rbf_sigma=0.677, num_comp=16&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ⓧ Newest results:	roc_auc=0.8075 (+/-0.0119)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ── Iteration 50 ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Current best:		roc_auc=0.8779 (@iter 45)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Gaussian process model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Generating 5000 candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Predicted candidates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i cost=0.00133, rbf_sigma=0.592, num_comp=14&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; i Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ✓ Estimating performance&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ⓧ Newest results:	roc_auc=0.8311 (+/-0.014)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The resulting tibble is a stacked set of rows of the rsample object with an additional column for the iteration number:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;search_res
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; #  10-fold cross-validation &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 510 x 5&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;    splits             id     .metrics         .notes           .iter&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  * &amp;lt;list&amp;gt;             &amp;lt;chr&amp;gt;  &amp;lt;list&amp;gt;           &amp;lt;list&amp;gt;           &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  1 &amp;lt;split [1.4K/152]&amp;gt; Fold01 &amp;lt;tibble [5 × 6]&amp;gt; &amp;lt;tibble [0 × 1]&amp;gt;     0&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  2 &amp;lt;split [1.4K/152]&amp;gt; Fold02 &amp;lt;tibble [5 × 6]&amp;gt; &amp;lt;tibble [0 × 1]&amp;gt;     0&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  3 &amp;lt;split [1.4K/152]&amp;gt; Fold03 &amp;lt;tibble [5 × 6]&amp;gt; &amp;lt;tibble [0 × 1]&amp;gt;     0&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  4 &amp;lt;split [1.4K/152]&amp;gt; Fold04 &amp;lt;tibble [5 × 6]&amp;gt; &amp;lt;tibble [0 × 1]&amp;gt;     0&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  5 &amp;lt;split [1.4K/152]&amp;gt; Fold05 &amp;lt;tibble [5 × 6]&amp;gt; &amp;lt;tibble [0 × 1]&amp;gt;     0&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  6 &amp;lt;split [1.4K/151]&amp;gt; Fold06 &amp;lt;tibble [5 × 6]&amp;gt; &amp;lt;tibble [0 × 1]&amp;gt;     0&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  7 &amp;lt;split [1.4K/151]&amp;gt; Fold07 &amp;lt;tibble [5 × 6]&amp;gt; &amp;lt;tibble [0 × 1]&amp;gt;     0&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  8 &amp;lt;split [1.4K/151]&amp;gt; Fold08 &amp;lt;tibble [5 × 6]&amp;gt; &amp;lt;tibble [0 × 1]&amp;gt;     0&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  9 &amp;lt;split [1.4K/151]&amp;gt; Fold09 &amp;lt;tibble [5 × 6]&amp;gt; &amp;lt;tibble [0 × 1]&amp;gt;     0&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 10 &amp;lt;split [1.4K/151]&amp;gt; Fold10 &amp;lt;tibble [5 × 6]&amp;gt; &amp;lt;tibble [0 × 1]&amp;gt;     0&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # … with 500 more rows&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;As with grid search, we can summarize the results over resamples:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;estimates &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;collect_metrics&lt;/span&gt;(search_res) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;arrange&lt;/span&gt;(.iter)

estimates
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 55 x 9&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;        cost rbf_sigma num_comp .iter .metric .estimator  mean     n std_err&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;       &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  1  0.00207  1.56e- 5       10     0 roc_auc binary     0.864    10 0.00888&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  2  0.0304   6.41e- 9        5     0 roc_auc binary     0.859    10 0.00922&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  3  0.348    4.43e- 2        1     0 roc_auc binary     0.757    10 0.0177 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  4  1.45     2.04e- 3       15     0 roc_auc binary     0.865    10 0.00962&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  5 15.5      1.28e- 7       20     0 roc_auc binary     0.865    10 0.00848&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  6  0.580    1.54e- 9       12     1 roc_auc binary     0.862    10 0.00897&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  7  0.0251   6.36e- 6       16     2 roc_auc binary     0.861    10 0.00908&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  8 23.0      1.02e-10        7     3 roc_auc binary     0.863    10 0.00923&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  9  0.0894   1.09e-10        0     4 roc_auc binary     0.849    10 0.0116 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 10  0.402    4.13e- 1       20     5 roc_auc binary     0.824    10 0.00885&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # … with 45 more rows&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The best performance of the initial set of candidate values was &lt;code&gt;AUC = 0.865 &lt;/code&gt;. The best results were achieved at iteration 45 with a corresponding AUC value of 0.878. The five best results are:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;show_best&lt;/span&gt;(search_res, metric &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;roc_auc&amp;#34;&lt;/span&gt;)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 5 x 9&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;       cost rbf_sigma num_comp .iter .metric .estimator  mean     n std_err&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;      &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 1 31.4       0.0118         6    45 roc_auc binary     0.878    10 0.00726&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 2  0.0915    0.0300        20    15 roc_auc binary     0.873    10 0.00842&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 3  0.00173   0.126         11    21 roc_auc binary     0.872    10 0.00749&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 4  0.00210   0.0109        20    17 roc_auc binary     0.870    10 0.00881&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 5 11.8       0.00143       20    14 roc_auc binary     0.869    10 0.00837&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;A plot of the search iterations can be created via:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;autoplot&lt;/span&gt;(search_res, type &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;performance&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;figs/bo-plot-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There are many parameter combinations have roughly equivalent results.&lt;/p&gt;
&lt;p&gt;How did the parameters change over iterations? Since two of the parameters are usually treated on the log scale, we can use &lt;code&gt;mutate()&lt;/code&gt; to transform them, and then construct a plot using ggplot2:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;library&lt;/span&gt;(tidyr)

&lt;span style=&#34;color:#00f&#34;&gt;collect_metrics&lt;/span&gt;(search_res) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;select&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;-.&lt;/span&gt;metric,&lt;span style=&#34;color:#666&#34;&gt;-.&lt;/span&gt;estimator,&lt;span style=&#34;color:#666&#34;&gt;-&lt;/span&gt;mean,&lt;span style=&#34;color:#666&#34;&gt;-&lt;/span&gt;n,&lt;span style=&#34;color:#666&#34;&gt;-&lt;/span&gt;std_err) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;mutate&lt;/span&gt;(cost &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;log10&lt;/span&gt;(cost), 
         rbf_sigma &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;log10&lt;/span&gt;(rbf_sigma)) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;pivot_longer&lt;/span&gt;(cols &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;-.i&lt;/span&gt;ter),
               names_to &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;parameter&amp;#34;&lt;/span&gt;,
               values_to &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;value&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;ggplot&lt;/span&gt;(&lt;span style=&#34;color:#00f&#34;&gt;aes&lt;/span&gt;(x &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; .iter, y &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; value)) &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;geom_point&lt;/span&gt;() &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;facet_wrap&lt;/span&gt;( &lt;span style=&#34;color:#666&#34;&gt;~&lt;/span&gt; parameter, scales &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;free_y&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;figs/bo-param-plot-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;session-information&#34;&gt;Session information&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;#&amp;gt; ─ Session info ───────────────────────────────────────────────────────────────
#&amp;gt;  setting  value                       
#&amp;gt;  version  R version 3.6.2 (2019-12-12)
#&amp;gt;  os       macOS Mojave 10.14.6        
#&amp;gt;  system   x86_64, darwin15.6.0        
#&amp;gt;  ui       X11                         
#&amp;gt;  language (EN)                        
#&amp;gt;  collate  en_US.UTF-8                 
#&amp;gt;  ctype    en_US.UTF-8                 
#&amp;gt;  tz       America/Denver              
#&amp;gt;  date     2020-04-17                  
#&amp;gt; 
#&amp;gt; ─ Packages ───────────────────────────────────────────────────────────────────
#&amp;gt;  package    * version date       lib source        
#&amp;gt;  broom      * 0.5.5   2020-02-29 [1] CRAN (R 3.6.0)
#&amp;gt;  dials      * 0.0.6   2020-04-03 [1] CRAN (R 3.6.2)
#&amp;gt;  dplyr      * 0.8.5   2020-03-07 [1] CRAN (R 3.6.0)
#&amp;gt;  ggplot2    * 3.3.0   2020-03-05 [1] CRAN (R 3.6.0)
#&amp;gt;  infer      * 0.5.1   2019-11-19 [1] CRAN (R 3.6.0)
#&amp;gt;  kernlab    * 0.9-29  2019-11-12 [1] CRAN (R 3.6.0)
#&amp;gt;  modeldata  * 0.0.1   2019-12-06 [1] CRAN (R 3.6.0)
#&amp;gt;  parsnip    * 0.1.0   2020-04-09 [1] CRAN (R 3.6.2)
#&amp;gt;  purrr      * 0.3.3   2019-10-18 [1] CRAN (R 3.6.0)
#&amp;gt;  recipes    * 0.1.10  2020-03-18 [1] CRAN (R 3.6.0)
#&amp;gt;  rlang      * 0.4.5   2020-03-01 [1] CRAN (R 3.6.0)
#&amp;gt;  rsample    * 0.0.6   2020-03-31 [1] CRAN (R 3.6.2)
#&amp;gt;  tibble     * 2.1.3   2019-06-06 [1] CRAN (R 3.6.2)
#&amp;gt;  tidymodels * 0.1.0   2020-02-16 [1] CRAN (R 3.6.0)
#&amp;gt;  tidyr      * 1.0.2   2020-01-24 [1] CRAN (R 3.6.0)
#&amp;gt;  tune       * 0.1.0   2020-04-02 [1] CRAN (R 3.6.2)
#&amp;gt;  workflows  * 0.1.1   2020-03-17 [1] CRAN (R 3.6.0)
#&amp;gt;  yardstick  * 0.0.6   2020-03-17 [1] CRAN (R 3.6.0)
#&amp;gt; 
#&amp;gt; [1] /Library/Frameworks/R.framework/Versions/3.6/Resources/library
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Hypothesis testing using resampling and tidy data</title>
      <link>https://nutriverse.io/learn/statistics/infer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://nutriverse.io/learn/statistics/infer/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This article only requires the tidymodels package.&lt;/p&gt;
&lt;p&gt;The tidymodels package 
&lt;a href=&#34;https://tidymodels.github.io/infer/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;infer&lt;/a&gt; implements an expressive grammar to perform statistical inference that coheres with the &lt;code&gt;tidyverse&lt;/code&gt; design framework. Rather than providing methods for specific statistical tests, this package consolidates the principles that are shared among common hypothesis tests into a set of 4 main verbs (functions), supplemented with many utilities to visualize and extract information from their outputs.&lt;/p&gt;
&lt;p&gt;Regardless of which hypothesis test we&amp;rsquo;re using, we&amp;rsquo;re still asking the same kind of question:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Is the effect or difference in our observed data real, or due to chance?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;To answer this question, we start by assuming that the observed data came from some world where &amp;ldquo;nothing is going on&amp;rdquo; (i.e. the observed effect was simply due to random chance), and call this assumption our &lt;strong&gt;null hypothesis&lt;/strong&gt;. (In reality, we might not believe in the null hypothesis at all; the null hypothesis is in opposition to the &lt;strong&gt;alternate hypothesis&lt;/strong&gt;, which supposes that the effect present in the observed data is actually due to the fact that &amp;ldquo;something is going on.&amp;quot;) We then calculate a &lt;strong&gt;test statistic&lt;/strong&gt; from our data that describes the observed effect. We can use this test statistic to calculate a &lt;strong&gt;p-value&lt;/strong&gt;, giving the probability that our observed data could come about if the null hypothesis was true. If this probability is below some pre-defined &lt;strong&gt;significance level&lt;/strong&gt; &lt;code&gt;\(\alpha\)&lt;/code&gt;, then we can reject our null hypothesis.&lt;/p&gt;
&lt;p&gt;If you are new to hypothesis testing, take a look at&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://moderndive.com/9-hypothesis-testing.html#understanding-ht&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Section 9.2 of &lt;em&gt;Statistical Inference via Data Science&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;The American Statistical Association&amp;rsquo;s recent 
&lt;a href=&#34;https://doi.org/10.1080/00031305.2016.1154108&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;statement on p-values&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The workflow of this package is designed around these ideas. Starting from some data set,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;specify()&lt;/code&gt; allows you to specify the variable, or relationship between variables, that you&amp;rsquo;re interested in,&lt;/li&gt;
&lt;li&gt;&lt;code&gt;hypothesize()&lt;/code&gt; allows you to declare the null hypothesis,&lt;/li&gt;
&lt;li&gt;&lt;code&gt;generate()&lt;/code&gt; allows you to generate data reflecting the null hypothesis, and&lt;/li&gt;
&lt;li&gt;&lt;code&gt;calculate()&lt;/code&gt; allows you to calculate a distribution of statistics from the generated data to form the null distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Throughout this vignette, we make use of &lt;code&gt;gss&lt;/code&gt;, a data set available in infer containing a sample of 500 observations of 11 variables from the &lt;em&gt;General Social Survey&lt;/em&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;library&lt;/span&gt;(tidymodels) &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# Includes the infer package&lt;/span&gt;

&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# load in the data set&lt;/span&gt;
&lt;span style=&#34;color:#00f&#34;&gt;data&lt;/span&gt;(gss)

&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# take a look at its structure&lt;/span&gt;
dplyr&lt;span style=&#34;color:#666&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#00f&#34;&gt;glimpse&lt;/span&gt;(gss)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Observations: 3,000&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Variables: 11&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; $ year    &amp;lt;dbl&amp;gt; 2008, 2006, 1985, 1987, 2006, 1986, 1977, 1998, 2012, 1982, 1…&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; $ age     &amp;lt;dbl&amp;gt; 37, 29, 58, 40, 39, 37, 53, 41, 55, 47, 36, 75, 22, 19, 34, 5…&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; $ sex     &amp;lt;fct&amp;gt; male, female, male, male, female, male, female, male, male, m…&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; $ college &amp;lt;fct&amp;gt; no degree, no degree, degree, degree, no degree, no degree, n…&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; $ partyid &amp;lt;fct&amp;gt; dem, dem, ind, rep, dem, dem, dem, rep, ind, rep, rep, rep, r…&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; $ hompop  &amp;lt;dbl&amp;gt; 4, 3, 3, 5, 5, 5, 4, 1, 5, 4, 5, 2, 3, 2, 5, 2, 5, 7, 1, 3, 4…&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; $ hours   &amp;lt;dbl&amp;gt; 50, NA, 60, 84, 40, 50, NA, 60, NA, 40, 20, NA, 40, 40, 20, 5…&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; $ income  &amp;lt;ord&amp;gt; $25000 or more, lt $1000, $25000 or more, $25000 or more, $60…&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; $ class   &amp;lt;fct&amp;gt; working class, middle class, middle class, middle class, NA, …&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; $ finrela &amp;lt;fct&amp;gt; below average, below average, far above average, far below av…&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; $ weight  &amp;lt;dbl&amp;gt; 0.875, 0.430, 1.554, 1.010, 0.859, 1.007, 0.988, 0.550, 3.496…&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Each row is an individual survey response, containing some basic demographic information on the respondent as well as some additional variables. See &lt;code&gt;?gss&lt;/code&gt; for more information on the variables included and their source. Note that this data (and our examples on it) are for demonstration purposes only, and will not necessarily provide accurate estimates unless weighted properly. For these examples, let&amp;rsquo;s suppose that this data set is a representative sample of a population we want to learn about: American adults.&lt;/p&gt;
&lt;h2 id=&#34;specify-variables&#34;&gt;Specify variables&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;specify()&lt;/code&gt; function can be used to specify which of the variables in the data set you&amp;rsquo;re interested in. If you&amp;rsquo;re only interested in, say, the &lt;code&gt;age&lt;/code&gt; of the respondents, you might write:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;gss &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;specify&lt;/span&gt;(response &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; age)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Response: age (numeric)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 2,988 x 1&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;      age&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;    &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  1    37&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  2    29&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  3    58&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  4    40&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  5    39&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  6    37&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  7    53&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  8    41&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  9    55&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 10    47&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # … with 2,978 more rows&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;On the front end, the output of &lt;code&gt;specify()&lt;/code&gt; just looks like it selects off the columns in the dataframe that you&amp;rsquo;ve specified. What do we see if we check the class of this object, though?&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;gss &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;specify&lt;/span&gt;(response &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; age) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;class&lt;/span&gt;()
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; [1] &amp;#34;infer&amp;#34;      &amp;#34;tbl_df&amp;#34;     &amp;#34;tbl&amp;#34;        &amp;#34;data.frame&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We can see that the infer class has been appended on top of the dataframe classes; this new class stores some extra metadata.&lt;/p&gt;
&lt;p&gt;If you&amp;rsquo;re interested in two variables (&lt;code&gt;age&lt;/code&gt; and &lt;code&gt;partyid&lt;/code&gt;, for example) you can &lt;code&gt;specify()&lt;/code&gt; their relationship in one of two (equivalent) ways:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# as a formula&lt;/span&gt;
gss &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;specify&lt;/span&gt;(age &lt;span style=&#34;color:#666&#34;&gt;~&lt;/span&gt; partyid)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Response: age (numeric)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Explanatory: partyid (factor)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 2,963 x 2&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;      age partyid&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;fct&amp;gt;  &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  1    37 dem    &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  2    29 dem    &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  3    58 ind    &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  4    40 rep    &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  5    39 dem    &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  6    37 dem    &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  7    53 dem    &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  8    41 rep    &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  9    55 ind    &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 10    47 rep    &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # … with 2,953 more rows&lt;/span&gt;

&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# with the named arguments&lt;/span&gt;
gss &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;specify&lt;/span&gt;(response &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; age, explanatory &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; partyid)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Response: age (numeric)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Explanatory: partyid (factor)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 2,963 x 2&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;      age partyid&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;fct&amp;gt;  &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  1    37 dem    &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  2    29 dem    &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  3    58 ind    &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  4    40 rep    &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  5    39 dem    &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  6    37 dem    &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  7    53 dem    &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  8    41 rep    &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  9    55 ind    &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 10    47 rep    &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # … with 2,953 more rows&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If you&amp;rsquo;re doing inference on one proportion or a difference in proportions, you will need to use the &lt;code&gt;success&lt;/code&gt; argument to specify which level of your &lt;code&gt;response&lt;/code&gt; variable is a success. For instance, if you&amp;rsquo;re interested in the proportion of the population with a college degree, you might use the following code:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# specifying for inference on proportions&lt;/span&gt;
gss &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;specify&lt;/span&gt;(response &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; college, success &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;degree&amp;#34;&lt;/span&gt;)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Response: college (factor)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 2,990 x 1&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;    college  &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;    &amp;lt;fct&amp;gt;    &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  1 no degree&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  2 no degree&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  3 degree   &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  4 degree   &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  5 no degree&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  6 no degree&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  7 no degree&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  8 no degree&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  9 no degree&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 10 no degree&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # … with 2,980 more rows&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;declare-the-hypothesis&#34;&gt;Declare the hypothesis&lt;/h2&gt;
&lt;p&gt;The next step in the infer pipeline is often to declare a null hypothesis using &lt;code&gt;hypothesize()&lt;/code&gt;. The first step is to supply one of &amp;ldquo;independence&amp;rdquo; or &amp;ldquo;point&amp;rdquo; to the &lt;code&gt;null&lt;/code&gt; argument. If your null hypothesis assumes independence between two variables, then this is all you need to supply to &lt;code&gt;hypothesize()&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;gss &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;specify&lt;/span&gt;(college &lt;span style=&#34;color:#666&#34;&gt;~&lt;/span&gt; partyid, success &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;degree&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;hypothesize&lt;/span&gt;(null &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;independence&amp;#34;&lt;/span&gt;)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Response: college (factor)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Explanatory: partyid (factor)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Null Hypothesis: independence&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 2,967 x 2&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;    college   partyid&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;    &amp;lt;fct&amp;gt;     &amp;lt;fct&amp;gt;  &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  1 no degree dem    &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  2 no degree dem    &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  3 degree    ind    &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  4 degree    rep    &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  5 no degree dem    &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  6 no degree dem    &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  7 no degree dem    &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  8 no degree rep    &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  9 no degree ind    &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 10 no degree rep    &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # … with 2,957 more rows&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If you&amp;rsquo;re doing inference on a point estimate, you will also need to provide one of &lt;code&gt;p&lt;/code&gt; (the true proportion of successes, between 0 and 1), &lt;code&gt;mu&lt;/code&gt; (the true mean), &lt;code&gt;med&lt;/code&gt; (the true median), or &lt;code&gt;sigma&lt;/code&gt; (the true standard deviation). For instance, if the null hypothesis is that the mean number of hours worked per week in our population is 40, we would write:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;gss &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;specify&lt;/span&gt;(response &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; hours) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;hypothesize&lt;/span&gt;(null &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;point&amp;#34;&lt;/span&gt;, mu &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;40&lt;/span&gt;)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Response: hours (numeric)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Null Hypothesis: point&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 1,756 x 1&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;    hours&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;    &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  1    50&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  2    60&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  3    84&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  4    40&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  5    50&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  6    60&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  7    40&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  8    20&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  9    40&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 10    40&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # … with 1,746 more rows&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Again, from the front-end, the dataframe outputted from &lt;code&gt;hypothesize()&lt;/code&gt; looks almost exactly the same as it did when it came out of &lt;code&gt;specify()&lt;/code&gt;, but infer now &amp;ldquo;knows&amp;rdquo; your null hypothesis.&lt;/p&gt;
&lt;h2 id=&#34;generate-the-distribution&#34;&gt;Generate the distribution&lt;/h2&gt;
&lt;p&gt;Once we&amp;rsquo;ve asserted our null hypothesis using &lt;code&gt;hypothesize()&lt;/code&gt;, we can construct a null distribution based on this hypothesis. We can do this using one of several methods, supplied in the &lt;code&gt;type&lt;/code&gt; argument:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;bootstrap&lt;/code&gt;: A bootstrap sample will be drawn for each replicate, where a sample of size equal to the input sample size is drawn (with replacement) from the input sample data.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;permute&lt;/code&gt;: For each replicate, each input value will be randomly reassigned (without replacement) to a new output value in the sample.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;simulate&lt;/code&gt;: A value will be sampled from a theoretical distribution with parameters specified in &lt;code&gt;hypothesize()&lt;/code&gt; for each replicate. (This option is currently only applicable for testing point estimates.)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Continuing on with our example above, about the average number of hours worked a week, we might write:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;gss &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;specify&lt;/span&gt;(response &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; hours) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;hypothesize&lt;/span&gt;(null &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;point&amp;#34;&lt;/span&gt;, mu &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;40&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;generate&lt;/span&gt;(reps &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;5000&lt;/span&gt;, type &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;bootstrap&amp;#34;&lt;/span&gt;)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Response: hours (numeric)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Null Hypothesis: point&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 8,780,000 x 2&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # Groups:   replicate [5,000]&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;    replicate hours&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;        &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  1         1  42.2&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  2         1  54.2&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  3         1  29.2&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  4         1  39.2&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  5         1  39.2&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  6         1  54.2&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  7         1  39.2&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  8         1  24.2&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  9         1  42.2&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 10         1  23.2&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # … with 8,779,990 more rows&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In the above example, we take 5000 bootstrap samples to form our null distribution.&lt;/p&gt;
&lt;p&gt;To generate a null distribution for the independence of two variables, we could also randomly reshuffle the pairings of explanatory and response variables to break any existing association. For instance, to generate 5000 replicates that can be used to create a null distribution under the assumption that political party affiliation is not affected by age:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;gss &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;specify&lt;/span&gt;(partyid &lt;span style=&#34;color:#666&#34;&gt;~&lt;/span&gt; age) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;hypothesize&lt;/span&gt;(null &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;independence&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;generate&lt;/span&gt;(reps &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;5000&lt;/span&gt;, type &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;permute&amp;#34;&lt;/span&gt;)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Response: partyid (factor)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Explanatory: age (numeric)&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Null Hypothesis: independence&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 14,815,000 x 3&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # Groups:   replicate [5,000]&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;    partyid   age replicate&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;    &amp;lt;fct&amp;gt;   &amp;lt;dbl&amp;gt;     &amp;lt;int&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  1 dem        37         1&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  2 dem        29         1&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  3 rep        58         1&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  4 rep        40         1&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  5 other      39         1&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  6 ind        37         1&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  7 ind        53         1&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  8 ind        41         1&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  9 dem        55         1&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 10 rep        47         1&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # … with 14,814,990 more rows&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;calculate-statistics&#34;&gt;Calculate statistics&lt;/h2&gt;
&lt;p&gt;Depending on whether you&amp;rsquo;re carrying out computation-based inference or theory-based inference, you will either supply &lt;code&gt;calculate()&lt;/code&gt; with the output of &lt;code&gt;generate()&lt;/code&gt; or &lt;code&gt;hypothesize()&lt;/code&gt;, respectively. The function, for one, takes in a &lt;code&gt;stat&lt;/code&gt; argument, which is currently one of &lt;code&gt;&amp;quot;mean&amp;quot;&lt;/code&gt;, &lt;code&gt;&amp;quot;median&amp;quot;&lt;/code&gt;, &lt;code&gt;&amp;quot;sum&amp;quot;&lt;/code&gt;, &lt;code&gt;&amp;quot;sd&amp;quot;&lt;/code&gt;, &lt;code&gt;&amp;quot;prop&amp;quot;&lt;/code&gt;, &lt;code&gt;&amp;quot;count&amp;quot;&lt;/code&gt;, &lt;code&gt;&amp;quot;diff in means&amp;quot;&lt;/code&gt;, &lt;code&gt;&amp;quot;diff in medians&amp;quot;&lt;/code&gt;, &lt;code&gt;&amp;quot;diff in props&amp;quot;&lt;/code&gt;, &lt;code&gt;&amp;quot;Chisq&amp;quot;&lt;/code&gt;, &lt;code&gt;&amp;quot;F&amp;quot;&lt;/code&gt;, &lt;code&gt;&amp;quot;t&amp;quot;&lt;/code&gt;, &lt;code&gt;&amp;quot;z&amp;quot;&lt;/code&gt;, &lt;code&gt;&amp;quot;slope&amp;quot;&lt;/code&gt;, or &lt;code&gt;&amp;quot;correlation&amp;quot;&lt;/code&gt;. For example, continuing our example above to calculate the null distribution of mean hours worked per week:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;gss &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;specify&lt;/span&gt;(response &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; hours) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;hypothesize&lt;/span&gt;(null &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;point&amp;#34;&lt;/span&gt;, mu &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;40&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;generate&lt;/span&gt;(reps &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;5000&lt;/span&gt;, type &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;bootstrap&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;calculate&lt;/span&gt;(stat &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;mean&amp;#34;&lt;/span&gt;)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 5,000 x 2&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;    replicate  stat&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;        &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  1         1  40.1&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  2         2  39.8&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  3         3  39.2&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  4         4  39.9&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  5         5  40.3&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  6         6  40.1&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  7         7  40.2&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  8         8  40.4&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  9         9  39.8&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 10        10  39.9&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # … with 4,990 more rows&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The output of &lt;code&gt;calculate()&lt;/code&gt; here shows us the sample statistic (in this case, the mean) for each of our 1000 replicates. If you&amp;rsquo;re carrying out inference on differences in means, medians, or proportions, or &lt;code&gt;\(t\)&lt;/code&gt; and &lt;code&gt;\(z\)&lt;/code&gt; statistics, you will need to supply an &lt;code&gt;order&lt;/code&gt; argument, giving the order in which the explanatory variables should be subtracted. For instance, to find the difference in mean age of those that have a college degree and those that don&amp;rsquo;t, we might write:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;gss &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;specify&lt;/span&gt;(age &lt;span style=&#34;color:#666&#34;&gt;~&lt;/span&gt; college) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;hypothesize&lt;/span&gt;(null &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;independence&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;generate&lt;/span&gt;(reps &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;5000&lt;/span&gt;, type &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;permute&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;calculate&lt;/span&gt;(&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;diff in means&amp;#34;&lt;/span&gt;, order &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;degree&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;no degree&amp;#34;&lt;/span&gt;))
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 5,000 x 2&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;    replicate    stat&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;        &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  1         1 -0.0914&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  2         2 -0.0354&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  3         3  0.112 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  4         4 -0.665 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  5         5 -1.32  &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  6         6 -1.01  &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  7         7 -1.41  &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  8         8 -0.0506&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  9         9  0.247 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 10        10 -0.214 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # … with 4,990 more rows&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;other-utilities&#34;&gt;Other utilities&lt;/h2&gt;
&lt;p&gt;The infer package also offers several utilities to extract meaning out of summary statistics and null distributions; the package provides functions to visualize where a statistic is relative to a distribution (with &lt;code&gt;visualize()&lt;/code&gt;), calculate p-values (with &lt;code&gt;get_p_value()&lt;/code&gt;), and calculate confidence intervals (with &lt;code&gt;get_confidence_interval()&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;To illustrate, we&amp;rsquo;ll go back to the example of determining whether the mean number of hours worked per week is 40 hours.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# find the point estimate&lt;/span&gt;
point_estimate &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; gss &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;specify&lt;/span&gt;(response &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; hours) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;calculate&lt;/span&gt;(stat &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;mean&amp;#34;&lt;/span&gt;)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Warning: Removed 1244 rows containing missing values.&lt;/span&gt;

&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# generate a null distribution&lt;/span&gt;
null_dist &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; gss &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;specify&lt;/span&gt;(response &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; hours) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;hypothesize&lt;/span&gt;(null &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;point&amp;#34;&lt;/span&gt;, mu &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;40&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;generate&lt;/span&gt;(reps &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;5000&lt;/span&gt;, type &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;bootstrap&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;calculate&lt;/span&gt;(stat &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;mean&amp;#34;&lt;/span&gt;)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Warning: Removed 1244 rows containing missing values.&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;(Notice the warning: &lt;code&gt;Removed 1244 rows containing missing values.&lt;/code&gt; This would be worth noting if you were actually carrying out this hypothesis test.)&lt;/p&gt;
&lt;p&gt;Our point estimate 40.772 seems &lt;em&gt;pretty&lt;/em&gt; close to 40, but a little bit different. We might wonder if this difference is just due to random chance, or if the mean number of hours worked per week in the population really isn&amp;rsquo;t 40.&lt;/p&gt;
&lt;p&gt;We could initially just visualize the null distribution.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;null_dist &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;visualize&lt;/span&gt;()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;figs/visualize-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Where does our sample&amp;rsquo;s observed statistic lie on this distribution? We can use the &lt;code&gt;obs_stat&lt;/code&gt; argument to specify this.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;null_dist &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;visualize&lt;/span&gt;() &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;shade_p_value&lt;/span&gt;(obs_stat &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; point_estimate, direction &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;two_sided&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;figs/visualize2-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Notice that infer has also shaded the regions of the null distribution that are as (or more) extreme than our observed statistic. (Also, note that we now use the &lt;code&gt;+&lt;/code&gt; operator to apply the &lt;code&gt;shade_p_value()&lt;/code&gt; function. This is because &lt;code&gt;visualize()&lt;/code&gt; outputs a plot object from ggplot2 instead of a dataframe, and the &lt;code&gt;+&lt;/code&gt; operator is needed to add the p-value layer to the plot object.) The red bar looks like it&amp;rsquo;s slightly far out on the right tail of the null distribution, so observing a sample mean of 40.772 hours would be somewhat unlikely if the mean was actually 40 hours. How unlikely, though?&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# get a two-tailed p-value&lt;/span&gt;
p_value &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; null_dist &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;get_p_value&lt;/span&gt;(obs_stat &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; point_estimate, direction &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;two_sided&amp;#34;&lt;/span&gt;)

p_value
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 1 x 1&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   p_value&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;     &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 1  0.0216&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;It looks like the p-value is 0.022, which is pretty small&amp;mdash;if the true mean number of hours worked per week was actually 40, the probability of our sample mean being this far (0.772 hours) from 40 would be 0.022. This may or may not be statistically significantly different, depending on the significance level &lt;code&gt;\(\alpha\)&lt;/code&gt; you decided on &lt;em&gt;before&lt;/em&gt; you ran this analysis. If you had set &lt;code&gt;\(\alpha = .05\)&lt;/code&gt;, then this difference would be statistically significant, but if you had set &lt;code&gt;\(\alpha = .01\)&lt;/code&gt;, then it would not be.&lt;/p&gt;
&lt;p&gt;To get a confidence interval around our estimate, we can write:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# start with the null distribution&lt;/span&gt;
null_dist &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# calculate the confidence interval around the point estimate&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;get_confidence_interval&lt;/span&gt;(point_estimate &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; point_estimate,
                          &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# at the 95% confidence level&lt;/span&gt;
                          level &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;.95&lt;/span&gt;,
                          &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# using the standard error&lt;/span&gt;
                          type &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;se&amp;#34;&lt;/span&gt;)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 1 x 2&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   lower upper&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 1  40.1  41.4&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;As you can see, 40 hours per week is not contained in this interval, which aligns with our previous conclusion that this finding is significant at the confidence level &lt;code&gt;\(\alpha = .05\)&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;theoretical-methods&#34;&gt;Theoretical methods&lt;/h2&gt;
&lt;p&gt;The infer package also provides functionality to use theoretical methods for &lt;code&gt;&amp;quot;Chisq&amp;quot;&lt;/code&gt;, &lt;code&gt;&amp;quot;F&amp;quot;&lt;/code&gt; and &lt;code&gt;&amp;quot;t&amp;quot;&lt;/code&gt; test statistics.&lt;/p&gt;
&lt;p&gt;Generally, to find a null distribution using theory-based methods, use the same code that you would use to find the null distribution using randomization-based methods, but skip the &lt;code&gt;generate()&lt;/code&gt; step. For example, if we wanted to find a null distribution for the relationship between age (&lt;code&gt;age&lt;/code&gt;) and party identification (&lt;code&gt;partyid&lt;/code&gt;) using randomization, we could write:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;null_f_distn &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; gss &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
   &lt;span style=&#34;color:#00f&#34;&gt;specify&lt;/span&gt;(age &lt;span style=&#34;color:#666&#34;&gt;~&lt;/span&gt; partyid) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
   &lt;span style=&#34;color:#00f&#34;&gt;hypothesize&lt;/span&gt;(null &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;independence&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
   &lt;span style=&#34;color:#00f&#34;&gt;generate&lt;/span&gt;(reps &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;5000&lt;/span&gt;, type &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;permute&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
   &lt;span style=&#34;color:#00f&#34;&gt;calculate&lt;/span&gt;(stat &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;F&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;To find the null distribution using theory-based methods, instead, skip the &lt;code&gt;generate()&lt;/code&gt; step entirely:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;null_f_distn_theoretical &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; gss &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
   &lt;span style=&#34;color:#00f&#34;&gt;specify&lt;/span&gt;(age &lt;span style=&#34;color:#666&#34;&gt;~&lt;/span&gt; partyid) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
   &lt;span style=&#34;color:#00f&#34;&gt;hypothesize&lt;/span&gt;(null &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;independence&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
   &lt;span style=&#34;color:#00f&#34;&gt;calculate&lt;/span&gt;(stat &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;F&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We&amp;rsquo;ll calculate the observed statistic to make use of in the following visualizations; this procedure is the same, regardless of the methods used to find the null distribution.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;F_hat &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; gss &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;specify&lt;/span&gt;(age &lt;span style=&#34;color:#666&#34;&gt;~&lt;/span&gt; partyid) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;calculate&lt;/span&gt;(stat &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;F&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now, instead of just piping the null distribution into &lt;code&gt;visualize()&lt;/code&gt;, as we would do if we wanted to visualize the randomization-based null distribution, we also need to provide &lt;code&gt;method = &amp;quot;theoretical&amp;quot;&lt;/code&gt; to &lt;code&gt;visualize()&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;visualize&lt;/span&gt;(null_f_distn_theoretical, method &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;theoretical&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;shade_p_value&lt;/span&gt;(obs_stat &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; F_hat, direction &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;greater&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;figs/unnamed-chunk-4-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To get a sense of how the theory-based and randomization-based null distributions relate, we can pipe the randomization-based null distribution into &lt;code&gt;visualize()&lt;/code&gt; and also specify &lt;code&gt;method = &amp;quot;both&amp;quot;&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;visualize&lt;/span&gt;(null_f_distn, method &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;both&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;shade_p_value&lt;/span&gt;(obs_stat &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; F_hat, direction &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;greater&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;figs/unnamed-chunk-5-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s it! This vignette covers most all of the key functionality of infer. See &lt;code&gt;help(package = &amp;quot;infer&amp;quot;)&lt;/code&gt; for a full list of functions and vignettes.&lt;/p&gt;
&lt;h2 id=&#34;session-information&#34;&gt;Session information&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;#&amp;gt; ─ Session info ───────────────────────────────────────────────────────────────
#&amp;gt;  setting  value                       
#&amp;gt;  version  R version 3.6.2 (2019-12-12)
#&amp;gt;  os       macOS Mojave 10.14.6        
#&amp;gt;  system   x86_64, darwin15.6.0        
#&amp;gt;  ui       X11                         
#&amp;gt;  language (EN)                        
#&amp;gt;  collate  en_US.UTF-8                 
#&amp;gt;  ctype    en_US.UTF-8                 
#&amp;gt;  tz       America/Denver              
#&amp;gt;  date     2020-04-17                  
#&amp;gt; 
#&amp;gt; ─ Packages ───────────────────────────────────────────────────────────────────
#&amp;gt;  package    * version date       lib source        
#&amp;gt;  broom      * 0.5.5   2020-02-29 [1] CRAN (R 3.6.0)
#&amp;gt;  dials      * 0.0.6   2020-04-03 [1] CRAN (R 3.6.2)
#&amp;gt;  dplyr      * 0.8.5   2020-03-07 [1] CRAN (R 3.6.0)
#&amp;gt;  ggplot2    * 3.3.0   2020-03-05 [1] CRAN (R 3.6.0)
#&amp;gt;  infer      * 0.5.1   2019-11-19 [1] CRAN (R 3.6.0)
#&amp;gt;  parsnip    * 0.1.0   2020-04-09 [1] CRAN (R 3.6.2)
#&amp;gt;  purrr      * 0.3.3   2019-10-18 [1] CRAN (R 3.6.0)
#&amp;gt;  recipes    * 0.1.10  2020-03-18 [1] CRAN (R 3.6.0)
#&amp;gt;  rlang        0.4.5   2020-03-01 [1] CRAN (R 3.6.0)
#&amp;gt;  rsample    * 0.0.6   2020-03-31 [1] CRAN (R 3.6.2)
#&amp;gt;  tibble     * 2.1.3   2019-06-06 [1] CRAN (R 3.6.2)
#&amp;gt;  tidymodels * 0.1.0   2020-02-16 [1] CRAN (R 3.6.0)
#&amp;gt;  tune       * 0.1.0   2020-04-02 [1] CRAN (R 3.6.2)
#&amp;gt;  workflows  * 0.1.1   2020-03-17 [1] CRAN (R 3.6.0)
#&amp;gt;  yardstick  * 0.0.6   2020-03-17 [1] CRAN (R 3.6.0)
#&amp;gt; 
#&amp;gt; [1] /Library/Frameworks/R.framework/Versions/3.6/Resources/library
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Multivariate analysis using partial least squares</title>
      <link>https://nutriverse.io/learn/models/pls/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://nutriverse.io/learn/models/pls/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;To use the code in this article, you will need to install the following packages: modeldata, pls, tidymodels, and tidyr.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Multivariate analysis&amp;rdquo; usually refers to multiple &lt;em&gt;outcomes&lt;/em&gt; being modeled, analyzed, and/or predicted. There are multivariate versions of many common statistical tools. For example, suppose there was a data set with columns &lt;code&gt;y1&lt;/code&gt; and &lt;code&gt;y2&lt;/code&gt; representing two outcomes to be predicted. The &lt;code&gt;lm()&lt;/code&gt; function would look something like:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;lm&lt;/span&gt;(&lt;span style=&#34;color:#00f&#34;&gt;cbind&lt;/span&gt;(y1, y2) &lt;span style=&#34;color:#666&#34;&gt;~&lt;/span&gt; ., data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; dat)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This &lt;code&gt;cbind()&lt;/code&gt; call is pretty awkward and is a consequence of how the traditional formula infrastructure works. The recipes package is a lot easier to work with! This article demonstrates how to model multiple outcomes.&lt;/p&gt;
&lt;p&gt;The data that we&amp;rsquo;ll use has three outcomes. From &lt;code&gt;?modeldata::meats&lt;/code&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;These data are recorded on a Tecator Infratec Food and Feed Analyzer working in the wavelength range 850 - 1050 nm by the Near Infrared Transmission (NIT) principle. Each sample contains finely chopped pure meat with different moisture, fat and protein contents.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;For each meat sample the data consists of a 100 channel spectrum of absorbances and the contents of moisture (water), fat and protein. The absorbance is &lt;code&gt;-log10&lt;/code&gt; of the transmittance measured by the spectrometer. The three contents, measured in percent, are determined by analytic chemistry.&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The goal is to predict the proportion of the three substances using the chemistry test. There can often be a high degree of between-variable correlations in predictors, and that is certainly the case here.&lt;/p&gt;
&lt;p&gt;To start, let&amp;rsquo;s take the two data matrices (called &lt;code&gt;endpoints&lt;/code&gt; and &lt;code&gt;absorp&lt;/code&gt;) and bind them together in a data frame:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;library&lt;/span&gt;(modeldata)
&lt;span style=&#34;color:#00f&#34;&gt;data&lt;/span&gt;(meats)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The three &lt;em&gt;outcomes&lt;/em&gt; have fairly high correlations also.&lt;/p&gt;
&lt;h2 id=&#34;preprocessing-the-data&#34;&gt;Preprocessing the data&lt;/h2&gt;
&lt;p&gt;If the outcomes can be predicted using a linear model, partial least squares (PLS) is an ideal method. PLS models the data as a function of a set of unobserved &lt;em&gt;latent&lt;/em&gt; variables that are derived in a manner similar to principal component analysis (PCA).&lt;/p&gt;
&lt;p&gt;PLS, unlike PCA, also incorporates the outcome data when creating the PLS components. Like PCA, it tries to maximize the variance of the predictors that are explained by the components but it also tries to simultaneously maximize the correlation between those components and the outcomes. In this way, PLS &lt;em&gt;chases&lt;/em&gt; variation of the predictors and outcomes.&lt;/p&gt;
&lt;p&gt;Since we are working with variances and covariances, we need to standardize the data. The recipe will center and scale all of the variables.&lt;/p&gt;
&lt;p&gt;Many base R functions that deal with multivariate outcomes using a formula require the use of &lt;code&gt;cbind()&lt;/code&gt; on the left-hand side of the formula to work with the traditional formula methods. In tidymodels, recipes do not; the outcomes can be symbolically &amp;ldquo;added&amp;rdquo; together on the left-hand side:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;norm_rec &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;recipe&lt;/span&gt;(water &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; fat &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; protein &lt;span style=&#34;color:#666&#34;&gt;~&lt;/span&gt; ., data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; meats) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;step_normalize&lt;/span&gt;(&lt;span style=&#34;color:#00f&#34;&gt;everything&lt;/span&gt;()) 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Before we can finalize the PLS model, the number of PLS components to retain must be determined. This can be done using performance metrics such as the root mean squared error. However, we can also calculate the proportion of variance explained by the components for the &lt;em&gt;predictors and each of the outcomes&lt;/em&gt;. This allows an informed choice to be made based on the level of evidence that the situation requires.&lt;/p&gt;
&lt;p&gt;Since the data set isn&amp;rsquo;t large, let&amp;rsquo;s use resampling to measure these proportions. With ten repeats of 10-fold cross-validation, we build the PLS model on 90% of the data and evaluate on the heldout 10%. For each of the 100 models, we extract and save the proportions.&lt;/p&gt;
&lt;p&gt;The folds can be created using the 
&lt;a href=&#34;https://tidymodels.github.io/rsample/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;rsample&lt;/a&gt; package and the recipe can be estimated for each resample using the 
&lt;a href=&#34;https://tidymodels.github.io/rsample/reference/prepper.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;prepper()&lt;/code&gt;&lt;/a&gt; function:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;set.seed&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;57343&lt;/span&gt;)
folds &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;vfold_cv&lt;/span&gt;(meats, repeats &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;10&lt;/span&gt;)

folds &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  folds &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;mutate&lt;/span&gt;(recipes &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;map&lt;/span&gt;(splits, prepper, recipe &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; norm_rec))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;partial-least-squares&#34;&gt;Partial least squares&lt;/h2&gt;
&lt;p&gt;The complicated parts for moving forward are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Formatting the predictors and outcomes into the format that the pls package requires, and&lt;/li&gt;
&lt;li&gt;Estimating the proportions.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For the first part, the standardized outcomes and predictors need to be formatted into two separate matrices. Since we used &lt;code&gt;retain = TRUE&lt;/code&gt; when prepping the recipes, we can use the &lt;code&gt;juice()&lt;/code&gt; function. To save the data as a matrix, the option &lt;code&gt;composition = &amp;quot;matrix&amp;quot;&lt;/code&gt; will avoid saving the data as tibbles and use the required format.&lt;/p&gt;
&lt;p&gt;The pls package expects a simple formula to specify the model, but each side of the formula should &lt;em&gt;represent a matrix&lt;/em&gt;. In other words, we need a data set with two columns where each column is a matrix. The secret to doing this is to &amp;ldquo;protect&amp;rdquo; the two matrices using &lt;code&gt;I()&lt;/code&gt; when adding them to the data frame.&lt;/p&gt;
&lt;p&gt;The calculation for the proportion of variance explained is straightforward for the predictors; the function &lt;code&gt;pls::explvar()&lt;/code&gt; will compute that. For the outcomes, the process is more complicated. A ready-made function to compute these is not obvious but there is some code inside of the summary function to do the computation (see below).&lt;/p&gt;
&lt;p&gt;The function &lt;code&gt;get_var_explained()&lt;/code&gt; shown here will do all these computations and return a data frame with columns &lt;code&gt;components&lt;/code&gt;, &lt;code&gt;source&lt;/code&gt; (for the predictors, water, etc), and the &lt;code&gt;proportion&lt;/code&gt; of variance that is explained by the components.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;library&lt;/span&gt;(pls)
&lt;span style=&#34;color:#00f&#34;&gt;library&lt;/span&gt;(tidyr)

get_var_explained &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;function&lt;/span&gt;(recipe, &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;...&lt;/span&gt;) {
  
  &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# Extract the predictors and outcomes into their own matrices&lt;/span&gt;
  y_mat &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;juice&lt;/span&gt;(recipe, composition &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;matrix&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#00f&#34;&gt;all_outcomes&lt;/span&gt;())
  x_mat &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;juice&lt;/span&gt;(recipe, composition &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;matrix&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#00f&#34;&gt;all_predictors&lt;/span&gt;())
  
  &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# The pls package prefers the data in a data frame where the outcome&lt;/span&gt;
  &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# and predictors are in _matrices_. To make sure this is formatted&lt;/span&gt;
  &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# properly, use the `I()` function to inhibit `data.frame()` from making&lt;/span&gt;
  &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# all the individual columns. `pls_format` should have two columns.&lt;/span&gt;
  pls_format &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;data.frame&lt;/span&gt;(
    endpoints &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;I&lt;/span&gt;(y_mat),
    measurements &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;I&lt;/span&gt;(x_mat)
  )
  &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# Fit the model&lt;/span&gt;
  mod &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;plsr&lt;/span&gt;(endpoints &lt;span style=&#34;color:#666&#34;&gt;~&lt;/span&gt; measurements, data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; pls_format)
  
  &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# Get the proportion of the predictor variance that is explained&lt;/span&gt;
  &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# by the model for different number of components. &lt;/span&gt;
  xve &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;explvar&lt;/span&gt;(mod)&lt;span style=&#34;color:#666&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;100&lt;/span&gt; 

  &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# To do the same for the outcome, it is more complex. This code &lt;/span&gt;
  &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# was extracted from pls:::summary.mvr. &lt;/span&gt;
  explained &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
    &lt;span style=&#34;color:#00f&#34;&gt;drop&lt;/span&gt;(pls&lt;span style=&#34;color:#666&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#00f&#34;&gt;R2&lt;/span&gt;(mod, estimate &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;train&amp;#34;&lt;/span&gt;, intercept &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;FALSE&lt;/span&gt;)&lt;span style=&#34;color:#666&#34;&gt;$&lt;/span&gt;val) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
    &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# transpose so that components are in rows&lt;/span&gt;
    &lt;span style=&#34;color:#00f&#34;&gt;t&lt;/span&gt;() &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
    &lt;span style=&#34;color:#00f&#34;&gt;as_tibble&lt;/span&gt;() &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
    &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# Add the predictor proportions&lt;/span&gt;
    &lt;span style=&#34;color:#00f&#34;&gt;mutate&lt;/span&gt;(predictors &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;cumsum&lt;/span&gt;(xve) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;as.vector&lt;/span&gt;(),
           components &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;seq_along&lt;/span&gt;(xve)) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
    &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# Put into a tidy format that is tall&lt;/span&gt;
    &lt;span style=&#34;color:#00f&#34;&gt;pivot_longer&lt;/span&gt;(
      cols &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;-&lt;/span&gt;components),
      names_to &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;source&amp;#34;&lt;/span&gt;,
      values_to &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;proportion&amp;#34;&lt;/span&gt;
    )
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We compute this data frame for each resample and save the results in the different columns.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;folds &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  folds &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;mutate&lt;/span&gt;(var &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;map&lt;/span&gt;(recipes, get_var_explained),
         var &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;unname&lt;/span&gt;(var))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;To extract and aggregate these data, simple row binding can be used to stack the data vertically. Most of the action happens in the first 15 components so let&amp;rsquo;s filter the data and compute the &lt;em&gt;average&lt;/em&gt; proportion.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;variance_data &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;bind_rows&lt;/span&gt;(folds[[&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;var&amp;#34;&lt;/span&gt;]]) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;filter&lt;/span&gt;(components &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;15&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;group_by&lt;/span&gt;(components, source) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;summarize&lt;/span&gt;(proportion &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;mean&lt;/span&gt;(proportion))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The plot below shows that, if the protein measurement is important, you might require 10 or so components to achieve a good representation of that outcome. Note that the predictor variance is captured extremely well using a single component. This is due to the high degree of correlation in those data.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;ggplot&lt;/span&gt;(variance_data, &lt;span style=&#34;color:#00f&#34;&gt;aes&lt;/span&gt;(x &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; components, y &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; proportion, col &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; source)) &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;geom_line&lt;/span&gt;() &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;geom_point&lt;/span&gt;() 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;figs/plot-1.svg&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;session-information&#34;&gt;Session information&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;#&amp;gt; ─ Session info ───────────────────────────────────────────────────────────────
#&amp;gt;  setting  value                       
#&amp;gt;  version  R version 3.6.2 (2019-12-12)
#&amp;gt;  os       macOS Mojave 10.14.6        
#&amp;gt;  system   x86_64, darwin15.6.0        
#&amp;gt;  ui       X11                         
#&amp;gt;  language (EN)                        
#&amp;gt;  collate  en_US.UTF-8                 
#&amp;gt;  ctype    en_US.UTF-8                 
#&amp;gt;  tz       America/Denver              
#&amp;gt;  date     2020-04-17                  
#&amp;gt; 
#&amp;gt; ─ Packages ───────────────────────────────────────────────────────────────────
#&amp;gt;  package    * version date       lib source        
#&amp;gt;  broom      * 0.5.5   2020-02-29 [1] CRAN (R 3.6.0)
#&amp;gt;  dials      * 0.0.6   2020-04-03 [1] CRAN (R 3.6.2)
#&amp;gt;  dplyr      * 0.8.5   2020-03-07 [1] CRAN (R 3.6.0)
#&amp;gt;  ggplot2    * 3.3.0   2020-03-05 [1] CRAN (R 3.6.0)
#&amp;gt;  infer      * 0.5.1   2019-11-19 [1] CRAN (R 3.6.0)
#&amp;gt;  modeldata  * 0.0.1   2019-12-06 [1] CRAN (R 3.6.0)
#&amp;gt;  parsnip    * 0.1.0   2020-04-09 [1] CRAN (R 3.6.2)
#&amp;gt;  pls        * 2.7-2   2019-10-01 [1] CRAN (R 3.6.0)
#&amp;gt;  purrr      * 0.3.3   2019-10-18 [1] CRAN (R 3.6.0)
#&amp;gt;  recipes    * 0.1.10  2020-03-18 [1] CRAN (R 3.6.0)
#&amp;gt;  rlang        0.4.5   2020-03-01 [1] CRAN (R 3.6.0)
#&amp;gt;  rsample    * 0.0.6   2020-03-31 [1] CRAN (R 3.6.2)
#&amp;gt;  tibble     * 2.1.3   2019-06-06 [1] CRAN (R 3.6.2)
#&amp;gt;  tidymodels * 0.1.0   2020-02-16 [1] CRAN (R 3.6.0)
#&amp;gt;  tidyr      * 1.0.2   2020-01-24 [1] CRAN (R 3.6.0)
#&amp;gt;  tune       * 0.1.0   2020-04-02 [1] CRAN (R 3.6.2)
#&amp;gt;  workflows  * 0.1.1   2020-03-17 [1] CRAN (R 3.6.0)
#&amp;gt;  yardstick  * 0.0.6   2020-03-17 [1] CRAN (R 3.6.0)
#&amp;gt; 
#&amp;gt; [1] /Library/Frameworks/R.framework/Versions/3.6/Resources/library
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Statistical analysis of contingency tables</title>
      <link>https://nutriverse.io/learn/statistics/xtabs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://nutriverse.io/learn/statistics/xtabs/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This article only requires that you have the tidymodels package installed.&lt;/p&gt;
&lt;p&gt;In this vignette, we&amp;rsquo;ll walk through conducting a &lt;code&gt;\(\chi^2\)&lt;/code&gt; (chi-squared) test of independence and a chi-squared goodness of fit test using infer. We&amp;rsquo;ll start out with a chi-squared test of independence, which can be used to test the association between two categorical variables. Then, we&amp;rsquo;ll move on to a chi-squared goodness of fit test, which tests how well the distribution of one categorical variable can be approximated by some theoretical distribution.&lt;/p&gt;
&lt;p&gt;Throughout this vignette, we&amp;rsquo;ll make use of the &lt;code&gt;ad_data&lt;/code&gt; data set (available in the modeldata package, which is part of tidymodels). This data set is related to cognitive impairment in 333 patients from 
&lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3079734/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Craig-Schapiro &lt;em&gt;et al&lt;/em&gt; (2011)&lt;/a&gt;. See &lt;code&gt;?ad_data&lt;/code&gt; for more information on the variables included and their source. One of the main research questions in these data were how a person&amp;rsquo;s genetics related to the Apolipoprotein E gene affect their cognitive skills. The data shows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;library&lt;/span&gt;(tidymodels) &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# Includes the infer package&lt;/span&gt;

&lt;span style=&#34;color:#00f&#34;&gt;data&lt;/span&gt;(ad_data, package &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;modeldata&amp;#34;&lt;/span&gt;)
ad_data &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;select&lt;/span&gt;(Genotype, Class)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 333 x 2&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;    Genotype Class   &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;    &amp;lt;fct&amp;gt;    &amp;lt;fct&amp;gt;   &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  1 E3E3     Control &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  2 E3E4     Control &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  3 E3E4     Control &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  4 E3E4     Control &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  5 E3E3     Control &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  6 E4E4     Impaired&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  7 E2E3     Control &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  8 E2E3     Control &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  9 E3E3     Control &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 10 E2E3     Impaired&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # … with 323 more rows&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The three main genetic variants are called E2, E3, and E4. The values in &lt;code&gt;Genotype&lt;/code&gt; represent the genetic makeup of patients based on what they inherited from their parents (i.e, a value of &amp;ldquo;E2E4&amp;rdquo; means E2 from one parent and E4 from the other).&lt;/p&gt;
&lt;h2 id=&#34;test-of-independence&#34;&gt;Test of independence&lt;/h2&gt;
&lt;p&gt;To carry out a chi-squared test of independence, we&amp;rsquo;ll examine the association between their cognitive ability (impaired and healthy) and the genetic makeup. This is what the relationship looks like in the sample data:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;figs/plot-indep-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If there were no relationship, we would expect to see the purple bars reaching to the same length, regardless of cognitive ability. Are the differences we see here, though, just due to random noise?&lt;/p&gt;
&lt;p&gt;First, to calculate the observed statistic, we can use &lt;code&gt;specify()&lt;/code&gt; and &lt;code&gt;calculate()&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# calculate the observed statistic&lt;/span&gt;
observed_indep_statistic &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; ad_data &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;specify&lt;/span&gt;(Genotype &lt;span style=&#34;color:#666&#34;&gt;~&lt;/span&gt; Class) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;calculate&lt;/span&gt;(stat &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;Chisq&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The observed &lt;code&gt;\(\chi^2\)&lt;/code&gt; statistic is 21.577. Now, we want to compare this statistic to a null distribution, generated under the assumption that these variables are not actually related, to get a sense of how likely it would be for us to see this observed statistic if there were actually no association between cognitive ability and genetics.&lt;/p&gt;
&lt;p&gt;We can &lt;code&gt;generate()&lt;/code&gt; the null distribution in one of two ways: using randomization or theory-based methods. The randomization approach permutes the response and explanatory variables, so that each person&amp;rsquo;s genetics is matched up with a random cognitive rating from the sample in order to break up any association between the two.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# generate the null distribution using randomization&lt;/span&gt;
null_distribution_simulated &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; ad_data &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;specify&lt;/span&gt;(Genotype &lt;span style=&#34;color:#666&#34;&gt;~&lt;/span&gt; Class) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;hypothesize&lt;/span&gt;(null &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;independence&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;generate&lt;/span&gt;(reps &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;5000&lt;/span&gt;, type &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;permute&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;calculate&lt;/span&gt;(stat &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;Chisq&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Note that, in the line &lt;code&gt;specify(Genotype ~ Class)&lt;/code&gt; above, we could use the equivalent syntax &lt;code&gt;specify(response = Genotype, explanatory = Class)&lt;/code&gt;. The same goes in the code below, which generates the null distribution using theory-based methods instead of randomization.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# generate the null distribution by theoretical approximation&lt;/span&gt;
null_distribution_theoretical &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; ad_data &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;specify&lt;/span&gt;(Genotype &lt;span style=&#34;color:#666&#34;&gt;~&lt;/span&gt; Class) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;hypothesize&lt;/span&gt;(null &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;independence&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# note that we skip the generation step here!&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;calculate&lt;/span&gt;(stat &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;Chisq&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;To get a sense for what these distributions look like, and where our observed statistic falls, we can use &lt;code&gt;visualize()&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# visualize the null distribution and test statistic!&lt;/span&gt;
null_distribution_simulated &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;visualize&lt;/span&gt;() &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;shade_p_value&lt;/span&gt;(observed_indep_statistic,
                direction &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;greater&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;figs/visualize-indep-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We could also visualize the observed statistic against the theoretical null distribution. Note that we skip the &lt;code&gt;generate()&lt;/code&gt; and &lt;code&gt;calculate()&lt;/code&gt; steps when using the theoretical approach, and that we now need to provide &lt;code&gt;method = &amp;quot;theoretical&amp;quot;&lt;/code&gt; to &lt;code&gt;visualize()&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# visualize the theoretical null distribution and test statistic!&lt;/span&gt;
ad_data &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;specify&lt;/span&gt;(Genotype &lt;span style=&#34;color:#666&#34;&gt;~&lt;/span&gt; Class) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;hypothesize&lt;/span&gt;(null &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;independence&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;visualize&lt;/span&gt;(method &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;theoretical&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;shade_p_value&lt;/span&gt;(observed_indep_statistic,
                direction &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;greater&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;figs/visualize-indep-theor-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To visualize both the randomization-based and theoretical null distributions to get a sense of how the two relate, we can pipe the randomization-based null distribution into &lt;code&gt;visualize()&lt;/code&gt;, and further provide &lt;code&gt;method = &amp;quot;both&amp;quot;&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# visualize both null distributions and the test statistic!&lt;/span&gt;
null_distribution_simulated &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;visualize&lt;/span&gt;(method &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;both&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;shade_p_value&lt;/span&gt;(observed_indep_statistic,
                direction &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;greater&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;figs/visualize-indep-both-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Either way, it looks like our observed test statistic would be fairly unlikely if there were actually no association between cognition and genotype. More exactly, we can calculate the p-value:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# calculate the p value from the observed statistic and null distribution&lt;/span&gt;
p_value_independence &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; null_distribution_simulated &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;get_p_value&lt;/span&gt;(obs_stat &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; observed_indep_statistic,
              direction &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;greater&amp;#34;&lt;/span&gt;)

p_value_independence
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 1 x 1&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;    p_value&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;      &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 1 0.000600&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Thus, if there were really no relationship between cognition and genotype, the probability that we would see a statistic as or more extreme than 21.577 is approximately 6\times 10^{-4}.&lt;/p&gt;
&lt;p&gt;Note that, equivalently to the steps shown above, the package supplies a wrapper function, &lt;code&gt;chisq_test&lt;/code&gt;, to carry out Chi-Squared tests of independence on tidy data. The syntax goes like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;chisq_test&lt;/span&gt;(ad_data, Genotype &lt;span style=&#34;color:#666&#34;&gt;~&lt;/span&gt; Class)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 1 x 3&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   statistic chisq_df  p_value&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;       &amp;lt;dbl&amp;gt;    &amp;lt;int&amp;gt;    &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 1      21.6        5 0.000630&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;goodness-of-fit&#34;&gt;Goodness of fit&lt;/h2&gt;
&lt;p&gt;Now, moving on to a chi-squared goodness of fit test, we&amp;rsquo;ll take a look at just the genotype data. Many papers have investigated the relationship of Apolipoprotein E to diseases. For example, 
&lt;a href=&#34;https://annals.org/aim/article-abstract/717641/meta-analysis-apolipoprotein-e-genotypes-risk-coronary-heart-disease&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Song &lt;em&gt;et al&lt;/em&gt; (2004)&lt;/a&gt; conducted a meta-analysis of numerous studies that looked at this gene and heart disease. In their paper, they describe the frequency of the different genotypes across many samples. For the cognition study, it might be interesting to see if our sample of genotypes was consistent with this literature (treating the rates, for this analysis, as known).&lt;/p&gt;
&lt;p&gt;The rates of the meta-analysis and our observed data are:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# Song, Y., Stampfer, M. J., &amp;amp; Liu, S. (2004). Meta-Analysis: Apolipoprotein E &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# Genotypes and Risk for Coronary Heart Disease. Annals of Internal Medicine, &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# 141(2), 137.&lt;/span&gt;
meta_rates &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;E2E2&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;0.71&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;E2E3&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;11.4&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;E2E4&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;2.32&lt;/span&gt;,
                &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;E3E3&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;61.0&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;E3E4&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;22.6&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;E4E4&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;2.22&lt;/span&gt;)
meta_rates &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; meta_rates&lt;span style=&#34;color:#666&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#00f&#34;&gt;sum&lt;/span&gt;(meta_rates) &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# these add up to slightly &amp;gt; 100%&lt;/span&gt;

obs_rates &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;table&lt;/span&gt;(ad_data&lt;span style=&#34;color:#666&#34;&gt;$&lt;/span&gt;Genotype)&lt;span style=&#34;color:#666&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#00f&#34;&gt;nrow&lt;/span&gt;(ad_data)
&lt;span style=&#34;color:#00f&#34;&gt;round&lt;/span&gt;(&lt;span style=&#34;color:#00f&#34;&gt;cbind&lt;/span&gt;(obs_rates, meta_rates) &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;100&lt;/span&gt;, &lt;span style=&#34;color:#666&#34;&gt;2&lt;/span&gt;)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;      obs_rates meta_rates&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; E2E2       0.6       0.71&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; E2E3      11.1      11.37&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; E2E4       2.4       2.31&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; E3E3      50.1      60.85&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; E3E4      31.8      22.54&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; E4E4       3.9       2.21&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Suppose our null hypothesis is that &lt;code&gt;Genotype&lt;/code&gt; follows the same frequency distribution as the meta-analysis. Lets now test whether this difference in distributions is statistically significant.&lt;/p&gt;
&lt;p&gt;First, to carry out this hypothesis test, we would calculate our observed statistic.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# calculating the null distribution&lt;/span&gt;
observed_gof_statistic &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; ad_data &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;specify&lt;/span&gt;(response &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; Genotype) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;hypothesize&lt;/span&gt;(null &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;point&amp;#34;&lt;/span&gt;, p &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; meta_rates) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;calculate&lt;/span&gt;(stat &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;Chisq&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The observed statistic is 23.384. Now, generating a null distribution, by just dropping in a call to &lt;code&gt;generate()&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# generating a null distribution&lt;/span&gt;
null_distribution_gof &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; ad_data &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;specify&lt;/span&gt;(response &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; Genotype) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;hypothesize&lt;/span&gt;(null &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;point&amp;#34;&lt;/span&gt;, p &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; meta_rates) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;generate&lt;/span&gt;(reps &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;5000&lt;/span&gt;, type &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;simulate&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;calculate&lt;/span&gt;(stat &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;Chisq&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Again, to get a sense for what these distributions look like, and where our observed statistic falls, we can use &lt;code&gt;visualize()&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# visualize the null distribution and test statistic!&lt;/span&gt;
null_distribution_gof &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;visualize&lt;/span&gt;() &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;shade_p_value&lt;/span&gt;(observed_gof_statistic,
                direction &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;greater&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;figs/visualize-indep-gof-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This statistic seems like it would be unlikely if our rates were the same as the rates from the meta-analysis! How unlikely, though? Calculating the p-value:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# calculate the p-value&lt;/span&gt;
p_value_gof &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; null_distribution_gof &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;get_p_value&lt;/span&gt;(observed_gof_statistic,
              direction &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;greater&amp;#34;&lt;/span&gt;)

p_value_gof
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 1 x 1&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   p_value&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;     &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 1  0.0008&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Thus, if each genotype occurred at the same rate as the Song paper, the probability that we would see a distribution like the one we did is approximately 8\times 10^{-4}.&lt;/p&gt;
&lt;p&gt;Again, equivalently to the steps shown above, the package supplies a wrapper function, &lt;code&gt;chisq_test&lt;/code&gt;, to carry out chi-squared goodness of fit tests on tidy data. The syntax goes like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;chisq_test&lt;/span&gt;(ad_data, response &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; Genotype, p &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; meta_rates)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 1 x 3&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   statistic chisq_df  p_value&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;       &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 1      23.4        5 0.000285&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;session-information&#34;&gt;Session information&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;#&amp;gt; ─ Session info ───────────────────────────────────────────────────────────────
#&amp;gt;  setting  value                       
#&amp;gt;  version  R version 3.6.2 (2019-12-12)
#&amp;gt;  os       macOS Mojave 10.14.6        
#&amp;gt;  system   x86_64, darwin15.6.0        
#&amp;gt;  ui       X11                         
#&amp;gt;  language (EN)                        
#&amp;gt;  collate  en_US.UTF-8                 
#&amp;gt;  ctype    en_US.UTF-8                 
#&amp;gt;  tz       America/Denver              
#&amp;gt;  date     2020-04-17                  
#&amp;gt; 
#&amp;gt; ─ Packages ───────────────────────────────────────────────────────────────────
#&amp;gt;  package    * version date       lib source        
#&amp;gt;  broom      * 0.5.5   2020-02-29 [1] CRAN (R 3.6.0)
#&amp;gt;  dials      * 0.0.6   2020-04-03 [1] CRAN (R 3.6.2)
#&amp;gt;  dplyr      * 0.8.5   2020-03-07 [1] CRAN (R 3.6.0)
#&amp;gt;  ggplot2    * 3.3.0   2020-03-05 [1] CRAN (R 3.6.0)
#&amp;gt;  infer      * 0.5.1   2019-11-19 [1] CRAN (R 3.6.0)
#&amp;gt;  parsnip    * 0.1.0   2020-04-09 [1] CRAN (R 3.6.2)
#&amp;gt;  purrr      * 0.3.3   2019-10-18 [1] CRAN (R 3.6.0)
#&amp;gt;  recipes    * 0.1.10  2020-03-18 [1] CRAN (R 3.6.0)
#&amp;gt;  rlang        0.4.5   2020-03-01 [1] CRAN (R 3.6.0)
#&amp;gt;  rsample    * 0.0.6   2020-03-31 [1] CRAN (R 3.6.2)
#&amp;gt;  tibble     * 2.1.3   2019-06-06 [1] CRAN (R 3.6.2)
#&amp;gt;  tidymodels * 0.1.0   2020-02-16 [1] CRAN (R 3.6.0)
#&amp;gt;  tune       * 0.1.0   2020-04-02 [1] CRAN (R 3.6.2)
#&amp;gt;  workflows  * 0.1.1   2020-03-17 [1] CRAN (R 3.6.0)
#&amp;gt;  yardstick  * 0.0.6   2020-03-17 [1] CRAN (R 3.6.0)
#&amp;gt; 
#&amp;gt; [1] /Library/Frameworks/R.framework/Versions/3.6/Resources/library
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>pkgdown 1.5.0</title>
      <link>https://nutriverse.io/blog/2020/03/pkgdown-1-5-0/</link>
      <pubDate>Wed, 25 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://nutriverse.io/blog/2020/03/pkgdown-1-5-0/</guid>
      <description>&lt;p&gt;We&amp;rsquo;re well chuffed to announce that 
&lt;a href=&#34;https://pkgdown.r-lib.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pkgdown&lt;/a&gt; 1.5.0 is now available on CRAN. pkgdown is designed to make it quick and easy to build a website for your package. Install it with:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;install.packages&lt;/span&gt;(&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;pkgdown&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The most important changes are highlighted below and you can see a full list of changes in the 
&lt;a href=&#34;https://pkgdown.r-lib.org/news/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;changelog&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;articles&#34;&gt;Articles&lt;/h2&gt;
&lt;p&gt;For packages with many vignettes/articles, we&amp;rsquo;ve provided much greater control over the 
&lt;a href=&#34;https://pkgdown.r-lib.org/articles&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;articles index&lt;/a&gt; and navbar. There are two major new features in this release:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The articles index page now displays article &lt;code&gt;description&lt;/code&gt;s, taken from
YAML metadata in the header of each article. This lets you provide
more context for each article.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The articles navbar is now controlled by the &lt;code&gt;articles&lt;/code&gt; section in
&lt;code&gt;_pkgdown.yml&lt;/code&gt;. The ordering of the sections, and articles within
them, control the order of the articles in the navbar, and you can
use the new &lt;code&gt;navbar&lt;/code&gt; field to control whether or not each section
appears in the navbar.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Learn more about both of these features in 
&lt;a href=&#34;https://pkgdown.r-lib.org/reference/build_articles.html#index-and-navbar&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;?&lt;code&gt;build_articles&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Thanks to 
&lt;a href=&#34;https://github.com/gadenbuie&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Garrick Aden-Buie&lt;/a&gt;, you also get much richer control over Open Graph and Twitter metadata for individual articles. See new 
&lt;a href=&#34;https://pkgdown.r-lib.org/articles/metadata.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;vignette(&amp;quot;metadata&amp;quot;)&lt;/code&gt;&lt;/a&gt; for details.&lt;/p&gt;
&lt;h2 id=&#34;reference-index&#34;&gt;Reference index&lt;/h2&gt;
&lt;p&gt;For packages with many documentation topics, you can add an additional layer of hierarchy to the reference index, using the new &lt;code&gt;subtitle&lt;/code&gt; field. To give you some sense for what that might look like, here&amp;rsquo;s an example for a partial (and imaginary) dplyr reference index:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;references&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;- &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;title&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Data&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;manipulation&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;- &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;subtitle&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;One&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;table&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;- &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;contents&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;- arrange&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;- filter&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;- mutate&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;- &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;subtitle&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;two&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;table&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;- &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;contents&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;- ends_with(&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;_join&amp;#34;&lt;/span&gt;)&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;- &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;title&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Datasets&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;- &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;contents&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; 
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;- has_keyword(&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;datasets&amp;#34;&lt;/span&gt;)&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;tables-of-contents&#34;&gt;Tables of contents&lt;/h2&gt;
&lt;p&gt;Sidebar tables of contents now use 
&lt;a href=&#34;https://afeld.github.io/bootstrap-toc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;bootstrap-toc&lt;/a&gt;, which considerably improves navigation for long articles and reference pages.&lt;/p&gt;
&lt;h2 id=&#34;other-source-repositories&#34;&gt;Other source repositories&lt;/h2&gt;
&lt;p&gt;You can now control the links to source files (in reference pages and articles) and issues and users (in the NEWS) with new the &lt;code&gt;repo$url&lt;/code&gt; config parameter. This makes it easier to use pkgdown with GitHub enterprise, packages in subdirectories, and other source hosts (like bitbucket).&lt;/p&gt;
&lt;p&gt;The YAML looks something like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;repo&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;url&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;home&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;https://github.com/r-lib/pkgdown/&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;source&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;https://github.com/r-lib/pkgdown/blob/master/&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;issue&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;https://github.com/r-lib/pkgdown/issues/&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;user&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;https://github.com/&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The individual components (e.g. path, issue number, username) are pasted on the end of these urls so they should have trailing &lt;code&gt;/&lt;/code&gt;s.&lt;/p&gt;
&lt;p&gt;pkgdown now detects GitLab urls automatically (since they use the same structure as GitHub), so you don&amp;rsquo;t need to set these links if you package is hosted on GitLab, and you&amp;rsquo;ve included a link to your source repo in the &lt;code&gt;URL&lt;/code&gt; or &lt;code&gt;BugReports&lt;/code&gt; &lt;code&gt;DESCRIPTION&lt;/code&gt; fields.&lt;/p&gt;
&lt;h2 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;A big thank you goes to 
&lt;a href=&#34;https://github.com/jayhesselberth&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@jayhesselberth&lt;/a&gt; (the co-maintainer of pkgdown), and to to the 61 other people who helped make this release possible:

&lt;a href=&#34;https://github.com/AshesITR&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@AshesITR&lt;/a&gt;,  
&lt;a href=&#34;https://github.com/baptiste&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@baptiste&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/Bisaloo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@Bisaloo&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/carloscinelli&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@carloscinelli&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/cboettig&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@cboettig&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/coatless&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@coatless&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/coolbutuseless&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@coolbutuseless&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/DanielEWeeks&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@DanielEWeeks&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/davidchall&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@davidchall&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/DavorJ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@DavorJ&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/dimagor&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@dimagor&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/erikcs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@erikcs&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/ferroao&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@ferroao&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/floriandeboissieu&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@floriandeboissieu&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/flying-sheep&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@flying-sheep&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/fmichonneau&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@fmichonneau&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/fmmattioni&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@fmmattioni&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/gaborcsardi&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@gaborcsardi&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/genomaths&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@genomaths&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/gustavdelius&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@gustavdelius&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/hadley&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@hadley&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/hbaniecki&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@hbaniecki&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/ijlyttle&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@ijlyttle&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/IndrajeetPatil&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@IndrajeetPatil&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/jangorecki&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@jangorecki&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/jayhesselberth&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@jayhesselberth&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/jeffwong-nflx&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@jeffwong-nflx&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/jennybc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@jennybc&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/jeroen&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@jeroen&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/jimhester&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@jimhester&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/JoshuaSturm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@JoshuaSturm&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/jranke&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@jranke&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/kevinushey&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@kevinushey&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/kevinwang09&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@kevinwang09&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/krlmlr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@krlmlr&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/lbusett&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@lbusett&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/lcolladotor&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@lcolladotor&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/lgatto&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@lgatto&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/lindeloev&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@lindeloev&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/lionel-&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@lionel-&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/lorenzwalthert&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@lorenzwalthert&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/m-l-1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@m-l-1&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/maelle&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@maelle&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/mattmalin&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@mattmalin&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/meghapsimatrix&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@meghapsimatrix&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/mikldk&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@mikldk&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/mllg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@mllg&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/ms609&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@ms609&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/nealrichardson&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@nealrichardson&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/nschiett&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@nschiett&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/nteetor&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@nteetor&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/pat-s&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@pat-s&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/peterdesmet&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@peterdesmet&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/rupertoverall&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@rupertoverall&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/schloerke&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@schloerke&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/slowkow&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@slowkow&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/t-kalinowski&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@t-kalinowski&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/wendtke&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@wendtke&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/ycphs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@ycphs&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/yiluheihei&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@yiluheihei&lt;/a&gt;, and 
&lt;a href=&#34;https://github.com/yonicd&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@yonicd&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>rstudio::conf 2020</title>
      <link>https://nutriverse.io/events/rstudio-conf-2020/</link>
      <pubDate>Mon, 27 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://nutriverse.io/events/rstudio-conf-2020/</guid>
      <description>&lt;p&gt;rstudio::conf 2020 covers all things RStudio, including workshops to teach you the tidyverse, and talks to show you the latest and greatest features.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Contact</title>
      <link>https://nutriverse.io/home/band_three/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://nutriverse.io/home/band_three/</guid>
      <description>&lt;p&gt;For any queries or feedback, contact us by 
&lt;a href=&#34;mailto:contact@katilingban.io&#34;&gt;email&lt;/a&gt;, message us on 
&lt;a href=&#34;https://twitter.com/katilingban&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;twitter&lt;/a&gt;, file an issue or seek support by visiting us on 
&lt;a href=&#34;https://github.com/nutriverse&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt; and going to the respective package/s you have issues on.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Explore the nutriverse</title>
      <link>https://nutriverse.io/home/band_two/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://nutriverse.io/home/band_two/</guid>
      <description>&lt;p&gt;Explore the various tools and tips provided by the nutriverse for nutrition data analysis.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Get help!</title>
      <link>https://nutriverse.io/help/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://nutriverse.io/help/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;In space, no one can hear you scream.&lt;/p&gt;
&lt;p&gt;&amp;ndash; &lt;cite&gt;Alien (1979)&lt;/cite&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Luckily the tidyverse is a friendlier place. Ease of adoption and ease of use are fundamental design principles for the packages in the tidyverse. If you are banging your head in frustration, here&amp;rsquo;s how you can help us help you.&lt;/p&gt;
&lt;h2 id=&#34;reprex&#34;&gt;Make a reprex&lt;/h2&gt;
&lt;p&gt;If you need help getting unstuck, the first step is to create a &lt;strong&gt;reprex&lt;/strong&gt;, or reproducible example. The goal of a reprex is to package your problematic code in such a way that other people can run it and feel your pain. Then, hopefully, they can provide a solution and put you out of your misery.&lt;/p&gt;
&lt;p&gt;There are two parts to creating a reprex:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;First, you need to make your code reproducible. This means that you need
to capture everything, i.e., include any &lt;code&gt;library()&lt;/code&gt; calls and create all necessary objects. The easiest way to make sure you&amp;rsquo;ve done this is to use the 
&lt;a href=&#34;https://nutriverse.io/help#reprex-pkg&#34;&gt;reprex package&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Second, you need to make it minimal. Strip away everything that is not directly related to your problem. This usually involves creating a much smaller and simpler R object than the one you&amp;rsquo;re facing in real life or even using built-in data.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That sounds like a lot of work!  And it can be, but it has a great payoff:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;80% of the time creating an excellent reprex reveals the source of your problem. It&amp;rsquo;s amazing how often the process of writing up a self-contained and minimal example allows you to answer your own question.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The other 20% of time you will have captured the essence of your problem in
a way that is easy for others to play with. This substantially improves
your chances of getting help!&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reprex-pkg&#34;&gt;The reprex package&lt;/h2&gt;
&lt;p&gt;When creating a reprex by hand, it&amp;rsquo;s easy to accidentally miss something that means your code can&amp;rsquo;t be run on someone else&amp;rsquo;s computer. Avoid this problem by using the 
&lt;a href=&#34;http://reprex.tidyverse.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;reprex package&lt;/a&gt;. It&amp;rsquo;s installed as part of the tidyverse &lt;em&gt;(will be true soon)&lt;/em&gt; or you can install it by itself. Go ahead and load it.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-R&#34; data-lang=&#34;R&#34;&gt;&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;## pick one:&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;##   install.packages(&amp;#34;tidyverse&amp;#34;) &amp;lt;-- will work soon&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;##   install.packages(&amp;#34;reprex&amp;#34;)    &amp;lt;-- works today&lt;/span&gt;

&lt;span style=&#34;color:#00f&#34;&gt;library&lt;/span&gt;(reprex)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Write a bit of code and copy it to the clipboard:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-R&#34; data-lang=&#34;R&#34;&gt;(y &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;4&lt;/span&gt;)
&lt;span style=&#34;color:#00f&#34;&gt;mean&lt;/span&gt;(y)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Enter &lt;code&gt;reprex()&lt;/code&gt; in the R Console. In RStudio, you&amp;rsquo;ll see a preview of your rendered reprex.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-R&#34; data-lang=&#34;R&#34;&gt;(y &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;4&lt;/span&gt;)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; [1] 1 2 3 4&lt;/span&gt;
&lt;span style=&#34;color:#00f&#34;&gt;mean&lt;/span&gt;(y)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; [1] 2.5&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;It is now ready and waiting on your clipboard, so you can paste it into, say, a GitHub issue. In RStudio, you can access reprex from the 
&lt;a href=&#34;https://rstudio.github.io/rstudioaddins/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;addins menu&lt;/a&gt;, which makes it even easier to point out your code and select the output format.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;reprex-addins-menu.png&#34; alt=&#34;reprex addins menu&#34;&gt;&lt;/img&gt;
&lt;img src=&#34;reprex-addin.png&#34; alt=&#34;reprex addin interface&#34;&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;In either case, you can eventually 
&lt;a href=&#34;http://reprex.tidyverse.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;explore other features&lt;/a&gt;, such as formatting output for Stack Overflow or as a commented R script. reprex even uploads figures so you can easily ask questions about ggplot2.&lt;/p&gt;
&lt;p&gt;If your code is not self-contained, running &lt;code&gt;reprex()&lt;/code&gt; results in an error. It may feel like tough love, but this way you can get your story straight in private. The reprex format also strongly encourages you to find the minimal dataset necessary to show your problem. Creating an effective reprex is a learned skill and the immediate feedback from reprex makes this very concrete.&lt;/p&gt;
&lt;h2 id=&#34;where-to-ask&#34;&gt;Where to ask&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;help-is-on-the-way.jpg&#34; alt=&#34;&#34; width=&#34;200&#34; height=&#34;200&#34; align=&#34;right&#34; style=&#34;padding:1em;&#34; /&gt;&lt;/p&gt;
&lt;!-- Thanks to Mark Hansen for the image! https://twitter.com/cocteau/status/893811714420088832 --&gt;
&lt;p&gt;Now that you&amp;rsquo;ve made a reprex that you can easily inflict on others, you need to share it in an appropriate forum. Here are some options:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;http://community.rstudio.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;community.rstudio.com&lt;/strong&gt;&lt;/a&gt;: This is a warm
and welcoming place to ask any questions you might have about the
tidyverse (and you can also ask questions about shiny and RStudio there
too!)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;https://stackoverflow.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Stack Overflow&lt;/strong&gt;&lt;/a&gt;. You&amp;rsquo;re probably already familiar
with Stack Overflow from googling: it&amp;rsquo;s a frequent source of answers to
coding related questions. Asking a question on Stack Overflow can be
intimidating, but if you&amp;rsquo;ve taken the time to create a reprex, you&amp;rsquo;re much
more likely to get a useful answer. Make sure to 
&lt;a href=&#34;https://stackoverflow.com/help/tagging&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tag your question&lt;/a&gt; with R
and tidyverse so that the right people are more likely to see it.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;https://twitter.com/search?q=%23rstats&amp;amp;src=typd&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Twitter&lt;/strong&gt;&lt;/a&gt;. It&amp;rsquo;s hard to share your reprex only on twitter, because 140 characters are rarely enough and screenshots don&amp;rsquo;t help others play with your code. But twitter is a great place to share a link to your reprex that&amp;rsquo;s hosted elsewhere. The 
&lt;a href=&#34;https://twitter.com/search?q=%23rstats&amp;amp;src=typd&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;#rstats twitter&lt;/a&gt; community is extremely friendly and active, and is a great crowd to be a part of. Make sure you tag your tweet with #rstats and #tidyverse.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If you think you&amp;rsquo;ve found a &lt;strong&gt;bug&lt;/strong&gt;, please follow the instructions on

&lt;a href=&#34;https://nutriverse.io/contribute#issues&#34;&gt;contributing to the tidyverse&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Nutriverse</title>
      <link>https://nutriverse.io/home/band_one/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://nutriverse.io/home/band_one/</guid>
      <description>&lt;p&gt;Our mission is to foster a community of nutrition data analysts who use R language for statistical computing. We aim to provide this community with support through the development of robust and performant R packages as tools for their analytical tasks, and learning through day-to-day technical guidance and impactful peer-to-peer learning.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tidyverse packages</title>
      <link>https://nutriverse.io/packages/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://nutriverse.io/packages/</guid>
      <description>&lt;h2 id=&#34;installation-and-use&#34;&gt;Installation and use&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Install all the packages in the tidyverse by running &lt;code&gt;install.packages(&amp;quot;tidyverse&amp;quot;)&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run &lt;code&gt;library(tidyverse)&lt;/code&gt; to load the core tidyverse and make it available
in your current R session.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Learn more about the tidyverse package at &lt;a href=&#34;https://tidyverse.tidyverse.org&#34;&gt;https://tidyverse.tidyverse.org&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;core-tidyverse&#34;&gt;Core tidyverse&lt;/h2&gt;
&lt;p&gt;The core tidyverse includes the packages that you&amp;rsquo;re likely to use in everyday data analyses. As of tidyverse 1.3.0, the following packages are included in the core tidyverse:&lt;/p&gt;


&lt;div class=&#34;package-section&#34;&gt;
  &lt;div class=&#34;packages&#34;&gt;
  
  &lt;div class=&#34;package&#34;&gt;
    &lt;a href=&#34;https://nutriverse.io/zscorer&#34; target=&#34;_blank&#34;&gt;
    &lt;img class=&#34;package-image&#34; src=&#34;https://nutriverse.io/css/images/hex/zscorer.png&#34; alt=&#34;&#34; aria-hidden=&#34;true&#34;&gt;&lt;/img&gt;
    &lt;/a&gt;
    
    &lt;div class=&#34;package-info&#34;&gt;
      &lt;h3&gt;&lt;a href=&#34;https://nutriverse.io/zscorer&#34; target=&#34;_blank&#34;&gt;zscorer&lt;/a&gt;&lt;/h3&gt;
      &lt;p&gt;Anthropometric z-score calculator&amp;nbsp;&lt;a href=&#34;https://nutriverse.io/zscorer&#34; aria-hidden=&#34;true&#34;  target=&#34;_blank&#34;&gt;Go to docs...&lt;/a&gt;&lt;/p&gt;
    &lt;/div&gt; 
  &lt;/div&gt; 
  
  &lt;div class=&#34;package&#34;&gt;
    &lt;a href=&#34;https://nutriverse.io/nipnTK&#34; target=&#34;_blank&#34;&gt;
    &lt;img class=&#34;package-image&#34; src=&#34;https://nutriverse.io/css/images/hex/nipnTK.png&#34; alt=&#34;&#34; aria-hidden=&#34;true&#34;&gt;&lt;/img&gt;
    &lt;/a&gt;
    
    &lt;div class=&#34;package-info&#34;&gt;
      &lt;h3&gt;&lt;a href=&#34;https://nutriverse.io/nipnTK&#34; target=&#34;_blank&#34;&gt;nipnTK&lt;/a&gt;&lt;/h3&gt;
      &lt;p&gt;National Information Platforms for Nutrition (NiPN) data quality toolkit&amp;nbsp;&lt;a href=&#34;https://nutriverse.io/nipnTK&#34; aria-hidden=&#34;true&#34;  target=&#34;_blank&#34;&gt;Go to docs...&lt;/a&gt;&lt;/p&gt;
    &lt;/div&gt; 
  &lt;/div&gt; 
  
  &lt;div class=&#34;package&#34;&gt;
    &lt;a href=&#34;https://nutriverse.io/anthrocheckr&#34; target=&#34;_blank&#34;&gt;
    &lt;img class=&#34;package-image&#34; src=&#34;https://nutriverse.io/css/images/hex/anthrocheckr.png&#34; alt=&#34;&#34; aria-hidden=&#34;true&#34;&gt;&lt;/img&gt;
    &lt;/a&gt;
    
    &lt;div class=&#34;package-info&#34;&gt;
      &lt;h3&gt;&lt;a href=&#34;https://nutriverse.io/anthrocheckr&#34; target=&#34;_blank&#34;&gt;anthrocheckr&lt;/a&gt;&lt;/h3&gt;
      &lt;p&gt;An implementation of anthropometric measurement standardisation Tests in R&amp;nbsp;&lt;a href=&#34;https://nutriverse.io/anthrocheckr&#34; aria-hidden=&#34;true&#34;  target=&#34;_blank&#34;&gt;Go to docs...&lt;/a&gt;&lt;/p&gt;
    &lt;/div&gt; 
  &lt;/div&gt; 
  
  &lt;div class=&#34;package&#34;&gt;
    
    &lt;div class=&#34;package-info&#34;&gt;
      &lt;h3&gt;&lt;a href=&#34;https://nutriverse.io/nutricheckr&#34; target=&#34;_blank&#34;&gt;nutricheckr&lt;/a&gt;&lt;/h3&gt;
      &lt;p&gt;Tools for nutrition assessment and analysis&amp;nbsp;&lt;a href=&#34;https://nutriverse.io/nutricheckr&#34; aria-hidden=&#34;true&#34;  target=&#34;_blank&#34;&gt;Go to docs...&lt;/a&gt;&lt;/p&gt;
    &lt;/div&gt; 
  &lt;/div&gt; 
  
  &lt;div class=&#34;package&#34;&gt;
    &lt;a href=&#34;https://nutriverse.io/intergrowth&#34; target=&#34;_blank&#34;&gt;
    &lt;img class=&#34;package-image&#34; src=&#34;https://nutriverse.io/css/images/hex/intergrowth.png&#34; alt=&#34;&#34; aria-hidden=&#34;true&#34;&gt;&lt;/img&gt;
    &lt;/a&gt;
    
    &lt;div class=&#34;package-info&#34;&gt;
      &lt;h3&gt;&lt;a href=&#34;https://nutriverse.io/intergrowth&#34; target=&#34;_blank&#34;&gt;intergrowth&lt;/a&gt;&lt;/h3&gt;
      &lt;p&gt;An R implementation of the INTERGROWTH-21st standards and tools&amp;nbsp;&lt;a href=&#34;https://nutriverse.io/intergrowth&#34; aria-hidden=&#34;true&#34;  target=&#34;_blank&#34;&gt;Go to docs...&lt;/a&gt;&lt;/p&gt;
    &lt;/div&gt; 
  &lt;/div&gt; 
  
  &lt;div class=&#34;package&#34;&gt;
    &lt;a href=&#34;https://nutriverse.io/nutricea&#34; target=&#34;_blank&#34;&gt;
    &lt;img class=&#34;package-image&#34; src=&#34;https://nutriverse.io/css/images/hex/nutricea.png&#34; alt=&#34;&#34; aria-hidden=&#34;true&#34;&gt;&lt;/img&gt;
    &lt;/a&gt;
    
    &lt;div class=&#34;package-info&#34;&gt;
      &lt;h3&gt;&lt;a href=&#34;https://nutriverse.io/nutricea&#34; target=&#34;_blank&#34;&gt;nutricea&lt;/a&gt;&lt;/h3&gt;
      &lt;p&gt;Nutrition cost-effectiveness analysis (CEA) in R&amp;nbsp;&lt;a href=&#34;https://nutriverse.io/nutricea&#34; aria-hidden=&#34;true&#34;  target=&#34;_blank&#34;&gt;Go to docs...&lt;/a&gt;&lt;/p&gt;
    &lt;/div&gt; 
  &lt;/div&gt; 
  
  &lt;div class=&#34;package&#34;&gt;
    
    &lt;div class=&#34;package-info&#34;&gt;
      &lt;h3&gt;&lt;a href=&#34;https://nutriverse.io/nutribudgetr&#34; target=&#34;_blank&#34;&gt;nutribudgetr&lt;/a&gt;&lt;/h3&gt;
      &lt;p&gt;An implementation of SPRING&amp;#39;s nutrition budget analysis tool in R&amp;nbsp;&lt;a href=&#34;https://nutriverse.io/nutribudgetr&#34; aria-hidden=&#34;true&#34;  target=&#34;_blank&#34;&gt;Go to docs...&lt;/a&gt;&lt;/p&gt;
    &lt;/div&gt; 
  &lt;/div&gt; 
  
  &lt;div class=&#34;package&#34;&gt;
    &lt;a href=&#34;https://nutriverse.io/gnr&#34; target=&#34;_blank&#34;&gt;
    &lt;img class=&#34;package-image&#34; src=&#34;https://nutriverse.io/css/images/hex/gnr.png&#34; alt=&#34;&#34; aria-hidden=&#34;true&#34;&gt;&lt;/img&gt;
    &lt;/a&gt;
    
    &lt;div class=&#34;package-info&#34;&gt;
      &lt;h3&gt;&lt;a href=&#34;https://nutriverse.io/gnr&#34; target=&#34;_blank&#34;&gt;gnr&lt;/a&gt;&lt;/h3&gt;
      &lt;p&gt;R companion to the Global Nutrition Report&amp;nbsp;&lt;a href=&#34;https://nutriverse.io/gnr&#34; aria-hidden=&#34;true&#34;  target=&#34;_blank&#34;&gt;Go to docs...&lt;/a&gt;&lt;/p&gt;
    &lt;/div&gt; 
  &lt;/div&gt; 
  
  
  &lt;/div&gt; 
&lt;/div&gt; 
&lt;p&gt;The tidyverse also includes many other packages with more specialised usage. They are not loaded automatically with &lt;code&gt;library(tidyverse)&lt;/code&gt;, so you&amp;rsquo;ll need to load each one with its own call to &lt;code&gt;library()&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;import&#34;&gt;Import&lt;/h2&gt;
&lt;p&gt;As well as 
&lt;a href=&#34;https://readr.tidyverse.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;readr&lt;/a&gt;, for reading flat files, the tidyverse package installs a number of other packages for reading data:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/rstats-db/DBI&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DBI&lt;/a&gt; for relational databases.
(Maintained by 
&lt;a href=&#34;https://www.cynkra.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kirill Müller&lt;/a&gt;.)
You&amp;rsquo;ll need to pair DBI with a database specific backends like

&lt;a href=&#34;https://rsqlite.r-dbi.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RSQLite&lt;/a&gt;,

&lt;a href=&#34;https://rmariadb.r-dbi.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RMariaDB&lt;/a&gt;,

&lt;a href=&#34;https://rpostgres.r-dbi.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RPostgres&lt;/a&gt;, or

&lt;a href=&#34;https://github.com/r-dbi/odbc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;odbc&lt;/a&gt;.
Learn more at &lt;a href=&#34;https://db.rstudio.com&#34;&gt;https://db.rstudio.com&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;https://haven.tidyverse.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;haven&lt;/a&gt; for SPSS, Stata, and SAS data.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/r-lib/httr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;httr&lt;/a&gt; for web APIs.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;https://readxl.tidyverse.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;readxl&lt;/a&gt; for &lt;code&gt;.xls&lt;/code&gt; and &lt;code&gt;.xlsx&lt;/code&gt; sheets.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/tidyverse/rvest&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;rvest&lt;/a&gt; for web scraping.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/jeroen/jsonlite#jsonlite&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;jsonlite&lt;/a&gt;
for JSON. (Maintained by 
&lt;a href=&#34;https://github.com/jeroen&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jeroen Ooms&lt;/a&gt;.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/r-lib/xml2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;xml2&lt;/a&gt; for XML.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;wrangle&#34;&gt;Wrangle&lt;/h2&gt;
&lt;p&gt;In addition to 
&lt;a href=&#34;https://tidyr.tidyverse.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tidyr&lt;/a&gt;, and 
&lt;a href=&#34;https://dplyr.tidyverse.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;dplyr&lt;/a&gt;, there are five packages (including 
&lt;a href=&#34;https://stringr.tidyverse.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;stringr&lt;/a&gt; and 
&lt;a href=&#34;https://forcats.tidyverse.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;forcats&lt;/a&gt;) which are designed to work with specific types of data:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://lubridate.tidyverse.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;lubridate&lt;/a&gt; for dates and date-times.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/tidyverse/hms&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;hms&lt;/a&gt; for time-of-day values.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/tidyverse/blob&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blob&lt;/a&gt; for storing blob (binary) data.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;program&#34;&gt;Program&lt;/h2&gt;
&lt;p&gt;In addition to 
&lt;a href=&#34;https://purrr.tidyverse.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;purrr&lt;/a&gt;, which provides very consistent and natural methods for iterating on R objects, there are two additional tidyverse packages that help with general programming challenges:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;https://magrittr.tidyverse.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;magrittr&lt;/a&gt; provides the pipe, &lt;code&gt;%&amp;gt;%&lt;/code&gt; used
throughout the tidyverse. It also provide a number of more specialised
piping operators (like &lt;code&gt;%$%&lt;/code&gt; and &lt;code&gt;%&amp;lt;&amp;gt;%&lt;/code&gt;) that can be useful in other places.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/tidyverse/glue&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;glue&lt;/a&gt; provides an alternative to
&lt;code&gt;paste()&lt;/code&gt; that makes it easier to combine data and strings.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;model&#34;&gt;Model&lt;/h2&gt;
&lt;p&gt;Modeling with the tidyverse uses the collection of 
&lt;a href=&#34;https://www.tidymodels.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tidymodels packages&lt;/a&gt;, which largely replace the 
&lt;a href=&#34;https://github.com/tidyverse/modelr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;modelr&lt;/a&gt; package used in 
&lt;a href=&#34;https://r4ds.had.co.nz/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;R4DS&lt;/a&gt;. These packages provide a comprehensive foundation for creating and using models of all types. Visit the 
&lt;a href=&#34;https://www.tidymodels.org/start/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Getting Started&lt;/em&gt;&lt;/a&gt; guide or, for more detailed examples, go straight to the 
&lt;a href=&#34;https://www.tidymodels.org/learn/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Learn&lt;/em&gt;&lt;/a&gt; page.&lt;/p&gt;
&lt;h2 id=&#34;get-help&#34;&gt;Get help&lt;/h2&gt;
&lt;p&gt;If you’re asking for R help, reporting a bug, or requesting a new feature, you’re more likely to succeed if you include a good reproducible example, which is precisely what the 
&lt;a href=&#34;https://reprex.tidyverse.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;reprex&lt;/a&gt; package is meant for. You can learn more about reprex, along with other tips on how to help others help you in the 
&lt;a href=&#34;https://www.tidyverse.org/help/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;help section&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
