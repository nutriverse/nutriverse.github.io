<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Create robust models | Nutriverse</title>
    <link>https://nutriverse.io/learn/models/</link>
      <atom:link href="https://nutriverse.io/learn/models/index.xml" rel="self" type="application/rss+xml" />
    <description>Create robust models</description>
    <generator>Hugo -- gohugo.io</generator><language>en-gb</language>
    <item>
      <title>Regression models two ways</title>
      <link>https://nutriverse.io/learn/models/parsnip-ranger-glmnet/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://nutriverse.io/learn/models/parsnip-ranger-glmnet/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;To use the code in this article, you will need to install the following packages: AmesHousing, glmnet, randomForest, ranger, and tidymodels.&lt;/p&gt;
&lt;p&gt;We can create regression models with the tidymodels package 
&lt;a href=&#34;https://tidymodels.github.io/parsnip/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;parsnip&lt;/a&gt; to predict continuous or numeric quantities. Here, let&amp;rsquo;s first fit a random forest model, which does &lt;em&gt;not&lt;/em&gt; require all numeric input (see discussion 
&lt;a href=&#34;https://bookdown.org/max/FES/categorical-trees.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;) and discuss how to use &lt;code&gt;fit()&lt;/code&gt; and &lt;code&gt;fit_xy()&lt;/code&gt;, as well as &lt;em&gt;data descriptors&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Second, let&amp;rsquo;s fit a regularized linear regression model to demonstrate how to move between different types of models using parsnip.&lt;/p&gt;
&lt;h2 id=&#34;the-ames-housing-data&#34;&gt;The Ames housing data&lt;/h2&gt;
&lt;p&gt;We&amp;rsquo;ll use the Ames housing data set to demonstrate how to create regression models using parsnip. First, set up the data set and create a simple training/test set split:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;library&lt;/span&gt;(AmesHousing)
ames &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;make_ames&lt;/span&gt;()

&lt;span style=&#34;color:#00f&#34;&gt;library&lt;/span&gt;(tidymodels)

&lt;span style=&#34;color:#00f&#34;&gt;set.seed&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;4595&lt;/span&gt;)
data_split &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;initial_split&lt;/span&gt;(ames, strata &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;Sale_Price&amp;#34;&lt;/span&gt;, p &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;0.75&lt;/span&gt;)

ames_train &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;training&lt;/span&gt;(data_split)
ames_test  &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;testing&lt;/span&gt;(data_split)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The use of the test set here is &lt;em&gt;only for illustration&lt;/em&gt;; normally in a data analysis these data would be saved to the very end after many models have been evaluated.&lt;/p&gt;
&lt;h2 id=&#34;random-forest&#34;&gt;Random forest&lt;/h2&gt;
&lt;p&gt;We&amp;rsquo;ll start by fitting a random forest model to a small set of parameters. Let&amp;rsquo;s create a model with the predictors &lt;code&gt;Longitude&lt;/code&gt;, &lt;code&gt;Latitude&lt;/code&gt;, &lt;code&gt;Lot_Area&lt;/code&gt;, &lt;code&gt;Neighborhood&lt;/code&gt;, and &lt;code&gt;Year_Sold&lt;/code&gt;. A simple random forest model can be specified via:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;rf_defaults &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;rand_forest&lt;/span&gt;(mode &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;regression&amp;#34;&lt;/span&gt;)
rf_defaults
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Random Forest Model Specification (regression)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The model will be fit with the ranger package by default. Since we didn&amp;rsquo;t add any extra arguments to &lt;code&gt;fit&lt;/code&gt;, &lt;em&gt;many&lt;/em&gt; of the arguments will be set to their defaults from the function  &lt;code&gt;ranger::ranger()&lt;/code&gt;. The help pages for the model function describe the default parameters and you can also use the &lt;code&gt;translate()&lt;/code&gt; function to check out such details.&lt;/p&gt;
&lt;p&gt;The parsnip package provides two different interfaces to fit a model:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the formula interface (&lt;code&gt;fit()&lt;/code&gt;), and&lt;/li&gt;
&lt;li&gt;the non-formula interface (&lt;code&gt;fit_xy()&lt;/code&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s start with the non-formula interface:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;preds &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;Longitude&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;Latitude&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;Lot_Area&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;Neighborhood&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;Year_Sold&amp;#34;&lt;/span&gt;)

rf_xy_fit &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  rf_defaults &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;set_engine&lt;/span&gt;(&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;ranger&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;fit_xy&lt;/span&gt;(
    x &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; ames_train[, preds],
    y &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;log10&lt;/span&gt;(ames_train&lt;span style=&#34;color:#666&#34;&gt;$&lt;/span&gt;Sale_Price)
  )

rf_xy_fit
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; parsnip model object&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Fit time:  952ms &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Ranger result&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Call:&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  ranger::ranger(formula = formula, data = data, num.threads = 1,      verbose = FALSE, seed = sample.int(10^5, 1)) &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Type:                             Regression &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Number of trees:                  500 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Sample size:                      2199 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Number of independent variables:  5 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Mtry:                             2 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Target node size:                 5 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Variable importance mode:         none &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Splitrule:                        variance &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; OOB prediction error (MSE):       0.00844 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; R squared (OOB):                  0.736&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The non-formula interface doesn&amp;rsquo;t do anything to the predictors before passing them to the underlying model function. This particular model does &lt;em&gt;not&lt;/em&gt; require indicator variables (sometimes called &amp;ldquo;dummy variables&amp;rdquo;) to be created prior to fitting the model. Note that the output shows &amp;ldquo;Number of independent variables:  5&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;For regression models, we can use the basic &lt;code&gt;predict()&lt;/code&gt; method, which returns a tibble with a column named &lt;code&gt;.pred&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;test_results &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  ames_test &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;select&lt;/span&gt;(Sale_Price) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;mutate&lt;/span&gt;(Sale_Price &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;log10&lt;/span&gt;(Sale_Price)) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;bind_cols&lt;/span&gt;(
    &lt;span style=&#34;color:#00f&#34;&gt;predict&lt;/span&gt;(rf_xy_fit, new_data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; ames_test[, preds])
  )
test_results &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;slice&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;5&lt;/span&gt;)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 5 x 2&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   Sale_Price .pred&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;        &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 1       5.33  5.22&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 2       5.02  5.21&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 3       5.27  5.25&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 4       5.60  5.51&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 5       5.28  5.24&lt;/span&gt;

&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# summarize performance&lt;/span&gt;
test_results &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;metrics&lt;/span&gt;(truth &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; Sale_Price, estimate &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; .pred) 
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 3 x 3&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   .metric .estimator .estimate&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 1 rmse    standard      0.0914&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 2 rsq     standard      0.717 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 3 mae     standard      0.0662&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Note that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If the model required indicator variables, we would have to create them manually prior to using &lt;code&gt;fit()&lt;/code&gt; (perhaps using the recipes package).&lt;/li&gt;
&lt;li&gt;We had to manually log the outcome prior to modeling.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now, for illustration, let&amp;rsquo;s use the formula method using some new parameter values:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;rand_forest&lt;/span&gt;(mode &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;regression&amp;#34;&lt;/span&gt;, mtry &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;3&lt;/span&gt;, trees &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;1000&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;set_engine&lt;/span&gt;(&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;ranger&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;fit&lt;/span&gt;(
    &lt;span style=&#34;color:#00f&#34;&gt;log10&lt;/span&gt;(Sale_Price) &lt;span style=&#34;color:#666&#34;&gt;~&lt;/span&gt; Longitude &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; Latitude &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; Lot_Area &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; Neighborhood &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; Year_Sold,
    data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; ames_train
  )
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; parsnip model object&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Fit time:  2.6s &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Ranger result&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Call:&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  ranger::ranger(formula = formula, data = data, mtry = ~3, num.trees = ~1000,      num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1)) &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Type:                             Regression &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Number of trees:                  1000 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Sample size:                      2199 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Number of independent variables:  5 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Mtry:                             3 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Target node size:                 5 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Variable importance mode:         none &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Splitrule:                        variance &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; OOB prediction error (MSE):       0.00848 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; R squared (OOB):                  0.735&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Suppose that we would like to use the randomForest package instead of ranger. To do so, the only part of the syntax that needs to change is the &lt;code&gt;set_engine()&lt;/code&gt; argument:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;rand_forest&lt;/span&gt;(mode &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;regression&amp;#34;&lt;/span&gt;, mtry &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;3&lt;/span&gt;, trees &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;1000&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;set_engine&lt;/span&gt;(&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;randomForest&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;fit&lt;/span&gt;(
    &lt;span style=&#34;color:#00f&#34;&gt;log10&lt;/span&gt;(Sale_Price) &lt;span style=&#34;color:#666&#34;&gt;~&lt;/span&gt; Longitude &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; Latitude &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; Lot_Area &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; Neighborhood &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; Year_Sold,
    data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; ames_train
  )
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; parsnip model object&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Fit time:  2.1s &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Call:&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  randomForest(x = as.data.frame(x), y = y, ntree = ~1000, mtry = ~3) &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;                Type of random forest: regression&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;                      Number of trees: 1000&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; No. of variables tried at each split: 3&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;           Mean of squared residuals: 0.013&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;                     % Var explained: 59.4&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Look at the formula code that was printed out; one function uses the argument name &lt;code&gt;ntree&lt;/code&gt; and the other uses &lt;code&gt;num.trees&lt;/code&gt;. The parsnip models don&amp;rsquo;t require you to know the specific names of the main arguments.&lt;/p&gt;
&lt;p&gt;Now suppose that we want to modify the value of &lt;code&gt;mtry&lt;/code&gt; based on the number of predictors in the data. Usually, a good default value is &lt;code&gt;floor(sqrt(num_predictors))&lt;/code&gt; but a pure bagging model requires an &lt;code&gt;mtry&lt;/code&gt; value equal to the total number of parameters. There may be cases where you may not know how many predictors are going to be present when the model will be fit (perhaps due to the generation of indicator variables or a variable filter) so this might be difficult to know exactly ahead of time when you write your code.&lt;/p&gt;
&lt;p&gt;When the model it being fit by parsnip, 
&lt;a href=&#34;https://tidymodels.github.io/parsnip/reference/descriptors.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;data descriptors&lt;/em&gt;&lt;/a&gt; are made available. These attempt to let you know what you will have available when the model is fit. When a model object is created (say using &lt;code&gt;rand_forest()&lt;/code&gt;), the values of the arguments that you give it are &lt;em&gt;immediately evaluated&lt;/em&gt; unless you delay them. To delay the evaluation of any argument, you can used &lt;code&gt;rlang::expr()&lt;/code&gt; to make an expression.&lt;/p&gt;
&lt;p&gt;Two relevant data descriptors for our example model are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;.preds()&lt;/code&gt;: the number of predictor &lt;em&gt;variables&lt;/em&gt; in the data set that are associated with the predictors &lt;strong&gt;prior to dummy variable creation&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;.cols()&lt;/code&gt;: the number of predictor &lt;em&gt;columns&lt;/em&gt; after dummy variables (or other encodings) are created.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Since ranger won&amp;rsquo;t create indicator values, &lt;code&gt;.preds()&lt;/code&gt; would be appropriate for &lt;code&gt;mtry&lt;/code&gt; for a bagging model.&lt;/p&gt;
&lt;p&gt;For example, let&amp;rsquo;s use an expression with the &lt;code&gt;.preds()&lt;/code&gt; descriptor to fit a bagging model:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;rand_forest&lt;/span&gt;(mode &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;regression&amp;#34;&lt;/span&gt;, mtry &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;.preds&lt;/span&gt;(), trees &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;1000&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;set_engine&lt;/span&gt;(&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;ranger&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;fit&lt;/span&gt;(
    &lt;span style=&#34;color:#00f&#34;&gt;log10&lt;/span&gt;(Sale_Price) &lt;span style=&#34;color:#666&#34;&gt;~&lt;/span&gt; Longitude &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; Latitude &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; Lot_Area &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; Neighborhood &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; Year_Sold,
    data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; ames_train
  )
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; parsnip model object&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Fit time:  3.6s &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Ranger result&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Call:&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  ranger::ranger(formula = formula, data = data, mtry = ~.preds(),      num.trees = ~1000, num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1)) &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Type:                             Regression &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Number of trees:                  1000 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Sample size:                      2199 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Number of independent variables:  5 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Mtry:                             5 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Target node size:                 5 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Variable importance mode:         none &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Splitrule:                        variance &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; OOB prediction error (MSE):       0.00869 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; R squared (OOB):                  0.728&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;regularized-regression&#34;&gt;Regularized regression&lt;/h2&gt;
&lt;p&gt;A linear model might work for this data set as well. We can use the &lt;code&gt;linear_reg()&lt;/code&gt; parsnip model. There are two engines that can perform regularization/penalization, the glmnet and sparklyr packages. Let&amp;rsquo;s use the former here. The glmnet package only implements a non-formula method, but parsnip will allow either one to be used.&lt;/p&gt;
&lt;p&gt;When regularization is used, the predictors should first be centered and scaled before being passed to the model. The formula method won&amp;rsquo;t do that automatically so we will need to do this ourselves. We&amp;rsquo;ll use the 
&lt;a href=&#34;https://tidymodels.github.io/recipes/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;recipes&lt;/a&gt; package for these steps.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;norm_recipe &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;recipe&lt;/span&gt;(
    Sale_Price &lt;span style=&#34;color:#666&#34;&gt;~&lt;/span&gt; Longitude &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; Latitude &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; Lot_Area &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; Neighborhood &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; Year_Sold, 
    data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; ames_train
  ) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;step_other&lt;/span&gt;(Neighborhood) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;step_dummy&lt;/span&gt;(&lt;span style=&#34;color:#00f&#34;&gt;all_nominal&lt;/span&gt;()) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;step_center&lt;/span&gt;(&lt;span style=&#34;color:#00f&#34;&gt;all_predictors&lt;/span&gt;()) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;step_scale&lt;/span&gt;(&lt;span style=&#34;color:#00f&#34;&gt;all_predictors&lt;/span&gt;()) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;step_log&lt;/span&gt;(Sale_Price, base &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;10&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# estimate the means and standard deviations&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;prep&lt;/span&gt;(training &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; ames_train, retain &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;TRUE&lt;/span&gt;)

&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# Now let&amp;#39;s fit the model using the processed version of the data&lt;/span&gt;

glmn_fit &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;linear_reg&lt;/span&gt;(penalty &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;0.001&lt;/span&gt;, mixture &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;0.5&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;set_engine&lt;/span&gt;(&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;glmnet&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;fit&lt;/span&gt;(Sale_Price &lt;span style=&#34;color:#666&#34;&gt;~&lt;/span&gt; ., data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;juice&lt;/span&gt;(norm_recipe))
glmn_fit
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; parsnip model object&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Fit time:  13ms &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Call:  glmnet::glmnet(x = as.matrix(x), y = y, family = &amp;#34;gaussian&amp;#34;,      alpha = ~0.5) &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;    Df  %Dev Lambda&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 1   0 0.000 0.1370&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 2   1 0.019 0.1250&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 3   1 0.036 0.1140&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 4   1 0.050 0.1040&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 5   2 0.068 0.0946&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 6   4 0.093 0.0862&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 7   5 0.125 0.0785&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 8   5 0.153 0.0716&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 9   7 0.184 0.0652&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 10  7 0.214 0.0594&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 11  7 0.240 0.0541&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 12  8 0.262 0.0493&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 13  8 0.286 0.0449&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 14  8 0.306 0.0409&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 15  8 0.323 0.0373&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 16  8 0.338 0.0340&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 17  8 0.350 0.0310&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 18  8 0.361 0.0282&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 19  9 0.370 0.0257&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 20  9 0.379 0.0234&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 21  9 0.386 0.0213&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 22  9 0.392 0.0195&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 23  9 0.397 0.0177&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 24  9 0.401 0.0161&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 25  9 0.405 0.0147&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 26  9 0.408 0.0134&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 27 10 0.410 0.0122&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 28 11 0.413 0.0111&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 29 11 0.415 0.0101&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 30 11 0.417 0.0092&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 31 12 0.418 0.0084&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 32 12 0.420 0.0077&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 33 12 0.421 0.0070&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 34 12 0.422 0.0064&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 35 12 0.423 0.0058&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 36 12 0.423 0.0053&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 37 12 0.424 0.0048&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 38 12 0.425 0.0044&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 39 12 0.425 0.0040&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 40 12 0.425 0.0036&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 41 12 0.426 0.0033&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 42 12 0.426 0.0030&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 43 12 0.426 0.0028&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 44 12 0.426 0.0025&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 45 12 0.426 0.0023&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 46 12 0.426 0.0021&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 47 12 0.427 0.0019&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 48 12 0.427 0.0017&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 49 12 0.427 0.0016&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 50 12 0.427 0.0014&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 51 12 0.427 0.0013&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 52 12 0.427 0.0012&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 53 12 0.427 0.0011&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 54 12 0.427 0.0010&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 55 12 0.427 0.0009&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 56 12 0.427 0.0008&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 57 12 0.427 0.0008&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 58 12 0.427 0.0007&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 59 12 0.427 0.0006&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 60 12 0.427 0.0006&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 61 12 0.427 0.0005&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 62 12 0.427 0.0005&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 63 12 0.427 0.0004&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 64 12 0.427 0.0004&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 65 12 0.427 0.0004&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If &lt;code&gt;penalty&lt;/code&gt; were not specified, all of the &lt;code&gt;lambda&lt;/code&gt; values would be computed.&lt;/p&gt;
&lt;p&gt;To get the predictions for this specific value of &lt;code&gt;lambda&lt;/code&gt; (aka &lt;code&gt;penalty&lt;/code&gt;):&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# First, get the processed version of the test set predictors:&lt;/span&gt;
test_normalized &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;bake&lt;/span&gt;(norm_recipe, new_data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; ames_test, &lt;span style=&#34;color:#00f&#34;&gt;all_predictors&lt;/span&gt;())

test_results &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  test_results &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;rename&lt;/span&gt;(`random forest` &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; .pred) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;bind_cols&lt;/span&gt;(
    &lt;span style=&#34;color:#00f&#34;&gt;predict&lt;/span&gt;(glmn_fit, new_data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; test_normalized) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
      &lt;span style=&#34;color:#00f&#34;&gt;rename&lt;/span&gt;(glmnet &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; .pred)
  )
test_results
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 731 x 3&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;    Sale_Price `random forest` glmnet&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;         &amp;lt;dbl&amp;gt;           &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  1       5.33            5.22   5.27&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  2       5.02            5.21   5.17&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  3       5.27            5.25   5.23&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  4       5.60            5.51   5.25&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  5       5.28            5.24   5.25&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  6       5.17            5.19   5.19&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  7       5.02            4.97   5.19&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  8       5.46            5.50   5.49&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;  9       5.44            5.46   5.48&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 10       5.33            5.50   5.47&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # … with 721 more rows&lt;/span&gt;

test_results &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;metrics&lt;/span&gt;(truth &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; Sale_Price, estimate &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; glmnet) 
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 3 x 3&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   .metric .estimator .estimate&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 1 rmse    standard      0.132 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 2 rsq     standard      0.410 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 3 mae     standard      0.0956&lt;/span&gt;

test_results &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;gather&lt;/span&gt;(model, prediction, &lt;span style=&#34;color:#666&#34;&gt;-&lt;/span&gt;Sale_Price) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;ggplot&lt;/span&gt;(&lt;span style=&#34;color:#00f&#34;&gt;aes&lt;/span&gt;(x &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; prediction, y &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; Sale_Price)) &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;geom_abline&lt;/span&gt;(col &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;green&amp;#34;&lt;/span&gt;, lty &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;2&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;geom_point&lt;/span&gt;(alpha &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;.4&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;facet_wrap&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;~&lt;/span&gt;model) &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;coord_fixed&lt;/span&gt;()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;figs/glmn-pred-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This final plot compares the performance of the random forest and regularized regression models.&lt;/p&gt;
&lt;h2 id=&#34;session-information&#34;&gt;Session information&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;#&amp;gt; ─ Session info ───────────────────────────────────────────────────────────────
#&amp;gt;  setting  value                       
#&amp;gt;  version  R version 3.6.2 (2019-12-12)
#&amp;gt;  os       macOS Mojave 10.14.6        
#&amp;gt;  system   x86_64, darwin15.6.0        
#&amp;gt;  ui       X11                         
#&amp;gt;  language (EN)                        
#&amp;gt;  collate  en_US.UTF-8                 
#&amp;gt;  ctype    en_US.UTF-8                 
#&amp;gt;  tz       America/Denver              
#&amp;gt;  date     2020-04-17                  
#&amp;gt; 
#&amp;gt; ─ Packages ───────────────────────────────────────────────────────────────────
#&amp;gt;  package      * version date       lib source        
#&amp;gt;  AmesHousing  * 0.0.3   2017-12-17 [1] CRAN (R 3.6.0)
#&amp;gt;  broom        * 0.5.5   2020-02-29 [1] CRAN (R 3.6.0)
#&amp;gt;  dials        * 0.0.6   2020-04-03 [1] CRAN (R 3.6.2)
#&amp;gt;  dplyr        * 0.8.5   2020-03-07 [1] CRAN (R 3.6.0)
#&amp;gt;  ggplot2      * 3.3.0   2020-03-05 [1] CRAN (R 3.6.0)
#&amp;gt;  glmnet       * 3.0-2   2019-12-11 [1] CRAN (R 3.6.0)
#&amp;gt;  infer        * 0.5.1   2019-11-19 [1] CRAN (R 3.6.0)
#&amp;gt;  parsnip      * 0.1.0   2020-04-09 [1] CRAN (R 3.6.2)
#&amp;gt;  purrr        * 0.3.3   2019-10-18 [1] CRAN (R 3.6.0)
#&amp;gt;  randomForest * 4.6-14  2018-03-25 [1] CRAN (R 3.6.0)
#&amp;gt;  ranger       * 0.12.1  2020-01-10 [1] CRAN (R 3.6.0)
#&amp;gt;  recipes      * 0.1.10  2020-03-18 [1] CRAN (R 3.6.0)
#&amp;gt;  rlang          0.4.5   2020-03-01 [1] CRAN (R 3.6.0)
#&amp;gt;  rsample      * 0.0.6   2020-03-31 [1] CRAN (R 3.6.2)
#&amp;gt;  tibble       * 2.1.3   2019-06-06 [1] CRAN (R 3.6.2)
#&amp;gt;  tidymodels   * 0.1.0   2020-02-16 [1] CRAN (R 3.6.0)
#&amp;gt;  tune         * 0.1.0   2020-04-02 [1] CRAN (R 3.6.2)
#&amp;gt;  workflows    * 0.1.1   2020-03-17 [1] CRAN (R 3.6.0)
#&amp;gt;  yardstick    * 0.0.6   2020-03-17 [1] CRAN (R 3.6.0)
#&amp;gt; 
#&amp;gt; [1] /Library/Frameworks/R.framework/Versions/3.6/Resources/library
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Classification models using a neural network</title>
      <link>https://nutriverse.io/learn/models/parsnip-nnet/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://nutriverse.io/learn/models/parsnip-nnet/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;To use the code in this article, you will need to install the following packages: keras and tidymodels. You will also need the python keras library installed (see &lt;code&gt;?keras::install_keras()&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;We can create classification models with the tidymodels package 
&lt;a href=&#34;https://tidymodels.github.io/parsnip/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;parsnip&lt;/a&gt; to predict categorical quantities or class labels. Here, let&amp;rsquo;s fit a single classification model using a neural network and evaluate using a validation set. While the 
&lt;a href=&#34;https://tidymodels.github.io/tune/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tune&lt;/a&gt; package has functionality to also do this, the parsnip package is the center of attention in this article so that we can better understand its usage.&lt;/p&gt;
&lt;h2 id=&#34;fitting-a-neural-network&#34;&gt;Fitting a neural network&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s fit a model to a small, two predictor classification data set. The data are in the modeldata package (part of tidymodels) and have been split into training, validation, and test data sets. In this analysis, the test set is left untouched; this article tries to emulate a good data usage methodology where the test set would only be evaluated once at the end after a variety of models have been considered.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;data&lt;/span&gt;(bivariate)
&lt;span style=&#34;color:#00f&#34;&gt;nrow&lt;/span&gt;(bivariate_train)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; [1] 1009&lt;/span&gt;
&lt;span style=&#34;color:#00f&#34;&gt;nrow&lt;/span&gt;(bivariate_val)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; [1] 300&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;A plot of the data shows two right-skewed predictors:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;ggplot&lt;/span&gt;(bivariate_train, &lt;span style=&#34;color:#00f&#34;&gt;aes&lt;/span&gt;(x &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; A, y &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; B, col &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; Class)) &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;geom_point&lt;/span&gt;(alpha &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;.2&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;figs/biv-plot-1.svg&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s use a single hidden layer neural network to predict the outcome. To do this, we transform the predictor columns to be more symmetric (via the &lt;code&gt;step_BoxCox()&lt;/code&gt; function) and on a common scale (using &lt;code&gt;step_normalize()&lt;/code&gt;). We can use 
&lt;a href=&#34;https://tidymodels.github.io/recipes/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;recipes&lt;/a&gt; to do so:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;biv_rec &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;recipe&lt;/span&gt;(Class &lt;span style=&#34;color:#666&#34;&gt;~&lt;/span&gt; ., data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; bivariate_train) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;step_BoxCox&lt;/span&gt;(&lt;span style=&#34;color:#00f&#34;&gt;all_predictors&lt;/span&gt;())&lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;step_normalize&lt;/span&gt;(&lt;span style=&#34;color:#00f&#34;&gt;all_predictors&lt;/span&gt;()) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;prep&lt;/span&gt;(training &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; bivariate_train, retain &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;TRUE&lt;/span&gt;)

&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# We will juice() to get the processed training set back&lt;/span&gt;

&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# For validation:&lt;/span&gt;
val_normalized &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;bake&lt;/span&gt;(biv_rec, new_data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; bivariate_val, &lt;span style=&#34;color:#00f&#34;&gt;all_predictors&lt;/span&gt;())
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# For testing when we arrive at a final model: &lt;/span&gt;
test_normalized &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;bake&lt;/span&gt;(biv_rec, new_data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; bivariate_test, &lt;span style=&#34;color:#00f&#34;&gt;all_predictors&lt;/span&gt;())
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We can use the keras package to fit a model with 5 hidden units and a 10% dropout rate, to regularize the model:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;set.seed&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;57974&lt;/span&gt;)
nnet_fit &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;mlp&lt;/span&gt;(epochs &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;100&lt;/span&gt;, hidden_units &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;5&lt;/span&gt;, dropout &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;0.1&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;set_mode&lt;/span&gt;(&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;classification&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# Also set engine-specific `verbose` argument to prevent logging the results: &lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;set_engine&lt;/span&gt;(&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;keras&amp;#34;&lt;/span&gt;, verbose &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;0&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;fit&lt;/span&gt;(Class &lt;span style=&#34;color:#666&#34;&gt;~&lt;/span&gt; ., data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;juice&lt;/span&gt;(biv_rec))

nnet_fit
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; parsnip model object&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Fit time:  8.7s &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Model&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Model: &amp;#34;sequential&amp;#34;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ________________________________________________________________________________&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Layer (type)                        Output Shape                    Param #     &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ================================================================================&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; dense (Dense)                       (None, 5)                       15          &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ________________________________________________________________________________&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; dense_1 (Dense)                     (None, 5)                       30          &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ________________________________________________________________________________&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; dropout (Dropout)                   (None, 5)                       0           &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ________________________________________________________________________________&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; dense_2 (Dense)                     (None, 2)                       12          &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ================================================================================&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Total params: 57&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Trainable params: 57&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Non-trainable params: 0&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; ________________________________________________________________________________&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;model-performance&#34;&gt;Model performance&lt;/h2&gt;
&lt;p&gt;In parsnip, the &lt;code&gt;predict()&lt;/code&gt; function can be used to characterize performance on the validation set. Since parsnip always produces tibble outputs, these can just be column bound to the original data:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;val_results &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  bivariate_val &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;bind_cols&lt;/span&gt;(
    &lt;span style=&#34;color:#00f&#34;&gt;predict&lt;/span&gt;(nnet_fit, new_data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; val_normalized),
    &lt;span style=&#34;color:#00f&#34;&gt;predict&lt;/span&gt;(nnet_fit, new_data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; val_normalized, type &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;prob&amp;#34;&lt;/span&gt;)
  )
val_results &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;slice&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;5&lt;/span&gt;)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 5 x 6&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;       A     B Class .pred_class .pred_One .pred_Two&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt;           &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 1 1061.  74.5 One   Two             0.473    0.527 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 2 1241.  83.4 One   Two             0.484    0.516 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 3  939.  71.9 One   One             0.636    0.364 &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 4  813.  77.1 One   One             0.925    0.0746&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 5 1706.  92.8 Two   Two             0.355    0.645&lt;/span&gt;

val_results &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;roc_auc&lt;/span&gt;(truth &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; Class, .pred_One)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 1 x 3&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   .metric .estimator .estimate&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 1 roc_auc binary         0.815&lt;/span&gt;

val_results &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;accuracy&lt;/span&gt;(truth &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; Class, .pred_class)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; # A tibble: 1 x 3&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   .metric  .estimator .estimate&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; 1 accuracy binary         0.737&lt;/span&gt;

val_results &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;conf_mat&lt;/span&gt;(truth &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; Class, .pred_class)
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;           Truth&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt; Prediction One Two&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;        One 150  27&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#&amp;gt;        Two  52  71&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Let&amp;rsquo;s also create a grid to get a visual sense of the class boundary for the validation set.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;a_rng &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;range&lt;/span&gt;(bivariate_train&lt;span style=&#34;color:#666&#34;&gt;$&lt;/span&gt;A)
b_rng &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;range&lt;/span&gt;(bivariate_train&lt;span style=&#34;color:#666&#34;&gt;$&lt;/span&gt;B)
x_grid &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;expand.grid&lt;/span&gt;(A &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;seq&lt;/span&gt;(a_rng[1], a_rng[2], length.out &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;100&lt;/span&gt;),
              B &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;seq&lt;/span&gt;(b_rng[1], b_rng[2], length.out &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;100&lt;/span&gt;))
x_grid_trans &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;bake&lt;/span&gt;(biv_rec, x_grid)

&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# Make predictions using the transformed predictors but &lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# attach them to the predictors in the original units: &lt;/span&gt;
x_grid &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  x_grid &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;bind_cols&lt;/span&gt;(&lt;span style=&#34;color:#00f&#34;&gt;predict&lt;/span&gt;(nnet_fit, x_grid_trans, type &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;prob&amp;#34;&lt;/span&gt;))

&lt;span style=&#34;color:#00f&#34;&gt;ggplot&lt;/span&gt;(x_grid, &lt;span style=&#34;color:#00f&#34;&gt;aes&lt;/span&gt;(x &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; A, y &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; B)) &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;geom_contour&lt;/span&gt;(&lt;span style=&#34;color:#00f&#34;&gt;aes&lt;/span&gt;(z &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; .pred_One), breaks &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;.5&lt;/span&gt;, col &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;black&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;geom_point&lt;/span&gt;(data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; bivariate_val, &lt;span style=&#34;color:#00f&#34;&gt;aes&lt;/span&gt;(col &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; Class), alpha &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;0.3&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;figs/biv-boundary-1.svg&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;session-information&#34;&gt;Session information&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;#&amp;gt; ─ Session info ───────────────────────────────────────────────────────────────
#&amp;gt;  setting  value                       
#&amp;gt;  version  R version 3.6.2 (2019-12-12)
#&amp;gt;  os       macOS Mojave 10.14.6        
#&amp;gt;  system   x86_64, darwin15.6.0        
#&amp;gt;  ui       X11                         
#&amp;gt;  language (EN)                        
#&amp;gt;  collate  en_US.UTF-8                 
#&amp;gt;  ctype    en_US.UTF-8                 
#&amp;gt;  tz       America/Denver              
#&amp;gt;  date     2020-04-17                  
#&amp;gt; 
#&amp;gt; ─ Packages ───────────────────────────────────────────────────────────────────
#&amp;gt;  package    * version date       lib source        
#&amp;gt;  broom      * 0.5.5   2020-02-29 [1] CRAN (R 3.6.0)
#&amp;gt;  dials      * 0.0.6   2020-04-03 [1] CRAN (R 3.6.2)
#&amp;gt;  dplyr      * 0.8.5   2020-03-07 [1] CRAN (R 3.6.0)
#&amp;gt;  ggplot2    * 3.3.0   2020-03-05 [1] CRAN (R 3.6.0)
#&amp;gt;  infer      * 0.5.1   2019-11-19 [1] CRAN (R 3.6.0)
#&amp;gt;  keras        2.2.5.0 2019-10-08 [1] CRAN (R 3.6.0)
#&amp;gt;  parsnip    * 0.1.0   2020-04-09 [1] CRAN (R 3.6.2)
#&amp;gt;  purrr      * 0.3.3   2019-10-18 [1] CRAN (R 3.6.0)
#&amp;gt;  recipes    * 0.1.10  2020-03-18 [1] CRAN (R 3.6.0)
#&amp;gt;  rlang        0.4.5   2020-03-01 [1] CRAN (R 3.6.0)
#&amp;gt;  rsample    * 0.0.6   2020-03-31 [1] CRAN (R 3.6.2)
#&amp;gt;  tibble     * 2.1.3   2019-06-06 [1] CRAN (R 3.6.2)
#&amp;gt;  tidymodels * 0.1.0   2020-02-16 [1] CRAN (R 3.6.0)
#&amp;gt;  tune       * 0.1.0   2020-04-02 [1] CRAN (R 3.6.2)
#&amp;gt;  workflows  * 0.1.1   2020-03-17 [1] CRAN (R 3.6.0)
#&amp;gt;  yardstick  * 0.0.6   2020-03-17 [1] CRAN (R 3.6.0)
#&amp;gt; 
#&amp;gt; [1] /Library/Frameworks/R.framework/Versions/3.6/Resources/library
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Multivariate analysis using partial least squares</title>
      <link>https://nutriverse.io/learn/models/pls/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://nutriverse.io/learn/models/pls/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;To use the code in this article, you will need to install the following packages: modeldata, pls, tidymodels, and tidyr.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Multivariate analysis&amp;rdquo; usually refers to multiple &lt;em&gt;outcomes&lt;/em&gt; being modeled, analyzed, and/or predicted. There are multivariate versions of many common statistical tools. For example, suppose there was a data set with columns &lt;code&gt;y1&lt;/code&gt; and &lt;code&gt;y2&lt;/code&gt; representing two outcomes to be predicted. The &lt;code&gt;lm()&lt;/code&gt; function would look something like:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;lm&lt;/span&gt;(&lt;span style=&#34;color:#00f&#34;&gt;cbind&lt;/span&gt;(y1, y2) &lt;span style=&#34;color:#666&#34;&gt;~&lt;/span&gt; ., data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; dat)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This &lt;code&gt;cbind()&lt;/code&gt; call is pretty awkward and is a consequence of how the traditional formula infrastructure works. The recipes package is a lot easier to work with! This article demonstrates how to model multiple outcomes.&lt;/p&gt;
&lt;p&gt;The data that we&amp;rsquo;ll use has three outcomes. From &lt;code&gt;?modeldata::meats&lt;/code&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;These data are recorded on a Tecator Infratec Food and Feed Analyzer working in the wavelength range 850 - 1050 nm by the Near Infrared Transmission (NIT) principle. Each sample contains finely chopped pure meat with different moisture, fat and protein contents.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;For each meat sample the data consists of a 100 channel spectrum of absorbances and the contents of moisture (water), fat and protein. The absorbance is &lt;code&gt;-log10&lt;/code&gt; of the transmittance measured by the spectrometer. The three contents, measured in percent, are determined by analytic chemistry.&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The goal is to predict the proportion of the three substances using the chemistry test. There can often be a high degree of between-variable correlations in predictors, and that is certainly the case here.&lt;/p&gt;
&lt;p&gt;To start, let&amp;rsquo;s take the two data matrices (called &lt;code&gt;endpoints&lt;/code&gt; and &lt;code&gt;absorp&lt;/code&gt;) and bind them together in a data frame:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;library&lt;/span&gt;(modeldata)
&lt;span style=&#34;color:#00f&#34;&gt;data&lt;/span&gt;(meats)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The three &lt;em&gt;outcomes&lt;/em&gt; have fairly high correlations also.&lt;/p&gt;
&lt;h2 id=&#34;preprocessing-the-data&#34;&gt;Preprocessing the data&lt;/h2&gt;
&lt;p&gt;If the outcomes can be predicted using a linear model, partial least squares (PLS) is an ideal method. PLS models the data as a function of a set of unobserved &lt;em&gt;latent&lt;/em&gt; variables that are derived in a manner similar to principal component analysis (PCA).&lt;/p&gt;
&lt;p&gt;PLS, unlike PCA, also incorporates the outcome data when creating the PLS components. Like PCA, it tries to maximize the variance of the predictors that are explained by the components but it also tries to simultaneously maximize the correlation between those components and the outcomes. In this way, PLS &lt;em&gt;chases&lt;/em&gt; variation of the predictors and outcomes.&lt;/p&gt;
&lt;p&gt;Since we are working with variances and covariances, we need to standardize the data. The recipe will center and scale all of the variables.&lt;/p&gt;
&lt;p&gt;Many base R functions that deal with multivariate outcomes using a formula require the use of &lt;code&gt;cbind()&lt;/code&gt; on the left-hand side of the formula to work with the traditional formula methods. In tidymodels, recipes do not; the outcomes can be symbolically &amp;ldquo;added&amp;rdquo; together on the left-hand side:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;norm_rec &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;recipe&lt;/span&gt;(water &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; fat &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; protein &lt;span style=&#34;color:#666&#34;&gt;~&lt;/span&gt; ., data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; meats) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;step_normalize&lt;/span&gt;(&lt;span style=&#34;color:#00f&#34;&gt;everything&lt;/span&gt;()) 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Before we can finalize the PLS model, the number of PLS components to retain must be determined. This can be done using performance metrics such as the root mean squared error. However, we can also calculate the proportion of variance explained by the components for the &lt;em&gt;predictors and each of the outcomes&lt;/em&gt;. This allows an informed choice to be made based on the level of evidence that the situation requires.&lt;/p&gt;
&lt;p&gt;Since the data set isn&amp;rsquo;t large, let&amp;rsquo;s use resampling to measure these proportions. With ten repeats of 10-fold cross-validation, we build the PLS model on 90% of the data and evaluate on the heldout 10%. For each of the 100 models, we extract and save the proportions.&lt;/p&gt;
&lt;p&gt;The folds can be created using the 
&lt;a href=&#34;https://tidymodels.github.io/rsample/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;rsample&lt;/a&gt; package and the recipe can be estimated for each resample using the 
&lt;a href=&#34;https://tidymodels.github.io/rsample/reference/prepper.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;prepper()&lt;/code&gt;&lt;/a&gt; function:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;set.seed&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;57343&lt;/span&gt;)
folds &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;vfold_cv&lt;/span&gt;(meats, repeats &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;10&lt;/span&gt;)

folds &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  folds &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;mutate&lt;/span&gt;(recipes &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;map&lt;/span&gt;(splits, prepper, recipe &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; norm_rec))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;partial-least-squares&#34;&gt;Partial least squares&lt;/h2&gt;
&lt;p&gt;The complicated parts for moving forward are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Formatting the predictors and outcomes into the format that the pls package requires, and&lt;/li&gt;
&lt;li&gt;Estimating the proportions.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For the first part, the standardized outcomes and predictors need to be formatted into two separate matrices. Since we used &lt;code&gt;retain = TRUE&lt;/code&gt; when prepping the recipes, we can use the &lt;code&gt;juice()&lt;/code&gt; function. To save the data as a matrix, the option &lt;code&gt;composition = &amp;quot;matrix&amp;quot;&lt;/code&gt; will avoid saving the data as tibbles and use the required format.&lt;/p&gt;
&lt;p&gt;The pls package expects a simple formula to specify the model, but each side of the formula should &lt;em&gt;represent a matrix&lt;/em&gt;. In other words, we need a data set with two columns where each column is a matrix. The secret to doing this is to &amp;ldquo;protect&amp;rdquo; the two matrices using &lt;code&gt;I()&lt;/code&gt; when adding them to the data frame.&lt;/p&gt;
&lt;p&gt;The calculation for the proportion of variance explained is straightforward for the predictors; the function &lt;code&gt;pls::explvar()&lt;/code&gt; will compute that. For the outcomes, the process is more complicated. A ready-made function to compute these is not obvious but there is some code inside of the summary function to do the computation (see below).&lt;/p&gt;
&lt;p&gt;The function &lt;code&gt;get_var_explained()&lt;/code&gt; shown here will do all these computations and return a data frame with columns &lt;code&gt;components&lt;/code&gt;, &lt;code&gt;source&lt;/code&gt; (for the predictors, water, etc), and the &lt;code&gt;proportion&lt;/code&gt; of variance that is explained by the components.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;library&lt;/span&gt;(pls)
&lt;span style=&#34;color:#00f&#34;&gt;library&lt;/span&gt;(tidyr)

get_var_explained &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;function&lt;/span&gt;(recipe, &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;...&lt;/span&gt;) {
  
  &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# Extract the predictors and outcomes into their own matrices&lt;/span&gt;
  y_mat &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;juice&lt;/span&gt;(recipe, composition &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;matrix&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#00f&#34;&gt;all_outcomes&lt;/span&gt;())
  x_mat &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;juice&lt;/span&gt;(recipe, composition &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;matrix&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#00f&#34;&gt;all_predictors&lt;/span&gt;())
  
  &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# The pls package prefers the data in a data frame where the outcome&lt;/span&gt;
  &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# and predictors are in _matrices_. To make sure this is formatted&lt;/span&gt;
  &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# properly, use the `I()` function to inhibit `data.frame()` from making&lt;/span&gt;
  &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# all the individual columns. `pls_format` should have two columns.&lt;/span&gt;
  pls_format &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;data.frame&lt;/span&gt;(
    endpoints &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;I&lt;/span&gt;(y_mat),
    measurements &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;I&lt;/span&gt;(x_mat)
  )
  &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# Fit the model&lt;/span&gt;
  mod &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;plsr&lt;/span&gt;(endpoints &lt;span style=&#34;color:#666&#34;&gt;~&lt;/span&gt; measurements, data &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; pls_format)
  
  &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# Get the proportion of the predictor variance that is explained&lt;/span&gt;
  &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# by the model for different number of components. &lt;/span&gt;
  xve &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;explvar&lt;/span&gt;(mod)&lt;span style=&#34;color:#666&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;100&lt;/span&gt; 

  &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# To do the same for the outcome, it is more complex. This code &lt;/span&gt;
  &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# was extracted from pls:::summary.mvr. &lt;/span&gt;
  explained &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
    &lt;span style=&#34;color:#00f&#34;&gt;drop&lt;/span&gt;(pls&lt;span style=&#34;color:#666&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#00f&#34;&gt;R2&lt;/span&gt;(mod, estimate &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;train&amp;#34;&lt;/span&gt;, intercept &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;FALSE&lt;/span&gt;)&lt;span style=&#34;color:#666&#34;&gt;$&lt;/span&gt;val) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
    &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# transpose so that components are in rows&lt;/span&gt;
    &lt;span style=&#34;color:#00f&#34;&gt;t&lt;/span&gt;() &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; 
    &lt;span style=&#34;color:#00f&#34;&gt;as_tibble&lt;/span&gt;() &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
    &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# Add the predictor proportions&lt;/span&gt;
    &lt;span style=&#34;color:#00f&#34;&gt;mutate&lt;/span&gt;(predictors &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;cumsum&lt;/span&gt;(xve) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;as.vector&lt;/span&gt;(),
           components &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;seq_along&lt;/span&gt;(xve)) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
    &lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# Put into a tidy format that is tall&lt;/span&gt;
    &lt;span style=&#34;color:#00f&#34;&gt;pivot_longer&lt;/span&gt;(
      cols &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;-&lt;/span&gt;components),
      names_to &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;source&amp;#34;&lt;/span&gt;,
      values_to &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;proportion&amp;#34;&lt;/span&gt;
    )
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We compute this data frame for each resample and save the results in the different columns.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;folds &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  folds &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;mutate&lt;/span&gt;(var &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;map&lt;/span&gt;(recipes, get_var_explained),
         var &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;unname&lt;/span&gt;(var))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;To extract and aggregate these data, simple row binding can be used to stack the data vertically. Most of the action happens in the first 15 components so let&amp;rsquo;s filter the data and compute the &lt;em&gt;average&lt;/em&gt; proportion.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;variance_data &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;-&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;bind_rows&lt;/span&gt;(folds[[&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;var&amp;#34;&lt;/span&gt;]]) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;filter&lt;/span&gt;(components &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;15&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;group_by&lt;/span&gt;(components, source) &lt;span style=&#34;color:#666&#34;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span style=&#34;color:#00f&#34;&gt;summarize&lt;/span&gt;(proportion &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;mean&lt;/span&gt;(proportion))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The plot below shows that, if the protein measurement is important, you might require 10 or so components to achieve a good representation of that outcome. Note that the predictor variance is captured extremely well using a single component. This is due to the high degree of correlation in those data.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#00f&#34;&gt;ggplot&lt;/span&gt;(variance_data, &lt;span style=&#34;color:#00f&#34;&gt;aes&lt;/span&gt;(x &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; components, y &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; proportion, col &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; source)) &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;geom_line&lt;/span&gt;() &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; 
  &lt;span style=&#34;color:#00f&#34;&gt;geom_point&lt;/span&gt;() 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;figs/plot-1.svg&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;session-information&#34;&gt;Session information&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;#&amp;gt; ─ Session info ───────────────────────────────────────────────────────────────
#&amp;gt;  setting  value                       
#&amp;gt;  version  R version 3.6.2 (2019-12-12)
#&amp;gt;  os       macOS Mojave 10.14.6        
#&amp;gt;  system   x86_64, darwin15.6.0        
#&amp;gt;  ui       X11                         
#&amp;gt;  language (EN)                        
#&amp;gt;  collate  en_US.UTF-8                 
#&amp;gt;  ctype    en_US.UTF-8                 
#&amp;gt;  tz       America/Denver              
#&amp;gt;  date     2020-04-17                  
#&amp;gt; 
#&amp;gt; ─ Packages ───────────────────────────────────────────────────────────────────
#&amp;gt;  package    * version date       lib source        
#&amp;gt;  broom      * 0.5.5   2020-02-29 [1] CRAN (R 3.6.0)
#&amp;gt;  dials      * 0.0.6   2020-04-03 [1] CRAN (R 3.6.2)
#&amp;gt;  dplyr      * 0.8.5   2020-03-07 [1] CRAN (R 3.6.0)
#&amp;gt;  ggplot2    * 3.3.0   2020-03-05 [1] CRAN (R 3.6.0)
#&amp;gt;  infer      * 0.5.1   2019-11-19 [1] CRAN (R 3.6.0)
#&amp;gt;  modeldata  * 0.0.1   2019-12-06 [1] CRAN (R 3.6.0)
#&amp;gt;  parsnip    * 0.1.0   2020-04-09 [1] CRAN (R 3.6.2)
#&amp;gt;  pls        * 2.7-2   2019-10-01 [1] CRAN (R 3.6.0)
#&amp;gt;  purrr      * 0.3.3   2019-10-18 [1] CRAN (R 3.6.0)
#&amp;gt;  recipes    * 0.1.10  2020-03-18 [1] CRAN (R 3.6.0)
#&amp;gt;  rlang        0.4.5   2020-03-01 [1] CRAN (R 3.6.0)
#&amp;gt;  rsample    * 0.0.6   2020-03-31 [1] CRAN (R 3.6.2)
#&amp;gt;  tibble     * 2.1.3   2019-06-06 [1] CRAN (R 3.6.2)
#&amp;gt;  tidymodels * 0.1.0   2020-02-16 [1] CRAN (R 3.6.0)
#&amp;gt;  tidyr      * 1.0.2   2020-01-24 [1] CRAN (R 3.6.0)
#&amp;gt;  tune       * 0.1.0   2020-04-02 [1] CRAN (R 3.6.2)
#&amp;gt;  workflows  * 0.1.1   2020-03-17 [1] CRAN (R 3.6.0)
#&amp;gt;  yardstick  * 0.0.6   2020-03-17 [1] CRAN (R 3.6.0)
#&amp;gt; 
#&amp;gt; [1] /Library/Frameworks/R.framework/Versions/3.6/Resources/library
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
  </channel>
</rss>
